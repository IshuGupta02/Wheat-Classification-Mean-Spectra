{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c4fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35daa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6aebd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'wheat_40_var_avg_new.csv'\n",
    "\n",
    "def dir(file_name):\n",
    "    return '../data/'+file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a19daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_X_Y(dataframe):\n",
    "    return (dataframe.drop('classes', axis =1), dataframe.loc[:,'classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554bc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class filter_method(Enum):\n",
    "    none = 0\n",
    "    snv = 1\n",
    "    msc = 2\n",
    "    savgol = 3\n",
    "\n",
    "FILT = 3\n",
    "FILTER = filter_method(FILT).name\n",
    "\n",
    "# to be set if filter chosen is savgol\n",
    "WINDOW = 7\n",
    "ORDER = 2\n",
    "DERIVATIVE = 1\n",
    "\n",
    "#will be used for test dataset in msc\n",
    "reference= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c20231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def create_test_train(X, y, test_size = 0.2, shuffle = True):\n",
    "    return train_test_split(X,y, test_size = test_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e9301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Normal Variate\n",
    "def snv(input_data):\n",
    "  \n",
    "    # Define a new array and populate it with the corrected data  \n",
    "    output_data = np.zeros_like(input_data)\n",
    "    for i in range(input_data.shape[0]):\n",
    " \n",
    "        # Apply correction\n",
    "        output_data[i,:] = (input_data[i,:] - np.mean(input_data[i,:])) / np.std(input_data[i,:])\n",
    " \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d925acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicative scatter correction\n",
    "def msc(input_data, reference=None):\n",
    "#     print(reference)\n",
    "    ''' Perform Multiplicative scatter correction'''\n",
    "\n",
    "    # Baseline correction\n",
    "    for i in range(input_data.shape[0]):\n",
    "        input_data[i,:] -= input_data[i,:].mean()\n",
    "\n",
    "    # Get the reference spectrum. If not given, estimate from the mean    \n",
    "    if reference is None:    \n",
    "        # Calculate mean\n",
    "        matm = np.mean(input_data, axis=0)\n",
    "    else:\n",
    "        matm = reference\n",
    "\n",
    "    # Define a new data matrix and populate it with the corrected data    \n",
    "    output_data = np.zeros_like(input_data)\n",
    "    for i in range(input_data.shape[0]):\n",
    "        # Run regression\n",
    "        fit = np.polyfit(matm, input_data[i,:], 1, full=True)\n",
    "        # Apply correction\n",
    "        output_data[i,:] = (input_data[i,:] - fit[0][1]) / fit[0][0] \n",
    "\n",
    "    return (output_data, matm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b5090be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter, general_gaussian\n",
    "def savgol(input_data):\n",
    "    w = WINDOW\n",
    "    p = ORDER\n",
    "    d = DERIVATIVE\n",
    "    \n",
    "    output_data = savgol_filter(np.array(input_data), w, polyorder = p, deriv=d)\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68affd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X,y, type=\"train\"):\n",
    "    if FILTER == \"snv\":\n",
    "        return {\"X\": snv(np.array(X)), \"y\": y}\n",
    "    elif FILTER == \"msc\":\n",
    "        msc_output = msc(np.array(X), reference = reference if type==\"test\" else None)\n",
    "        X = msc_output[0]\n",
    "        ref = msc_output[1]\n",
    "        return {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"ref\": ref\n",
    "        }\n",
    "    elif FILTER == \"savgol\":\n",
    "        return {\n",
    "            \"X\": savgol(X),\n",
    "            \"y\": y\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"X\":X,\n",
    "            \"y\":y\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8dec1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dir(file_name))\n",
    "X,y = seperate_X_Y(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58535347",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_set = set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8c8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_CLASSES = len(y_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd357e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = create_test_train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f79eb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_results = preprocess_data(X_train_raw,y_train_raw)\n",
    "X_train, y_train = preprocessed_results[\"X\"], preprocessed_results[\"y\"]\n",
    "\n",
    "if FILTER == \"msc\":\n",
    "    reference = preprocessed_results[\"ref\"]\n",
    "    \n",
    "preprocessed_results_test = preprocess_data(X_test_raw, y_test_raw, type=\"test\")\n",
    "X_test, y_test = preprocessed_results_test[\"X\"], preprocessed_results_test[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e1e570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64512, 147, 1)\n",
      "(16128, 147, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "print(X_train.shape)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec820add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Conv1D, Flatten, Dense, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba682707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(input_shape,num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=5, input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=5 ))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d95ace51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel(X_train.shape[1:],NUM_OF_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33a0377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 143, 32)           192       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 28, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 896)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              897000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 4004      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 901,196\n",
      "Trainable params: 901,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20cd1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eva = []\n",
    "test_eva = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fd5eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dataframe = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "863f63f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  1\n",
      "504/504 - 20s - loss: 1.1892 - accuracy: 0.4350 - 20s/epoch - 39ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 21s 10ms/step - loss: 1.0950 - accuracy: 0.5378\n",
      "for testing\n",
      "504/504 [==============================] - 5s 9ms/step - loss: 1.0907 - accuracy: 0.5387\n",
      "\n",
      "Epoch:  2\n",
      "504/504 - 21s - loss: 0.9751 - accuracy: 0.6051 - 21s/epoch - 41ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.8342 - accuracy: 0.6628\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.8365 - accuracy: 0.6618\n",
      "\n",
      "Epoch:  3\n",
      "504/504 - 15s - loss: 0.7250 - accuracy: 0.6937 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.6543 - accuracy: 0.7185\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.6546 - accuracy: 0.7177\n",
      "\n",
      "Epoch:  4\n",
      "504/504 - 15s - loss: 0.6149 - accuracy: 0.7397 - 15s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 18s 9ms/step - loss: 0.5827 - accuracy: 0.7615\n",
      "for testing\n",
      "504/504 [==============================] - 5s 9ms/step - loss: 0.5803 - accuracy: 0.7652\n",
      "\n",
      "Epoch:  5\n",
      "504/504 - 22s - loss: 0.5455 - accuracy: 0.7756 - 22s/epoch - 44ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 19s 9ms/step - loss: 0.5138 - accuracy: 0.7973\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.5139 - accuracy: 0.7979\n",
      "\n",
      "Epoch:  6\n",
      "504/504 - 16s - loss: 0.4906 - accuracy: 0.8020 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.4675 - accuracy: 0.8158\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.4668 - accuracy: 0.8170\n",
      "\n",
      "Epoch:  7\n",
      "504/504 - 14s - loss: 0.4444 - accuracy: 0.8217 - 14s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 18s 9ms/step - loss: 0.4377 - accuracy: 0.8147\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.4374 - accuracy: 0.8157\n",
      "\n",
      "Epoch:  8\n",
      "504/504 - 15s - loss: 0.4046 - accuracy: 0.8385 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.3827 - accuracy: 0.8469\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.3828 - accuracy: 0.8468\n",
      "\n",
      "Epoch:  9\n",
      "504/504 - 15s - loss: 0.3728 - accuracy: 0.8515 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.3849 - accuracy: 0.8436\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.3867 - accuracy: 0.8429\n",
      "\n",
      "Epoch:  10\n",
      "504/504 - 14s - loss: 0.3535 - accuracy: 0.8594 - 14s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.3617 - accuracy: 0.8526\n",
      "for testing\n",
      "504/504 [==============================] - 3s 7ms/step - loss: 0.3655 - accuracy: 0.8525\n",
      "\n",
      "Epoch:  11\n",
      "504/504 - 14s - loss: 0.3344 - accuracy: 0.8671 - 14s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.3253 - accuracy: 0.8703\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.3283 - accuracy: 0.8709\n",
      "\n",
      "Epoch:  12\n",
      "504/504 - 16s - loss: 0.3200 - accuracy: 0.8733 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.3055 - accuracy: 0.8800\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.3059 - accuracy: 0.8797\n",
      "\n",
      "Epoch:  13\n",
      "504/504 - 15s - loss: 0.3069 - accuracy: 0.8780 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.2923 - accuracy: 0.8868\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.2948 - accuracy: 0.8873\n",
      "\n",
      "Epoch:  14\n",
      "504/504 - 16s - loss: 0.2949 - accuracy: 0.8835 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.3046 - accuracy: 0.8811\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.3048 - accuracy: 0.8813\n",
      "\n",
      "Epoch:  15\n",
      "504/504 - 17s - loss: 0.2855 - accuracy: 0.8873 - 17s/epoch - 33ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.2702 - accuracy: 0.8935\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2732 - accuracy: 0.8927\n",
      "\n",
      "Epoch:  16\n",
      "504/504 - 16s - loss: 0.2800 - accuracy: 0.8890 - 16s/epoch - 33ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.2901 - accuracy: 0.8822\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.2924 - accuracy: 0.8829\n",
      "\n",
      "Epoch:  17\n",
      "504/504 - 16s - loss: 0.2721 - accuracy: 0.8924 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.2527 - accuracy: 0.9007\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2545 - accuracy: 0.9002\n",
      "\n",
      "Epoch:  18\n",
      "504/504 - 15s - loss: 0.2614 - accuracy: 0.8967 - 15s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.2492 - accuracy: 0.9033\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2503 - accuracy: 0.9029\n",
      "\n",
      "Epoch:  19\n",
      "504/504 - 15s - loss: 0.2545 - accuracy: 0.8990 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.2548 - accuracy: 0.9021\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2562 - accuracy: 0.9009\n",
      "\n",
      "Epoch:  20\n",
      "504/504 - 15s - loss: 0.2490 - accuracy: 0.9029 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.2372 - accuracy: 0.9074\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2406 - accuracy: 0.9042\n",
      "\n",
      "Epoch:  21\n",
      "504/504 - 15s - loss: 0.2467 - accuracy: 0.9019 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.2426 - accuracy: 0.9039\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2473 - accuracy: 0.9009\n",
      "\n",
      "Epoch:  22\n",
      "504/504 - 15s - loss: 0.2371 - accuracy: 0.9074 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.2206 - accuracy: 0.9154\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2228 - accuracy: 0.9145\n",
      "\n",
      "Epoch:  23\n",
      "504/504 - 15s - loss: 0.2346 - accuracy: 0.9074 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.2605 - accuracy: 0.8940\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2646 - accuracy: 0.8932\n",
      "\n",
      "Epoch:  24\n",
      "504/504 - 15s - loss: 0.2313 - accuracy: 0.9092 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.2158 - accuracy: 0.9166\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2198 - accuracy: 0.9146\n",
      "\n",
      "Epoch:  25\n",
      "504/504 - 15s - loss: 0.2248 - accuracy: 0.9130 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.2189 - accuracy: 0.9136\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.2219 - accuracy: 0.9125\n",
      "\n",
      "Epoch:  26\n",
      "504/504 - 15s - loss: 0.2190 - accuracy: 0.9139 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.2283 - accuracy: 0.9092\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2320 - accuracy: 0.9084\n",
      "\n",
      "Epoch:  27\n",
      "504/504 - 16s - loss: 0.2192 - accuracy: 0.9143 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 22s 11ms/step - loss: 0.2412 - accuracy: 0.9038\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2455 - accuracy: 0.9014\n",
      "\n",
      "Epoch:  28\n",
      "504/504 - 15s - loss: 0.2117 - accuracy: 0.9179 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.2031 - accuracy: 0.9234\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2061 - accuracy: 0.9222\n",
      "\n",
      "Epoch:  29\n",
      "504/504 - 15s - loss: 0.2102 - accuracy: 0.9189 - 15s/epoch - 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.2007 - accuracy: 0.9231\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2029 - accuracy: 0.9237\n",
      "\n",
      "Epoch:  30\n",
      "504/504 - 14s - loss: 0.2061 - accuracy: 0.9199 - 14s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1949 - accuracy: 0.9264\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1961 - accuracy: 0.9255\n",
      "\n",
      "Epoch:  31\n",
      "504/504 - 17s - loss: 0.2008 - accuracy: 0.9227 - 17s/epoch - 33ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1896 - accuracy: 0.9286\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1922 - accuracy: 0.9281\n",
      "\n",
      "Epoch:  32\n",
      "504/504 - 15s - loss: 0.2016 - accuracy: 0.9218 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.2021 - accuracy: 0.9232\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.2029 - accuracy: 0.9239\n",
      "\n",
      "Epoch:  33\n",
      "504/504 - 15s - loss: 0.1963 - accuracy: 0.9237 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.2199 - accuracy: 0.9111\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2207 - accuracy: 0.9103\n",
      "\n",
      "Epoch:  34\n",
      "504/504 - 16s - loss: 0.1960 - accuracy: 0.9239 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.1868 - accuracy: 0.9296\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1899 - accuracy: 0.9273\n",
      "\n",
      "Epoch:  35\n",
      "504/504 - 15s - loss: 0.1876 - accuracy: 0.9278 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 20s 10ms/step - loss: 0.1945 - accuracy: 0.9246\n",
      "for testing\n",
      "504/504 [==============================] - 5s 9ms/step - loss: 0.1949 - accuracy: 0.9248\n",
      "\n",
      "Epoch:  36\n",
      "504/504 - 16s - loss: 0.1876 - accuracy: 0.9278 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1689 - accuracy: 0.9367\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.1713 - accuracy: 0.9363\n",
      "\n",
      "Epoch:  37\n",
      "504/504 - 15s - loss: 0.1874 - accuracy: 0.9280 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.2025 - accuracy: 0.9213\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.2026 - accuracy: 0.9224\n",
      "\n",
      "Epoch:  38\n",
      "504/504 - 15s - loss: 0.1846 - accuracy: 0.9294 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.1770 - accuracy: 0.9323\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1772 - accuracy: 0.9314\n",
      "\n",
      "Epoch:  39\n",
      "504/504 - 15s - loss: 0.1826 - accuracy: 0.9305 - 15s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.1724 - accuracy: 0.9349\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.1753 - accuracy: 0.9344\n",
      "\n",
      "Epoch:  40\n",
      "504/504 - 15s - loss: 0.1826 - accuracy: 0.9297 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1906 - accuracy: 0.9263\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1938 - accuracy: 0.9237\n",
      "\n",
      "Epoch:  41\n",
      "504/504 - 16s - loss: 0.1737 - accuracy: 0.9338 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.1634 - accuracy: 0.9388\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1661 - accuracy: 0.9372\n",
      "\n",
      "Epoch:  42\n",
      "504/504 - 17s - loss: 0.1755 - accuracy: 0.9338 - 17s/epoch - 34ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.1750 - accuracy: 0.9329\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1753 - accuracy: 0.9334\n",
      "\n",
      "Epoch:  43\n",
      "504/504 - 15s - loss: 0.1732 - accuracy: 0.9336 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.1717 - accuracy: 0.9338\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.1728 - accuracy: 0.9347\n",
      "\n",
      "Epoch:  44\n",
      "504/504 - 15s - loss: 0.1714 - accuracy: 0.9339 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1640 - accuracy: 0.9391\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1664 - accuracy: 0.9368\n",
      "\n",
      "Epoch:  45\n",
      "504/504 - 15s - loss: 0.1671 - accuracy: 0.9357 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1582 - accuracy: 0.9410\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1588 - accuracy: 0.9407\n",
      "\n",
      "Epoch:  46\n",
      "504/504 - 15s - loss: 0.1702 - accuracy: 0.9346 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.1703 - accuracy: 0.9360\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1716 - accuracy: 0.9378\n",
      "\n",
      "Epoch:  47\n",
      "504/504 - 15s - loss: 0.1656 - accuracy: 0.9375 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1549 - accuracy: 0.9421\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1585 - accuracy: 0.9399\n",
      "\n",
      "Epoch:  48\n",
      "504/504 - 15s - loss: 0.1654 - accuracy: 0.9374 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.1522 - accuracy: 0.9441\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.1542 - accuracy: 0.9424\n",
      "\n",
      "Epoch:  49\n",
      "504/504 - 18s - loss: 0.1661 - accuracy: 0.9379 - 18s/epoch - 36ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1720 - accuracy: 0.9339\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.1769 - accuracy: 0.9319\n",
      "\n",
      "Epoch:  50\n",
      "504/504 - 16s - loss: 0.1609 - accuracy: 0.9383 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1581 - accuracy: 0.9420\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1624 - accuracy: 0.9389\n",
      "\n",
      "Epoch:  51\n",
      "504/504 - 15s - loss: 0.1609 - accuracy: 0.9393 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.1564 - accuracy: 0.9411\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1583 - accuracy: 0.9409\n",
      "\n",
      "Epoch:  52\n",
      "504/504 - 17s - loss: 0.1580 - accuracy: 0.9413 - 17s/epoch - 33ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.1592 - accuracy: 0.9421\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.1625 - accuracy: 0.9392\n",
      "\n",
      "Epoch:  53\n",
      "504/504 - 23s - loss: 0.1565 - accuracy: 0.9409 - 23s/epoch - 46ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1422 - accuracy: 0.9473\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.1454 - accuracy: 0.9475\n",
      "\n",
      "Epoch:  54\n",
      "504/504 - 15s - loss: 0.1531 - accuracy: 0.9429 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1776 - accuracy: 0.9303\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1811 - accuracy: 0.9275\n",
      "\n",
      "Epoch:  55\n",
      "504/504 - 18s - loss: 0.1581 - accuracy: 0.9395 - 18s/epoch - 35ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1633 - accuracy: 0.9383\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.1655 - accuracy: 0.9382\n",
      "\n",
      "Epoch:  56\n",
      "504/504 - 15s - loss: 0.1541 - accuracy: 0.9423 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.1542 - accuracy: 0.9412\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1566 - accuracy: 0.9396\n",
      "\n",
      "Epoch:  57\n",
      "504/504 - 15s - loss: 0.1524 - accuracy: 0.9425 - 15s/epoch - 29ms/step\n",
      "for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1353 - accuracy: 0.9505\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1383 - accuracy: 0.9502\n",
      "\n",
      "Epoch:  58\n",
      "504/504 - 16s - loss: 0.1499 - accuracy: 0.9446 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.1390 - accuracy: 0.9482\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.1419 - accuracy: 0.9453\n",
      "\n",
      "Epoch:  59\n",
      "504/504 - 15s - loss: 0.1542 - accuracy: 0.9425 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.1427 - accuracy: 0.9467\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1453 - accuracy: 0.9448\n",
      "\n",
      "Epoch:  60\n",
      "504/504 - 13s - loss: 0.1512 - accuracy: 0.9431 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 6ms/step - loss: 0.1470 - accuracy: 0.9451\n",
      "for testing\n",
      "504/504 [==============================] - 3s 7ms/step - loss: 0.1487 - accuracy: 0.9438\n",
      "\n",
      "Epoch:  61\n",
      "504/504 - 13s - loss: 0.1488 - accuracy: 0.9426 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 12s 6ms/step - loss: 0.1357 - accuracy: 0.9498\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.1380 - accuracy: 0.9477\n",
      "\n",
      "Epoch:  62\n",
      "504/504 - 13s - loss: 0.1457 - accuracy: 0.9454 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.1371 - accuracy: 0.9491\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1401 - accuracy: 0.9477\n",
      "\n",
      "Epoch:  63\n",
      "504/504 - 12s - loss: 0.1424 - accuracy: 0.9468 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1436 - accuracy: 0.9444\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1452 - accuracy: 0.9444\n",
      "\n",
      "Epoch:  64\n",
      "504/504 - 11s - loss: 0.1442 - accuracy: 0.9462 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.1575 - accuracy: 0.9414\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1619 - accuracy: 0.9404\n",
      "\n",
      "Epoch:  65\n",
      "504/504 - 11s - loss: 0.1440 - accuracy: 0.9464 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.1348 - accuracy: 0.9502\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1381 - accuracy: 0.9480\n",
      "\n",
      "Epoch:  66\n",
      "504/504 - 11s - loss: 0.1458 - accuracy: 0.9440 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1433 - accuracy: 0.9451\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1466 - accuracy: 0.9443\n",
      "\n",
      "Epoch:  67\n",
      "504/504 - 13s - loss: 0.1437 - accuracy: 0.9466 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1735 - accuracy: 0.9339\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1746 - accuracy: 0.9338\n",
      "\n",
      "Epoch:  68\n",
      "504/504 - 11s - loss: 0.1422 - accuracy: 0.9469 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.1300 - accuracy: 0.9519\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1320 - accuracy: 0.9521\n",
      "\n",
      "Epoch:  69\n",
      "504/504 - 12s - loss: 0.1405 - accuracy: 0.9480 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1424 - accuracy: 0.9462\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1462 - accuracy: 0.9472\n",
      "\n",
      "Epoch:  70\n",
      "504/504 - 11s - loss: 0.1430 - accuracy: 0.9466 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1348 - accuracy: 0.9500\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1380 - accuracy: 0.9488\n",
      "\n",
      "Epoch:  71\n",
      "504/504 - 11s - loss: 0.1364 - accuracy: 0.9489 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1357 - accuracy: 0.9488\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1393 - accuracy: 0.9489\n",
      "\n",
      "Epoch:  72\n",
      "504/504 - 11s - loss: 0.1382 - accuracy: 0.9486 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1242 - accuracy: 0.9539\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1270 - accuracy: 0.9535\n",
      "\n",
      "Epoch:  73\n",
      "504/504 - 11s - loss: 0.1425 - accuracy: 0.9462 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1344 - accuracy: 0.9492\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1390 - accuracy: 0.9478\n",
      "\n",
      "Epoch:  74\n",
      "504/504 - 11s - loss: 0.1355 - accuracy: 0.9500 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.1538 - accuracy: 0.9407\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1565 - accuracy: 0.9397\n",
      "\n",
      "Epoch:  75\n",
      "504/504 - 11s - loss: 0.1357 - accuracy: 0.9496 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1377 - accuracy: 0.9481\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1429 - accuracy: 0.9460\n",
      "\n",
      "Epoch:  76\n",
      "504/504 - 13s - loss: 0.1347 - accuracy: 0.9505 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1207 - accuracy: 0.9560\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1237 - accuracy: 0.9558\n",
      "\n",
      "Epoch:  77\n",
      "504/504 - 12s - loss: 0.1376 - accuracy: 0.9486 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.1174 - accuracy: 0.9577\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1210 - accuracy: 0.9571\n",
      "\n",
      "Epoch:  78\n",
      "504/504 - 11s - loss: 0.1314 - accuracy: 0.9508 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1283 - accuracy: 0.9519\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1325 - accuracy: 0.9510\n",
      "\n",
      "Epoch:  79\n",
      "504/504 - 11s - loss: 0.1306 - accuracy: 0.9514 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1148 - accuracy: 0.9583\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1187 - accuracy: 0.9583\n",
      "\n",
      "Epoch:  80\n",
      "504/504 - 11s - loss: 0.1287 - accuracy: 0.9514 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1131 - accuracy: 0.9596\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1177 - accuracy: 0.9571\n",
      "\n",
      "Epoch:  81\n",
      "504/504 - 11s - loss: 0.1294 - accuracy: 0.9520 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1310 - accuracy: 0.9512\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1357 - accuracy: 0.9486\n",
      "\n",
      "Epoch:  82\n",
      "504/504 - 11s - loss: 0.1305 - accuracy: 0.9521 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1330 - accuracy: 0.9517\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1386 - accuracy: 0.9492\n",
      "\n",
      "Epoch:  83\n",
      "504/504 - 11s - loss: 0.1281 - accuracy: 0.9532 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1221 - accuracy: 0.9548\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1251 - accuracy: 0.9557\n",
      "\n",
      "Epoch:  84\n",
      "504/504 - 11s - loss: 0.1297 - accuracy: 0.9515 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1129 - accuracy: 0.9588\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1175 - accuracy: 0.9567\n",
      "\n",
      "Epoch:  85\n",
      "504/504 - 11s - loss: 0.1247 - accuracy: 0.9535 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1299 - accuracy: 0.9509\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1326 - accuracy: 0.9503\n",
      "\n",
      "Epoch:  86\n",
      "504/504 - 12s - loss: 0.1301 - accuracy: 0.9518 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 12s 6ms/step - loss: 0.1333 - accuracy: 0.9501\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.1393 - accuracy: 0.9487\n",
      "\n",
      "Epoch:  87\n",
      "504/504 - 16s - loss: 0.1243 - accuracy: 0.9545 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 9ms/step - loss: 0.1139 - accuracy: 0.9582\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1173 - accuracy: 0.9566\n",
      "\n",
      "Epoch:  88\n",
      "504/504 - 16s - loss: 0.1260 - accuracy: 0.9528 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 12s 6ms/step - loss: 0.1239 - accuracy: 0.9539\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.1291 - accuracy: 0.9520\n",
      "\n",
      "Epoch:  89\n",
      "504/504 - 14s - loss: 0.1262 - accuracy: 0.9535 - 14s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1204 - accuracy: 0.9550\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1236 - accuracy: 0.9541\n",
      "\n",
      "Epoch:  90\n",
      "504/504 - 11s - loss: 0.1250 - accuracy: 0.9537 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1132 - accuracy: 0.9589\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1189 - accuracy: 0.9555\n",
      "\n",
      "Epoch:  91\n",
      "504/504 - 13s - loss: 0.1226 - accuracy: 0.9553 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.1300 - accuracy: 0.9514\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1362 - accuracy: 0.9476\n",
      "\n",
      "Epoch:  92\n",
      "504/504 - 12s - loss: 0.1240 - accuracy: 0.9539 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1192 - accuracy: 0.9562\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1247 - accuracy: 0.9516\n",
      "\n",
      "Epoch:  93\n",
      "504/504 - 12s - loss: 0.1221 - accuracy: 0.9549 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1226 - accuracy: 0.9544\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1273 - accuracy: 0.9519\n",
      "\n",
      "Epoch:  94\n",
      "504/504 - 16s - loss: 0.1229 - accuracy: 0.9544 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.1473 - accuracy: 0.9429\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1534 - accuracy: 0.9420\n",
      "\n",
      "Epoch:  95\n",
      "504/504 - 13s - loss: 0.1213 - accuracy: 0.9551 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.1113 - accuracy: 0.9609\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1156 - accuracy: 0.9579\n",
      "\n",
      "Epoch:  96\n",
      "504/504 - 12s - loss: 0.1197 - accuracy: 0.9554 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1210 - accuracy: 0.9558\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1258 - accuracy: 0.9534\n",
      "\n",
      "Epoch:  97\n",
      "504/504 - 11s - loss: 0.1181 - accuracy: 0.9559 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.1043 - accuracy: 0.9632\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1086 - accuracy: 0.9621\n",
      "\n",
      "Epoch:  98\n",
      "504/504 - 12s - loss: 0.1186 - accuracy: 0.9568 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1627 - accuracy: 0.9394\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1639 - accuracy: 0.9395\n",
      "\n",
      "Epoch:  99\n",
      "504/504 - 12s - loss: 0.1186 - accuracy: 0.9560 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1073 - accuracy: 0.9615\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1137 - accuracy: 0.9589\n",
      "\n",
      "Epoch:  100\n",
      "504/504 - 11s - loss: 0.1163 - accuracy: 0.9578 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1372 - accuracy: 0.9476\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1411 - accuracy: 0.9465\n",
      "\n",
      "Epoch:  101\n",
      "504/504 - 11s - loss: 0.1163 - accuracy: 0.9569 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1110 - accuracy: 0.9593\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1170 - accuracy: 0.9567\n",
      "\n",
      "Epoch:  102\n",
      "504/504 - 11s - loss: 0.1144 - accuracy: 0.9583 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1000 - accuracy: 0.9651\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1043 - accuracy: 0.9622\n",
      "\n",
      "Epoch:  103\n",
      "504/504 - 12s - loss: 0.1190 - accuracy: 0.9564 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.1105 - accuracy: 0.9599\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.1164 - accuracy: 0.9581\n",
      "\n",
      "Epoch:  104\n",
      "504/504 - 14s - loss: 0.1128 - accuracy: 0.9591 - 14s/epoch - 28ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.1155 - accuracy: 0.9576\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1236 - accuracy: 0.9521\n",
      "\n",
      "Epoch:  105\n",
      "504/504 - 15s - loss: 0.1147 - accuracy: 0.9574 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.1026 - accuracy: 0.9633\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.1091 - accuracy: 0.9614\n",
      "\n",
      "Epoch:  106\n",
      "504/504 - 15s - loss: 0.1135 - accuracy: 0.9583 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1018 - accuracy: 0.9639\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1068 - accuracy: 0.9618\n",
      "\n",
      "Epoch:  107\n",
      "504/504 - 11s - loss: 0.1101 - accuracy: 0.9601 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.1048 - accuracy: 0.9632\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1117 - accuracy: 0.9583\n",
      "\n",
      "Epoch:  108\n",
      "504/504 - 13s - loss: 0.1120 - accuracy: 0.9591 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1099 - accuracy: 0.9591\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1144 - accuracy: 0.9573\n",
      "\n",
      "Epoch:  109\n",
      "504/504 - 13s - loss: 0.1103 - accuracy: 0.9593 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1133 - accuracy: 0.9590\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1178 - accuracy: 0.9590\n",
      "\n",
      "Epoch:  110\n",
      "504/504 - 12s - loss: 0.1136 - accuracy: 0.9587 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.1168 - accuracy: 0.9568\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1223 - accuracy: 0.9557\n",
      "\n",
      "Epoch:  111\n",
      "504/504 - 11s - loss: 0.1104 - accuracy: 0.9595 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1210 - accuracy: 0.9552\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1247 - accuracy: 0.9537\n",
      "\n",
      "Epoch:  112\n",
      "504/504 - 12s - loss: 0.1100 - accuracy: 0.9594 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1163 - accuracy: 0.9557\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1218 - accuracy: 0.9538\n",
      "\n",
      "Epoch:  113\n",
      "504/504 - 11s - loss: 0.1088 - accuracy: 0.9606 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0992 - accuracy: 0.9653\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1040 - accuracy: 0.9633\n",
      "\n",
      "Epoch:  114\n",
      "504/504 - 11s - loss: 0.1166 - accuracy: 0.9564 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0977 - accuracy: 0.9643\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1044 - accuracy: 0.9640\n",
      "\n",
      "Epoch:  115\n",
      "504/504 - 11s - loss: 0.1104 - accuracy: 0.9597 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1148 - accuracy: 0.9570\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1202 - accuracy: 0.9557\n",
      "\n",
      "Epoch:  116\n",
      "504/504 - 11s - loss: 0.1104 - accuracy: 0.9606 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.1014 - accuracy: 0.9634\n",
      "for testing\n",
      "504/504 [==============================] - 3s 7ms/step - loss: 0.1077 - accuracy: 0.9611\n",
      "\n",
      "Epoch:  117\n",
      "504/504 - 13s - loss: 0.1077 - accuracy: 0.9602 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1274 - accuracy: 0.9517\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1340 - accuracy: 0.9492\n",
      "\n",
      "Epoch:  118\n",
      "504/504 - 14s - loss: 0.1044 - accuracy: 0.9620 - 14s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.1003 - accuracy: 0.9630\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1074 - accuracy: 0.9611\n",
      "\n",
      "Epoch:  119\n",
      "504/504 - 11s - loss: 0.1089 - accuracy: 0.9604 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1149 - accuracy: 0.9573\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1196 - accuracy: 0.9557\n",
      "\n",
      "Epoch:  120\n",
      "504/504 - 13s - loss: 0.1055 - accuracy: 0.9617 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.1025 - accuracy: 0.9630\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1090 - accuracy: 0.9591\n",
      "\n",
      "Epoch:  121\n",
      "504/504 - 14s - loss: 0.1066 - accuracy: 0.9609 - 14s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 6ms/step - loss: 0.1085 - accuracy: 0.9598\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1129 - accuracy: 0.9591\n",
      "\n",
      "Epoch:  122\n",
      "504/504 - 13s - loss: 0.1070 - accuracy: 0.9616 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 12s 6ms/step - loss: 0.1011 - accuracy: 0.9642\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1051 - accuracy: 0.9612\n",
      "\n",
      "Epoch:  123\n",
      "504/504 - 16s - loss: 0.1124 - accuracy: 0.9583 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.1219 - accuracy: 0.9546\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1240 - accuracy: 0.9539\n",
      "\n",
      "Epoch:  124\n",
      "504/504 - 13s - loss: 0.1026 - accuracy: 0.9628 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.0952 - accuracy: 0.9666\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0999 - accuracy: 0.9646\n",
      "\n",
      "Epoch:  125\n",
      "504/504 - 14s - loss: 0.1050 - accuracy: 0.9616 - 14s/epoch - 28ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0969 - accuracy: 0.9643\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.1022 - accuracy: 0.9639\n",
      "\n",
      "Epoch:  126\n",
      "504/504 - 14s - loss: 0.1036 - accuracy: 0.9617 - 14s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 7ms/step - loss: 0.1298 - accuracy: 0.9509\n",
      "for testing\n",
      "504/504 [==============================] - 5s 10ms/step - loss: 0.1378 - accuracy: 0.9495\n",
      "\n",
      "Epoch:  127\n",
      "504/504 - 14s - loss: 0.1033 - accuracy: 0.9625 - 14s/epoch - 28ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0996 - accuracy: 0.9637\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1065 - accuracy: 0.9601\n",
      "\n",
      "Epoch:  128\n",
      "504/504 - 13s - loss: 0.1001 - accuracy: 0.9637 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0929 - accuracy: 0.9666\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1005 - accuracy: 0.9622\n",
      "\n",
      "Epoch:  129\n",
      "504/504 - 13s - loss: 0.0983 - accuracy: 0.9641 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0928 - accuracy: 0.9666\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0969 - accuracy: 0.9655\n",
      "\n",
      "Epoch:  130\n",
      "504/504 - 13s - loss: 0.1029 - accuracy: 0.9622 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.1142 - accuracy: 0.9570\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1205 - accuracy: 0.9554\n",
      "\n",
      "Epoch:  131\n",
      "504/504 - 13s - loss: 0.1020 - accuracy: 0.9624 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.1001 - accuracy: 0.9632\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1064 - accuracy: 0.9604\n",
      "\n",
      "Epoch:  132\n",
      "504/504 - 13s - loss: 0.1035 - accuracy: 0.9626 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.1096 - accuracy: 0.9588\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.1177 - accuracy: 0.9577\n",
      "\n",
      "Epoch:  133\n",
      "504/504 - 13s - loss: 0.1024 - accuracy: 0.9629 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0856 - accuracy: 0.9705\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0926 - accuracy: 0.9681\n",
      "\n",
      "Epoch:  134\n",
      "504/504 - 14s - loss: 0.1000 - accuracy: 0.9636 - 14s/epoch - 28ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.1291 - accuracy: 0.9504\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1391 - accuracy: 0.9466\n",
      "\n",
      "Epoch:  135\n",
      "504/504 - 13s - loss: 0.1019 - accuracy: 0.9629 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0910 - accuracy: 0.9683\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0977 - accuracy: 0.9660\n",
      "\n",
      "Epoch:  136\n",
      "504/504 - 13s - loss: 0.0983 - accuracy: 0.9648 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.0983 - accuracy: 0.9651\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1041 - accuracy: 0.9637\n",
      "\n",
      "Epoch:  137\n",
      "504/504 - 13s - loss: 0.1000 - accuracy: 0.9636 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0875 - accuracy: 0.9691\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0954 - accuracy: 0.9653\n",
      "\n",
      "Epoch:  138\n",
      "504/504 - 13s - loss: 0.0940 - accuracy: 0.9658 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 6ms/step - loss: 0.0846 - accuracy: 0.9697\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.0931 - accuracy: 0.9665\n",
      "\n",
      "Epoch:  139\n",
      "504/504 - 17s - loss: 0.0973 - accuracy: 0.9646 - 17s/epoch - 34ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0957 - accuracy: 0.9651\n",
      "for testing\n",
      "504/504 [==============================] - 3s 7ms/step - loss: 0.1019 - accuracy: 0.9638\n",
      "\n",
      "Epoch:  140\n",
      "504/504 - 17s - loss: 0.0985 - accuracy: 0.9638 - 17s/epoch - 33ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0866 - accuracy: 0.9692\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0944 - accuracy: 0.9652\n",
      "\n",
      "Epoch:  141\n",
      "504/504 - 14s - loss: 0.0940 - accuracy: 0.9666 - 14s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 7ms/step - loss: 0.0936 - accuracy: 0.9664\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1012 - accuracy: 0.9641\n",
      "\n",
      "Epoch:  142\n",
      "504/504 - 13s - loss: 0.0952 - accuracy: 0.9655 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0855 - accuracy: 0.9698\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0923 - accuracy: 0.9669\n",
      "\n",
      "Epoch:  143\n",
      "504/504 - 12s - loss: 0.0944 - accuracy: 0.9655 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0879 - accuracy: 0.9688\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0966 - accuracy: 0.9656\n",
      "\n",
      "Epoch:  144\n",
      "504/504 - 14s - loss: 0.0936 - accuracy: 0.9664 - 14s/epoch - 28ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0917 - accuracy: 0.9673\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0980 - accuracy: 0.9646\n",
      "\n",
      "Epoch:  145\n",
      "504/504 - 12s - loss: 0.0981 - accuracy: 0.9639 - 12s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.1054 - accuracy: 0.9605\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1120 - accuracy: 0.9582\n",
      "\n",
      "Epoch:  146\n",
      "504/504 - 15s - loss: 0.0966 - accuracy: 0.9650 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1011 - accuracy: 0.9626\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1064 - accuracy: 0.9599\n",
      "\n",
      "Epoch:  147\n",
      "504/504 - 11s - loss: 0.0934 - accuracy: 0.9664 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0926 - accuracy: 0.9660\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1005 - accuracy: 0.9632\n",
      "\n",
      "Epoch:  148\n",
      "504/504 - 11s - loss: 0.0945 - accuracy: 0.9654 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1111 - accuracy: 0.9585\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1190 - accuracy: 0.9552\n",
      "\n",
      "Epoch:  149\n",
      "504/504 - 11s - loss: 0.0943 - accuracy: 0.9654 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.1072 - accuracy: 0.9592\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1150 - accuracy: 0.9570\n",
      "\n",
      "Epoch:  150\n",
      "504/504 - 11s - loss: 0.0938 - accuracy: 0.9668 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0939 - accuracy: 0.9654\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1046 - accuracy: 0.9600\n",
      "\n",
      "Epoch:  151\n",
      "504/504 - 11s - loss: 0.0915 - accuracy: 0.9666 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0931 - accuracy: 0.9658\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1019 - accuracy: 0.9616\n",
      "\n",
      "Epoch:  152\n",
      "504/504 - 11s - loss: 0.0929 - accuracy: 0.9658 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0867 - accuracy: 0.9687\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0949 - accuracy: 0.9670\n",
      "\n",
      "Epoch:  153\n",
      "504/504 - 11s - loss: 0.0914 - accuracy: 0.9668 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0834 - accuracy: 0.9697\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0910 - accuracy: 0.9668\n",
      "\n",
      "Epoch:  154\n",
      "504/504 - 11s - loss: 0.0918 - accuracy: 0.9665 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1134 - accuracy: 0.9570\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1230 - accuracy: 0.9525\n",
      "\n",
      "Epoch:  155\n",
      "504/504 - 11s - loss: 0.0914 - accuracy: 0.9664 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0955 - accuracy: 0.9642\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1050 - accuracy: 0.9616\n",
      "\n",
      "Epoch:  156\n",
      "504/504 - 11s - loss: 0.0926 - accuracy: 0.9666 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0856 - accuracy: 0.9689\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0926 - accuracy: 0.9660\n",
      "\n",
      "Epoch:  157\n",
      "504/504 - 13s - loss: 0.0886 - accuracy: 0.9687 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 6ms/step - loss: 0.0826 - accuracy: 0.9713\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0910 - accuracy: 0.9696\n",
      "\n",
      "Epoch:  158\n",
      "504/504 - 15s - loss: 0.0908 - accuracy: 0.9666 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0835 - accuracy: 0.9699\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0912 - accuracy: 0.9653\n",
      "\n",
      "Epoch:  159\n",
      "504/504 - 13s - loss: 0.0907 - accuracy: 0.9668 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0905 - accuracy: 0.9668\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1011 - accuracy: 0.9619\n",
      "\n",
      "Epoch:  160\n",
      "504/504 - 11s - loss: 0.0884 - accuracy: 0.9679 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0855 - accuracy: 0.9693\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0911 - accuracy: 0.9673\n",
      "\n",
      "Epoch:  161\n",
      "504/504 - 11s - loss: 0.0874 - accuracy: 0.9688 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0836 - accuracy: 0.9694\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0933 - accuracy: 0.9658\n",
      "\n",
      "Epoch:  162\n",
      "504/504 - 12s - loss: 0.0888 - accuracy: 0.9675 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1246 - accuracy: 0.9525\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1356 - accuracy: 0.9503\n",
      "\n",
      "Epoch:  163\n",
      "504/504 - 14s - loss: 0.0892 - accuracy: 0.9679 - 14s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 6ms/step - loss: 0.0789 - accuracy: 0.9717\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0868 - accuracy: 0.9683\n",
      "\n",
      "Epoch:  164\n",
      "504/504 - 11s - loss: 0.0873 - accuracy: 0.9680 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0885 - accuracy: 0.9674\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0958 - accuracy: 0.9646\n",
      "\n",
      "Epoch:  165\n",
      "504/504 - 11s - loss: 0.0865 - accuracy: 0.9690 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0978 - accuracy: 0.9649\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1079 - accuracy: 0.9622\n",
      "\n",
      "Epoch:  166\n",
      "504/504 - 11s - loss: 0.0908 - accuracy: 0.9676 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0716 - accuracy: 0.9753\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0804 - accuracy: 0.9720\n",
      "\n",
      "Epoch:  167\n",
      "504/504 - 11s - loss: 0.0845 - accuracy: 0.9692 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0869 - accuracy: 0.9681\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0959 - accuracy: 0.9627\n",
      "\n",
      "Epoch:  168\n",
      "504/504 - 11s - loss: 0.0859 - accuracy: 0.9693 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0847 - accuracy: 0.9689\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0944 - accuracy: 0.9646\n",
      "\n",
      "Epoch:  169\n",
      "504/504 - 11s - loss: 0.0865 - accuracy: 0.9688 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0784 - accuracy: 0.9723\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0864 - accuracy: 0.9684\n",
      "\n",
      "Epoch:  170\n",
      "504/504 - 11s - loss: 0.0857 - accuracy: 0.9683 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0917 - accuracy: 0.9663\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1016 - accuracy: 0.9622\n",
      "\n",
      "Epoch:  171\n",
      "504/504 - 11s - loss: 0.0852 - accuracy: 0.9693 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0864 - accuracy: 0.9685\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0966 - accuracy: 0.9639\n",
      "\n",
      "Epoch:  172\n",
      "504/504 - 11s - loss: 0.0861 - accuracy: 0.9685 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0827 - accuracy: 0.9701\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0913 - accuracy: 0.9670\n",
      "\n",
      "Epoch:  173\n",
      "504/504 - 11s - loss: 0.0824 - accuracy: 0.9704 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0764 - accuracy: 0.9733\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0864 - accuracy: 0.9699\n",
      "\n",
      "Epoch:  174\n",
      "504/504 - 11s - loss: 0.0861 - accuracy: 0.9689 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0921 - accuracy: 0.9656\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0989 - accuracy: 0.9645\n",
      "\n",
      "Epoch:  175\n",
      "504/504 - 12s - loss: 0.0850 - accuracy: 0.9692 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0866 - accuracy: 0.9683\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0957 - accuracy: 0.9655\n",
      "\n",
      "Epoch:  176\n",
      "504/504 - 11s - loss: 0.0825 - accuracy: 0.9702 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0763 - accuracy: 0.9733\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0837 - accuracy: 0.9705\n",
      "\n",
      "Epoch:  177\n",
      "504/504 - 11s - loss: 0.0820 - accuracy: 0.9707 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0914 - accuracy: 0.9666\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1022 - accuracy: 0.9622\n",
      "\n",
      "Epoch:  178\n",
      "504/504 - 11s - loss: 0.0865 - accuracy: 0.9683 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0788 - accuracy: 0.9719\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0860 - accuracy: 0.9697\n",
      "\n",
      "Epoch:  179\n",
      "504/504 - 11s - loss: 0.0861 - accuracy: 0.9689 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0806 - accuracy: 0.9704\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0892 - accuracy: 0.9674\n",
      "\n",
      "Epoch:  180\n",
      "504/504 - 11s - loss: 0.0827 - accuracy: 0.9700 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0706 - accuracy: 0.9755\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0800 - accuracy: 0.9723\n",
      "\n",
      "Epoch:  181\n",
      "504/504 - 11s - loss: 0.0824 - accuracy: 0.9697 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0782 - accuracy: 0.9726\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0863 - accuracy: 0.9711\n",
      "\n",
      "Epoch:  182\n",
      "504/504 - 11s - loss: 0.0816 - accuracy: 0.9703 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0688 - accuracy: 0.9765\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0790 - accuracy: 0.9731\n",
      "\n",
      "Epoch:  183\n",
      "504/504 - 11s - loss: 0.0801 - accuracy: 0.9715 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0872 - accuracy: 0.9676\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0970 - accuracy: 0.9634\n",
      "\n",
      "Epoch:  184\n",
      "504/504 - 11s - loss: 0.0812 - accuracy: 0.9709 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0870 - accuracy: 0.9674\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0950 - accuracy: 0.9650\n",
      "\n",
      "Epoch:  185\n",
      "504/504 - 11s - loss: 0.0785 - accuracy: 0.9714 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0888 - accuracy: 0.9680\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0996 - accuracy: 0.9653\n",
      "\n",
      "Epoch:  186\n",
      "504/504 - 11s - loss: 0.0803 - accuracy: 0.9713 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0826 - accuracy: 0.9693\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0926 - accuracy: 0.9651\n",
      "\n",
      "Epoch:  187\n",
      "504/504 - 11s - loss: 0.0821 - accuracy: 0.9700 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0784 - accuracy: 0.9720\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0858 - accuracy: 0.9697\n",
      "\n",
      "Epoch:  188\n",
      "504/504 - 11s - loss: 0.0783 - accuracy: 0.9720 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0734 - accuracy: 0.9743\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0837 - accuracy: 0.9696\n",
      "\n",
      "Epoch:  189\n",
      "504/504 - 11s - loss: 0.0807 - accuracy: 0.9710 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0835 - accuracy: 0.9693\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0936 - accuracy: 0.9650\n",
      "\n",
      "Epoch:  190\n",
      "504/504 - 11s - loss: 0.0771 - accuracy: 0.9725 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0754 - accuracy: 0.9735\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0861 - accuracy: 0.9695\n",
      "\n",
      "Epoch:  191\n",
      "504/504 - 11s - loss: 0.0787 - accuracy: 0.9714 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0740 - accuracy: 0.9732\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0849 - accuracy: 0.9695\n",
      "\n",
      "Epoch:  192\n",
      "504/504 - 11s - loss: 0.0774 - accuracy: 0.9723 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0720 - accuracy: 0.9738\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0813 - accuracy: 0.9715\n",
      "\n",
      "Epoch:  193\n",
      "504/504 - 11s - loss: 0.0823 - accuracy: 0.9699 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0652 - accuracy: 0.9774\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0754 - accuracy: 0.9727\n",
      "\n",
      "Epoch:  194\n",
      "504/504 - 11s - loss: 0.0760 - accuracy: 0.9722 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0773 - accuracy: 0.9723\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0871 - accuracy: 0.9679\n",
      "\n",
      "Epoch:  195\n",
      "504/504 - 11s - loss: 0.0806 - accuracy: 0.9708 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0774 - accuracy: 0.9725\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0884 - accuracy: 0.9691\n",
      "\n",
      "Epoch:  196\n",
      "504/504 - 11s - loss: 0.0809 - accuracy: 0.9706 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0668 - accuracy: 0.9766\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0765 - accuracy: 0.9735\n",
      "\n",
      "Epoch:  197\n",
      "504/504 - 11s - loss: 0.0780 - accuracy: 0.9720 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0786 - accuracy: 0.9712\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0902 - accuracy: 0.9666\n",
      "\n",
      "Epoch:  198\n",
      "504/504 - 11s - loss: 0.0748 - accuracy: 0.9729 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0691 - accuracy: 0.9759\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0810 - accuracy: 0.9715\n",
      "\n",
      "Epoch:  199\n",
      "504/504 - 11s - loss: 0.0775 - accuracy: 0.9726 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0768 - accuracy: 0.9716\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0867 - accuracy: 0.9688\n",
      "\n",
      "Epoch:  200\n",
      "504/504 - 11s - loss: 0.0773 - accuracy: 0.9719 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0792 - accuracy: 0.9718\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0898 - accuracy: 0.9685\n",
      "\n",
      "Epoch:  201\n",
      "504/504 - 11s - loss: 0.0761 - accuracy: 0.9728 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0643 - accuracy: 0.9777\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0722 - accuracy: 0.9754\n",
      "\n",
      "Epoch:  202\n",
      "504/504 - 11s - loss: 0.0768 - accuracy: 0.9723 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0665 - accuracy: 0.9766\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0765 - accuracy: 0.9722\n",
      "\n",
      "Epoch:  203\n",
      "504/504 - 11s - loss: 0.0725 - accuracy: 0.9736 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0619 - accuracy: 0.9784\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0718 - accuracy: 0.9746\n",
      "\n",
      "Epoch:  204\n",
      "504/504 - 11s - loss: 0.0735 - accuracy: 0.9738 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0600 - accuracy: 0.9798\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0693 - accuracy: 0.9762\n",
      "\n",
      "Epoch:  205\n",
      "504/504 - 11s - loss: 0.0764 - accuracy: 0.9725 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0848 - accuracy: 0.9679\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0949 - accuracy: 0.9639\n",
      "\n",
      "Epoch:  206\n",
      "504/504 - 11s - loss: 0.0762 - accuracy: 0.9723 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0695 - accuracy: 0.9753\n",
      "for testing\n",
      "504/504 [==============================] - 3s 7ms/step - loss: 0.0779 - accuracy: 0.9724\n",
      "\n",
      "Epoch:  207\n",
      "504/504 - 13s - loss: 0.0764 - accuracy: 0.9721 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0683 - accuracy: 0.9759\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0788 - accuracy: 0.9723\n",
      "\n",
      "Epoch:  208\n",
      "504/504 - 16s - loss: 0.0776 - accuracy: 0.9722 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 18s 9ms/step - loss: 0.0648 - accuracy: 0.9775\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0745 - accuracy: 0.9732\n",
      "\n",
      "Epoch:  209\n",
      "504/504 - 16s - loss: 0.0736 - accuracy: 0.9737 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0685 - accuracy: 0.9755\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0782 - accuracy: 0.9710\n",
      "\n",
      "Epoch:  210\n",
      "504/504 - 12s - loss: 0.0738 - accuracy: 0.9731 - 12s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0707 - accuracy: 0.9750\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0781 - accuracy: 0.9721\n",
      "\n",
      "Epoch:  211\n",
      "504/504 - 11s - loss: 0.0752 - accuracy: 0.9721 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0822 - accuracy: 0.9704\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0907 - accuracy: 0.9666\n",
      "\n",
      "Epoch:  212\n",
      "504/504 - 11s - loss: 0.0723 - accuracy: 0.9738 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0600 - accuracy: 0.9799\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0689 - accuracy: 0.9763\n",
      "\n",
      "Epoch:  213\n",
      "504/504 - 11s - loss: 0.0732 - accuracy: 0.9741 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0664 - accuracy: 0.9763\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0768 - accuracy: 0.9720\n",
      "\n",
      "Epoch:  214\n",
      "504/504 - 15s - loss: 0.0728 - accuracy: 0.9742 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0768 - accuracy: 0.9717\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0871 - accuracy: 0.9688\n",
      "\n",
      "Epoch:  215\n",
      "504/504 - 11s - loss: 0.0716 - accuracy: 0.9741 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0657 - accuracy: 0.9771\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0769 - accuracy: 0.9727\n",
      "\n",
      "Epoch:  216\n",
      "504/504 - 12s - loss: 0.0743 - accuracy: 0.9729 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0953 - accuracy: 0.9627\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1061 - accuracy: 0.9599\n",
      "\n",
      "Epoch:  217\n",
      "504/504 - 12s - loss: 0.0713 - accuracy: 0.9742 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0823 - accuracy: 0.9696\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0908 - accuracy: 0.9670\n",
      "\n",
      "Epoch:  218\n",
      "504/504 - 12s - loss: 0.0729 - accuracy: 0.9737 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0611 - accuracy: 0.9793\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0731 - accuracy: 0.9754\n",
      "\n",
      "Epoch:  219\n",
      "504/504 - 13s - loss: 0.0738 - accuracy: 0.9736 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0683 - accuracy: 0.9751\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0786 - accuracy: 0.9727\n",
      "\n",
      "Epoch:  220\n",
      "504/504 - 11s - loss: 0.0724 - accuracy: 0.9743 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1032 - accuracy: 0.9634\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1162 - accuracy: 0.9611\n",
      "\n",
      "Epoch:  221\n",
      "504/504 - 11s - loss: 0.0712 - accuracy: 0.9737 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0593 - accuracy: 0.9800\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0694 - accuracy: 0.9756\n",
      "\n",
      "Epoch:  222\n",
      "504/504 - 11s - loss: 0.0694 - accuracy: 0.9750 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0935 - accuracy: 0.9648\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1044 - accuracy: 0.9634\n",
      "\n",
      "Epoch:  223\n",
      "504/504 - 11s - loss: 0.0697 - accuracy: 0.9752 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0582 - accuracy: 0.9801\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0707 - accuracy: 0.9756\n",
      "\n",
      "Epoch:  224\n",
      "504/504 - 11s - loss: 0.0726 - accuracy: 0.9739 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0834 - accuracy: 0.9705\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0935 - accuracy: 0.9674\n",
      "\n",
      "Epoch:  225\n",
      "504/504 - 11s - loss: 0.0718 - accuracy: 0.9736 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0694 - accuracy: 0.9751\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0800 - accuracy: 0.9720\n",
      "\n",
      "Epoch:  226\n",
      "504/504 - 11s - loss: 0.0702 - accuracy: 0.9753 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0704 - accuracy: 0.9755\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0811 - accuracy: 0.9710\n",
      "\n",
      "Epoch:  227\n",
      "504/504 - 11s - loss: 0.0687 - accuracy: 0.9759 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0640 - accuracy: 0.9772\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0763 - accuracy: 0.9729\n",
      "\n",
      "Epoch:  228\n",
      "504/504 - 12s - loss: 0.0681 - accuracy: 0.9757 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0820 - accuracy: 0.9688\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0899 - accuracy: 0.9661\n",
      "\n",
      "Epoch:  229\n",
      "504/504 - 12s - loss: 0.0702 - accuracy: 0.9748 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0579 - accuracy: 0.9799\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0699 - accuracy: 0.9731\n",
      "\n",
      "Epoch:  230\n",
      "504/504 - 12s - loss: 0.0680 - accuracy: 0.9757 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.0587 - accuracy: 0.9800\n",
      "for testing\n",
      "504/504 [==============================] - 3s 7ms/step - loss: 0.0686 - accuracy: 0.9767\n",
      "\n",
      "Epoch:  231\n",
      "504/504 - 15s - loss: 0.0670 - accuracy: 0.9766 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.0565 - accuracy: 0.9809\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0676 - accuracy: 0.9767\n",
      "\n",
      "Epoch:  232\n",
      "504/504 - 13s - loss: 0.0685 - accuracy: 0.9755 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0706 - accuracy: 0.9742\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0841 - accuracy: 0.9702\n",
      "\n",
      "Epoch:  233\n",
      "504/504 - 14s - loss: 0.0696 - accuracy: 0.9749 - 14s/epoch - 28ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0574 - accuracy: 0.9803\n",
      "for testing\n",
      "504/504 [==============================] - 3s 7ms/step - loss: 0.0660 - accuracy: 0.9770\n",
      "\n",
      "Epoch:  234\n",
      "504/504 - 13s - loss: 0.0687 - accuracy: 0.9753 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0664 - accuracy: 0.9766\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.0794 - accuracy: 0.9711\n",
      "\n",
      "Epoch:  235\n",
      "504/504 - 14s - loss: 0.0682 - accuracy: 0.9754 - 14s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0773 - accuracy: 0.9718\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0933 - accuracy: 0.9665\n",
      "\n",
      "Epoch:  236\n",
      "504/504 - 11s - loss: 0.0676 - accuracy: 0.9759 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0734 - accuracy: 0.9733\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0869 - accuracy: 0.9689\n",
      "\n",
      "Epoch:  237\n",
      "504/504 - 12s - loss: 0.0675 - accuracy: 0.9756 - 12s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0660 - accuracy: 0.9751\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0768 - accuracy: 0.9732\n",
      "\n",
      "Epoch:  238\n",
      "504/504 - 11s - loss: 0.0675 - accuracy: 0.9762 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0806 - accuracy: 0.9704\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0929 - accuracy: 0.9657\n",
      "\n",
      "Epoch:  239\n",
      "504/504 - 12s - loss: 0.0653 - accuracy: 0.9764 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0592 - accuracy: 0.9788\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0724 - accuracy: 0.9756\n",
      "\n",
      "Epoch:  240\n",
      "504/504 - 11s - loss: 0.0679 - accuracy: 0.9754 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0647 - accuracy: 0.9766\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0769 - accuracy: 0.9725\n",
      "\n",
      "Epoch:  241\n",
      "504/504 - 13s - loss: 0.0673 - accuracy: 0.9759 - 13s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0559 - accuracy: 0.9805\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0657 - accuracy: 0.9764\n",
      "\n",
      "Epoch:  242\n",
      "504/504 - 11s - loss: 0.0707 - accuracy: 0.9738 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0585 - accuracy: 0.9791\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0705 - accuracy: 0.9755\n",
      "\n",
      "Epoch:  243\n",
      "504/504 - 12s - loss: 0.0673 - accuracy: 0.9761 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0759 - accuracy: 0.9729\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0883 - accuracy: 0.9678\n",
      "\n",
      "Epoch:  244\n",
      "504/504 - 15s - loss: 0.0660 - accuracy: 0.9763 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0777 - accuracy: 0.9714\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0882 - accuracy: 0.9683\n",
      "\n",
      "Epoch:  245\n",
      "504/504 - 11s - loss: 0.0666 - accuracy: 0.9761 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0662 - accuracy: 0.9766\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0785 - accuracy: 0.9731\n",
      "\n",
      "Epoch:  246\n",
      "504/504 - 12s - loss: 0.0657 - accuracy: 0.9764 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0597 - accuracy: 0.9788\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0704 - accuracy: 0.9752\n",
      "\n",
      "Epoch:  247\n",
      "504/504 - 11s - loss: 0.0668 - accuracy: 0.9755 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0567 - accuracy: 0.9803\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0667 - accuracy: 0.9763\n",
      "\n",
      "Epoch:  248\n",
      "504/504 - 11s - loss: 0.0655 - accuracy: 0.9764 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0623 - accuracy: 0.9782\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0745 - accuracy: 0.9746\n",
      "\n",
      "Epoch:  249\n",
      "504/504 - 11s - loss: 0.0684 - accuracy: 0.9757 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0603 - accuracy: 0.9789\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0718 - accuracy: 0.9748\n",
      "\n",
      "Epoch:  250\n",
      "504/504 - 11s - loss: 0.0652 - accuracy: 0.9765 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0739 - accuracy: 0.9723\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0845 - accuracy: 0.9701\n",
      "\n",
      "Epoch:  251\n",
      "504/504 - 11s - loss: 0.0632 - accuracy: 0.9773 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0616 - accuracy: 0.9783\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0710 - accuracy: 0.9753\n",
      "\n",
      "Epoch:  252\n",
      "504/504 - 11s - loss: 0.0706 - accuracy: 0.9744 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0753 - accuracy: 0.9725\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0864 - accuracy: 0.9686\n",
      "\n",
      "Epoch:  253\n",
      "504/504 - 11s - loss: 0.0668 - accuracy: 0.9763 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0571 - accuracy: 0.9803\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0687 - accuracy: 0.9752\n",
      "\n",
      "Epoch:  254\n",
      "504/504 - 11s - loss: 0.0629 - accuracy: 0.9780 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0555 - accuracy: 0.9807\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0682 - accuracy: 0.9763\n",
      "\n",
      "Epoch:  255\n",
      "504/504 - 11s - loss: 0.0671 - accuracy: 0.9758 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0622 - accuracy: 0.9780\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0751 - accuracy: 0.9748\n",
      "\n",
      "Epoch:  256\n",
      "504/504 - 11s - loss: 0.0636 - accuracy: 0.9774 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0521 - accuracy: 0.9818\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0630 - accuracy: 0.9785\n",
      "\n",
      "Epoch:  257\n",
      "504/504 - 11s - loss: 0.0635 - accuracy: 0.9768 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.1108 - accuracy: 0.9582\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.1270 - accuracy: 0.9547\n",
      "\n",
      "Epoch:  258\n",
      "504/504 - 15s - loss: 0.0635 - accuracy: 0.9774 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.0560 - accuracy: 0.9805\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0699 - accuracy: 0.9750\n",
      "\n",
      "Epoch:  259\n",
      "504/504 - 11s - loss: 0.0640 - accuracy: 0.9774 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1001 - accuracy: 0.9641\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.1118 - accuracy: 0.9609\n",
      "\n",
      "Epoch:  260\n",
      "504/504 - 11s - loss: 0.0673 - accuracy: 0.9755 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0579 - accuracy: 0.9796\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0706 - accuracy: 0.9745\n",
      "\n",
      "Epoch:  261\n",
      "504/504 - 12s - loss: 0.0650 - accuracy: 0.9764 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0637 - accuracy: 0.9768\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0745 - accuracy: 0.9724\n",
      "\n",
      "Epoch:  262\n",
      "504/504 - 11s - loss: 0.0659 - accuracy: 0.9766 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0556 - accuracy: 0.9803\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0683 - accuracy: 0.9747\n",
      "\n",
      "Epoch:  263\n",
      "504/504 - 11s - loss: 0.0614 - accuracy: 0.9785 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.0557 - accuracy: 0.9802\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0694 - accuracy: 0.9757\n",
      "\n",
      "Epoch:  264\n",
      "504/504 - 12s - loss: 0.0607 - accuracy: 0.9785 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0500 - accuracy: 0.9826\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0615 - accuracy: 0.9789\n",
      "\n",
      "Epoch:  265\n",
      "504/504 - 11s - loss: 0.0597 - accuracy: 0.9784 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0602 - accuracy: 0.9785\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0719 - accuracy: 0.9745\n",
      "\n",
      "Epoch:  266\n",
      "504/504 - 11s - loss: 0.0634 - accuracy: 0.9774 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0617 - accuracy: 0.9779\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0739 - accuracy: 0.9732\n",
      "\n",
      "Epoch:  267\n",
      "504/504 - 11s - loss: 0.0650 - accuracy: 0.9764 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0505 - accuracy: 0.9826\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0616 - accuracy: 0.9796\n",
      "\n",
      "Epoch:  268\n",
      "504/504 - 11s - loss: 0.0616 - accuracy: 0.9782 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0738 - accuracy: 0.9739\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0854 - accuracy: 0.9684\n",
      "\n",
      "Epoch:  269\n",
      "504/504 - 11s - loss: 0.0626 - accuracy: 0.9775 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0582 - accuracy: 0.9792\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0729 - accuracy: 0.9754\n",
      "\n",
      "Epoch:  270\n",
      "504/504 - 15s - loss: 0.0606 - accuracy: 0.9784 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0520 - accuracy: 0.9818\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0639 - accuracy: 0.9779\n",
      "\n",
      "Epoch:  271\n",
      "504/504 - 15s - loss: 0.0602 - accuracy: 0.9791 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0702 - accuracy: 0.9738\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0807 - accuracy: 0.9708\n",
      "\n",
      "Epoch:  272\n",
      "504/504 - 15s - loss: 0.0636 - accuracy: 0.9771 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.1100 - accuracy: 0.9568\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.1240 - accuracy: 0.9536\n",
      "\n",
      "Epoch:  273\n",
      "504/504 - 15s - loss: 0.0614 - accuracy: 0.9773 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0626 - accuracy: 0.9768\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0763 - accuracy: 0.9728\n",
      "\n",
      "Epoch:  274\n",
      "504/504 - 15s - loss: 0.0590 - accuracy: 0.9791 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0580 - accuracy: 0.9798\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0722 - accuracy: 0.9740\n",
      "\n",
      "Epoch:  275\n",
      "504/504 - 15s - loss: 0.0619 - accuracy: 0.9782 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0605 - accuracy: 0.9785\n",
      "for testing\n",
      "504/504 [==============================] - 4s 9ms/step - loss: 0.0752 - accuracy: 0.9723\n",
      "\n",
      "Epoch:  276\n",
      "504/504 - 19s - loss: 0.0606 - accuracy: 0.9779 - 19s/epoch - 37ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0725 - accuracy: 0.9732\n",
      "for testing\n",
      "504/504 [==============================] - 5s 9ms/step - loss: 0.0828 - accuracy: 0.9699\n",
      "\n",
      "Epoch:  277\n",
      "504/504 - 15s - loss: 0.0625 - accuracy: 0.9771 - 15s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0570 - accuracy: 0.9794\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0685 - accuracy: 0.9753\n",
      "\n",
      "Epoch:  278\n",
      "504/504 - 16s - loss: 0.0574 - accuracy: 0.9796 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0659 - accuracy: 0.9761\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0783 - accuracy: 0.9717\n",
      "\n",
      "Epoch:  279\n",
      "504/504 - 25s - loss: 0.0616 - accuracy: 0.9782 - 25s/epoch - 49ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0720 - accuracy: 0.9726\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0852 - accuracy: 0.9685\n",
      "\n",
      "Epoch:  280\n",
      "504/504 - 15s - loss: 0.0610 - accuracy: 0.9779 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0548 - accuracy: 0.9807\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0665 - accuracy: 0.9748\n",
      "\n",
      "Epoch:  281\n",
      "504/504 - 15s - loss: 0.0603 - accuracy: 0.9781 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0504 - accuracy: 0.9821\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0607 - accuracy: 0.9781\n",
      "\n",
      "Epoch:  282\n",
      "504/504 - 15s - loss: 0.0614 - accuracy: 0.9780 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0565 - accuracy: 0.9801\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0704 - accuracy: 0.9752\n",
      "\n",
      "Epoch:  283\n",
      "504/504 - 15s - loss: 0.0614 - accuracy: 0.9780 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0612 - accuracy: 0.9776\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0709 - accuracy: 0.9743\n",
      "\n",
      "Epoch:  284\n",
      "504/504 - 15s - loss: 0.0579 - accuracy: 0.9788 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0557 - accuracy: 0.9802\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0691 - accuracy: 0.9749\n",
      "\n",
      "Epoch:  285\n",
      "504/504 - 15s - loss: 0.0635 - accuracy: 0.9770 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0586 - accuracy: 0.9792\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0702 - accuracy: 0.9743\n",
      "\n",
      "Epoch:  286\n",
      "504/504 - 15s - loss: 0.0562 - accuracy: 0.9798 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0527 - accuracy: 0.9818\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0660 - accuracy: 0.9772\n",
      "\n",
      "Epoch:  287\n",
      "504/504 - 14s - loss: 0.0601 - accuracy: 0.9782 - 14s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0673 - accuracy: 0.9756\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0800 - accuracy: 0.9727\n",
      "\n",
      "Epoch:  288\n",
      "504/504 - 15s - loss: 0.0578 - accuracy: 0.9793 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0522 - accuracy: 0.9811\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0644 - accuracy: 0.9768\n",
      "\n",
      "Epoch:  289\n",
      "504/504 - 15s - loss: 0.0601 - accuracy: 0.9788 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0623 - accuracy: 0.9773\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0744 - accuracy: 0.9723\n",
      "\n",
      "Epoch:  290\n",
      "504/504 - 15s - loss: 0.0596 - accuracy: 0.9785 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0590 - accuracy: 0.9786\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0713 - accuracy: 0.9743\n",
      "\n",
      "Epoch:  291\n",
      "504/504 - 15s - loss: 0.0617 - accuracy: 0.9778 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0518 - accuracy: 0.9822\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0639 - accuracy: 0.9784\n",
      "\n",
      "Epoch:  292\n",
      "504/504 - 14s - loss: 0.0569 - accuracy: 0.9796 - 14s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0494 - accuracy: 0.9828\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0619 - accuracy: 0.9796\n",
      "\n",
      "Epoch:  293\n",
      "504/504 - 15s - loss: 0.0577 - accuracy: 0.9791 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0509 - accuracy: 0.9824\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0635 - accuracy: 0.9779\n",
      "\n",
      "Epoch:  294\n",
      "504/504 - 15s - loss: 0.0573 - accuracy: 0.9794 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0519 - accuracy: 0.9818\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0623 - accuracy: 0.9790\n",
      "\n",
      "Epoch:  295\n",
      "504/504 - 14s - loss: 0.0607 - accuracy: 0.9779 - 14s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0618 - accuracy: 0.9776\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0757 - accuracy: 0.9731\n",
      "\n",
      "Epoch:  296\n",
      "504/504 - 15s - loss: 0.0572 - accuracy: 0.9799 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 9ms/step - loss: 0.0496 - accuracy: 0.9828\n",
      "for testing\n",
      "504/504 [==============================] - 5s 9ms/step - loss: 0.0642 - accuracy: 0.9776\n",
      "\n",
      "Epoch:  297\n",
      "504/504 - 16s - loss: 0.0589 - accuracy: 0.9785 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0553 - accuracy: 0.9809\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0657 - accuracy: 0.9769\n",
      "\n",
      "Epoch:  298\n",
      "504/504 - 16s - loss: 0.0568 - accuracy: 0.9798 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 8ms/step - loss: 0.0502 - accuracy: 0.9825\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0633 - accuracy: 0.9775\n",
      "\n",
      "Epoch:  299\n",
      "504/504 - 17s - loss: 0.0598 - accuracy: 0.9785 - 17s/epoch - 33ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 8ms/step - loss: 0.0682 - accuracy: 0.9749\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0790 - accuracy: 0.9710\n",
      "\n",
      "Epoch:  300\n",
      "504/504 - 15s - loss: 0.0586 - accuracy: 0.9793 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0530 - accuracy: 0.9812\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0641 - accuracy: 0.9767\n",
      "\n",
      "Epoch:  301\n",
      "504/504 - 15s - loss: 0.0578 - accuracy: 0.9794 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0631 - accuracy: 0.9770\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0769 - accuracy: 0.9726\n",
      "\n",
      "Epoch:  302\n",
      "504/504 - 18s - loss: 0.0585 - accuracy: 0.9789 - 18s/epoch - 37ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 8ms/step - loss: 0.0693 - accuracy: 0.9753\n",
      "for testing\n",
      "504/504 [==============================] - 5s 9ms/step - loss: 0.0833 - accuracy: 0.9701\n",
      "\n",
      "Epoch:  303\n",
      "504/504 - 17s - loss: 0.0581 - accuracy: 0.9788 - 17s/epoch - 34ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0567 - accuracy: 0.9794\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0703 - accuracy: 0.9733\n",
      "\n",
      "Epoch:  304\n",
      "504/504 - 17s - loss: 0.0577 - accuracy: 0.9793 - 17s/epoch - 33ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 24s 12ms/step - loss: 0.0620 - accuracy: 0.9776\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0751 - accuracy: 0.9730\n",
      "\n",
      "Epoch:  305\n",
      "504/504 - 15s - loss: 0.0578 - accuracy: 0.9788 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 8ms/step - loss: 0.0463 - accuracy: 0.9841\n",
      "for testing\n",
      "504/504 [==============================] - 4s 9ms/step - loss: 0.0591 - accuracy: 0.9801\n",
      "\n",
      "Epoch:  306\n",
      "504/504 - 19s - loss: 0.0546 - accuracy: 0.9804 - 19s/epoch - 37ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 9ms/step - loss: 0.0635 - accuracy: 0.9760\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0734 - accuracy: 0.9728\n",
      "\n",
      "Epoch:  307\n",
      "504/504 - 15s - loss: 0.0567 - accuracy: 0.9795 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0597 - accuracy: 0.9783\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0703 - accuracy: 0.9759\n",
      "\n",
      "Epoch:  308\n",
      "504/504 - 17s - loss: 0.0553 - accuracy: 0.9799 - 17s/epoch - 34ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0461 - accuracy: 0.9832\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0580 - accuracy: 0.9792\n",
      "\n",
      "Epoch:  309\n",
      "504/504 - 15s - loss: 0.0572 - accuracy: 0.9792 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 8ms/step - loss: 0.0460 - accuracy: 0.9839\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0591 - accuracy: 0.9798\n",
      "\n",
      "Epoch:  310\n",
      "504/504 - 15s - loss: 0.0556 - accuracy: 0.9801 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0457 - accuracy: 0.9841\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0591 - accuracy: 0.9798\n",
      "\n",
      "Epoch:  311\n",
      "504/504 - 15s - loss: 0.0565 - accuracy: 0.9798 - 15s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 19s 9ms/step - loss: 0.0569 - accuracy: 0.9795\n",
      "for testing\n",
      "504/504 [==============================] - 5s 10ms/step - loss: 0.0733 - accuracy: 0.9737\n",
      "\n",
      "Epoch:  312\n",
      "504/504 - 20s - loss: 0.0571 - accuracy: 0.9795 - 20s/epoch - 40ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 8ms/step - loss: 0.0516 - accuracy: 0.9817\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0655 - accuracy: 0.9769\n",
      "\n",
      "Epoch:  313\n",
      "504/504 - 15s - loss: 0.0567 - accuracy: 0.9798 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0799 - accuracy: 0.9702\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0964 - accuracy: 0.9658\n",
      "\n",
      "Epoch:  314\n",
      "504/504 - 15s - loss: 0.0575 - accuracy: 0.9792 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0539 - accuracy: 0.9802\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0701 - accuracy: 0.9741\n",
      "\n",
      "Epoch:  315\n",
      "504/504 - 15s - loss: 0.0555 - accuracy: 0.9800 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 19s 9ms/step - loss: 0.0694 - accuracy: 0.9738\n",
      "for testing\n",
      "504/504 [==============================] - 4s 9ms/step - loss: 0.0831 - accuracy: 0.9695\n",
      "\n",
      "Epoch:  316\n",
      "504/504 - 18s - loss: 0.0557 - accuracy: 0.9798 - 18s/epoch - 36ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 8ms/step - loss: 0.0671 - accuracy: 0.9749\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0781 - accuracy: 0.9706\n",
      "\n",
      "Epoch:  317\n",
      "504/504 - 18s - loss: 0.0551 - accuracy: 0.9803 - 18s/epoch - 35ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 18s 9ms/step - loss: 0.0526 - accuracy: 0.9817\n",
      "for testing\n",
      "504/504 [==============================] - 4s 9ms/step - loss: 0.0672 - accuracy: 0.9750\n",
      "\n",
      "Epoch:  318\n",
      "504/504 - 15s - loss: 0.0537 - accuracy: 0.9807 - 15s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 19s 9ms/step - loss: 0.0463 - accuracy: 0.9839\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0612 - accuracy: 0.9783\n",
      "\n",
      "Epoch:  319\n",
      "504/504 - 17s - loss: 0.0571 - accuracy: 0.9794 - 17s/epoch - 33ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 8ms/step - loss: 0.0594 - accuracy: 0.9779\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0728 - accuracy: 0.9735\n",
      "\n",
      "Epoch:  320\n",
      "504/504 - 18s - loss: 0.0557 - accuracy: 0.9799 - 18s/epoch - 35ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 18s 9ms/step - loss: 0.0472 - accuracy: 0.9831\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0606 - accuracy: 0.9790\n",
      "\n",
      "Epoch:  321\n",
      "504/504 - 16s - loss: 0.0537 - accuracy: 0.9813 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 18s 9ms/step - loss: 0.0477 - accuracy: 0.9832\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0615 - accuracy: 0.9780\n",
      "\n",
      "Epoch:  322\n",
      "504/504 - 17s - loss: 0.0522 - accuracy: 0.9814 - 17s/epoch - 34ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0470 - accuracy: 0.9834\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0627 - accuracy: 0.9790\n",
      "\n",
      "Epoch:  323\n",
      "504/504 - 15s - loss: 0.0536 - accuracy: 0.9807 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0425 - accuracy: 0.9858\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0558 - accuracy: 0.9807\n",
      "\n",
      "Epoch:  324\n",
      "504/504 - 15s - loss: 0.0562 - accuracy: 0.9799 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0551 - accuracy: 0.9798\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0697 - accuracy: 0.9750\n",
      "\n",
      "Epoch:  325\n",
      "504/504 - 17s - loss: 0.0548 - accuracy: 0.9800 - 17s/epoch - 33ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0513 - accuracy: 0.9815\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0641 - accuracy: 0.9767\n",
      "\n",
      "Epoch:  326\n",
      "504/504 - 15s - loss: 0.0545 - accuracy: 0.9801 - 15s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0590 - accuracy: 0.9791\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0727 - accuracy: 0.9750\n",
      "\n",
      "Epoch:  327\n",
      "504/504 - 16s - loss: 0.0535 - accuracy: 0.9809 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 18s 9ms/step - loss: 0.0507 - accuracy: 0.9818\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0659 - accuracy: 0.9769\n",
      "\n",
      "Epoch:  328\n",
      "504/504 - 26s - loss: 0.0563 - accuracy: 0.9800 - 26s/epoch - 52ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0656 - accuracy: 0.9757\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0790 - accuracy: 0.9720\n",
      "\n",
      "Epoch:  329\n",
      "504/504 - 15s - loss: 0.0516 - accuracy: 0.9821 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0457 - accuracy: 0.9839\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0577 - accuracy: 0.9791\n",
      "\n",
      "Epoch:  330\n",
      "504/504 - 15s - loss: 0.0544 - accuracy: 0.9804 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0541 - accuracy: 0.9809\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0665 - accuracy: 0.9766\n",
      "\n",
      "Epoch:  331\n",
      "504/504 - 15s - loss: 0.0579 - accuracy: 0.9793 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0460 - accuracy: 0.9833\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0610 - accuracy: 0.9785\n",
      "\n",
      "Epoch:  332\n",
      "504/504 - 15s - loss: 0.0524 - accuracy: 0.9812 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0465 - accuracy: 0.9836\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0609 - accuracy: 0.9786\n",
      "\n",
      "Epoch:  333\n",
      "504/504 - 15s - loss: 0.0531 - accuracy: 0.9810 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0441 - accuracy: 0.9846\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0578 - accuracy: 0.9798\n",
      "\n",
      "Epoch:  334\n",
      "504/504 - 16s - loss: 0.0528 - accuracy: 0.9811 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 9ms/step - loss: 0.0520 - accuracy: 0.9813\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0659 - accuracy: 0.9761\n",
      "\n",
      "Epoch:  335\n",
      "504/504 - 16s - loss: 0.0513 - accuracy: 0.9819 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0478 - accuracy: 0.9837\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0600 - accuracy: 0.9789\n",
      "\n",
      "Epoch:  336\n",
      "504/504 - 15s - loss: 0.0539 - accuracy: 0.9807 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0581 - accuracy: 0.9779\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0737 - accuracy: 0.9738\n",
      "\n",
      "Epoch:  337\n",
      "504/504 - 15s - loss: 0.0533 - accuracy: 0.9809 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0430 - accuracy: 0.9852\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0560 - accuracy: 0.9804\n",
      "\n",
      "Epoch:  338\n",
      "504/504 - 16s - loss: 0.0522 - accuracy: 0.9819 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0533 - accuracy: 0.9814\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0670 - accuracy: 0.9761\n",
      "\n",
      "Epoch:  339\n",
      "504/504 - 16s - loss: 0.0524 - accuracy: 0.9811 - 16s/epoch - 32ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0444 - accuracy: 0.9846\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0591 - accuracy: 0.9790\n",
      "\n",
      "Epoch:  340\n",
      "504/504 - 16s - loss: 0.0505 - accuracy: 0.9821 - 16s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0480 - accuracy: 0.9829\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0603 - accuracy: 0.9787\n",
      "\n",
      "Epoch:  341\n",
      "504/504 - 15s - loss: 0.0503 - accuracy: 0.9819 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0520 - accuracy: 0.9810\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0658 - accuracy: 0.9774\n",
      "\n",
      "Epoch:  342\n",
      "504/504 - 15s - loss: 0.0530 - accuracy: 0.9808 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0642 - accuracy: 0.9758\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0812 - accuracy: 0.9703\n",
      "\n",
      "Epoch:  343\n",
      "504/504 - 15s - loss: 0.0554 - accuracy: 0.9804 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0416 - accuracy: 0.9859\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0559 - accuracy: 0.9812\n",
      "\n",
      "Epoch:  344\n",
      "504/504 - 15s - loss: 0.0512 - accuracy: 0.9816 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0406 - accuracy: 0.9865\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0533 - accuracy: 0.9819\n",
      "\n",
      "Epoch:  345\n",
      "504/504 - 15s - loss: 0.0513 - accuracy: 0.9812 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0648 - accuracy: 0.9758\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0780 - accuracy: 0.9712\n",
      "\n",
      "Epoch:  346\n",
      "504/504 - 15s - loss: 0.0512 - accuracy: 0.9815 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0440 - accuracy: 0.9844\n",
      "for testing\n",
      "504/504 [==============================] - 4s 8ms/step - loss: 0.0553 - accuracy: 0.9807\n",
      "\n",
      "Epoch:  347\n",
      "504/504 - 15s - loss: 0.0521 - accuracy: 0.9811 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 16s 8ms/step - loss: 0.0422 - accuracy: 0.9853\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0554 - accuracy: 0.9806\n",
      "\n",
      "Epoch:  348\n",
      "504/504 - 15s - loss: 0.0497 - accuracy: 0.9824 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0440 - accuracy: 0.9844\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0560 - accuracy: 0.9807\n",
      "\n",
      "Epoch:  349\n",
      "504/504 - 15s - loss: 0.0516 - accuracy: 0.9816 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0397 - accuracy: 0.9862\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0532 - accuracy: 0.9813\n",
      "\n",
      "Epoch:  350\n",
      "504/504 - 15s - loss: 0.0515 - accuracy: 0.9816 - 15s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 8ms/step - loss: 0.0573 - accuracy: 0.9791\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.0728 - accuracy: 0.9741\n",
      "\n",
      "Epoch:  351\n",
      "504/504 - 15s - loss: 0.0515 - accuracy: 0.9814 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0547 - accuracy: 0.9808\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0688 - accuracy: 0.9767\n",
      "\n",
      "Epoch:  352\n",
      "504/504 - 12s - loss: 0.0490 - accuracy: 0.9830 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0515 - accuracy: 0.9814\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0699 - accuracy: 0.9750\n",
      "\n",
      "Epoch:  353\n",
      "504/504 - 11s - loss: 0.0509 - accuracy: 0.9818 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 4ms/step - loss: 0.0485 - accuracy: 0.9831\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0619 - accuracy: 0.9791\n",
      "\n",
      "Epoch:  354\n",
      "504/504 - 12s - loss: 0.0532 - accuracy: 0.9812 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0417 - accuracy: 0.9859\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0541 - accuracy: 0.9814\n",
      "\n",
      "Epoch:  355\n",
      "504/504 - 13s - loss: 0.0490 - accuracy: 0.9825 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 6ms/step - loss: 0.0435 - accuracy: 0.9850\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0556 - accuracy: 0.9800\n",
      "\n",
      "Epoch:  356\n",
      "504/504 - 11s - loss: 0.0526 - accuracy: 0.9812 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0628 - accuracy: 0.9773\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0791 - accuracy: 0.9723\n",
      "\n",
      "Epoch:  357\n",
      "504/504 - 11s - loss: 0.0518 - accuracy: 0.9815 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0616 - accuracy: 0.9775\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0756 - accuracy: 0.9745\n",
      "\n",
      "Epoch:  358\n",
      "504/504 - 11s - loss: 0.0516 - accuracy: 0.9811 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0450 - accuracy: 0.9841\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0574 - accuracy: 0.9808\n",
      "\n",
      "Epoch:  359\n",
      "504/504 - 11s - loss: 0.0478 - accuracy: 0.9827 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0689 - accuracy: 0.9753\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0799 - accuracy: 0.9721\n",
      "\n",
      "Epoch:  360\n",
      "504/504 - 11s - loss: 0.0477 - accuracy: 0.9828 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0459 - accuracy: 0.9835\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0607 - accuracy: 0.9774\n",
      "\n",
      "Epoch:  361\n",
      "504/504 - 11s - loss: 0.0502 - accuracy: 0.9817 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0564 - accuracy: 0.9792\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0742 - accuracy: 0.9737\n",
      "\n",
      "Epoch:  362\n",
      "504/504 - 12s - loss: 0.0487 - accuracy: 0.9826 - 12s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0614 - accuracy: 0.9768\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0730 - accuracy: 0.9738\n",
      "\n",
      "Epoch:  363\n",
      "504/504 - 13s - loss: 0.0516 - accuracy: 0.9811 - 13s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0451 - accuracy: 0.9845\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0584 - accuracy: 0.9787\n",
      "\n",
      "Epoch:  364\n",
      "504/504 - 14s - loss: 0.0520 - accuracy: 0.9812 - 14s/epoch - 28ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 6ms/step - loss: 0.0388 - accuracy: 0.9869\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.0516 - accuracy: 0.9821\n",
      "\n",
      "Epoch:  365\n",
      "504/504 - 15s - loss: 0.0463 - accuracy: 0.9834 - 15s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0391 - accuracy: 0.9866\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0510 - accuracy: 0.9829\n",
      "\n",
      "Epoch:  366\n",
      "504/504 - 14s - loss: 0.0509 - accuracy: 0.9817 - 14s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0529 - accuracy: 0.9800\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0659 - accuracy: 0.9766\n",
      "\n",
      "Epoch:  367\n",
      "504/504 - 13s - loss: 0.0484 - accuracy: 0.9828 - 13s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.0411 - accuracy: 0.9851\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0568 - accuracy: 0.9801\n",
      "\n",
      "Epoch:  368\n",
      "504/504 - 13s - loss: 0.0495 - accuracy: 0.9823 - 13s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0429 - accuracy: 0.9851\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0560 - accuracy: 0.9802\n",
      "\n",
      "Epoch:  369\n",
      "504/504 - 11s - loss: 0.0489 - accuracy: 0.9824 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0631 - accuracy: 0.9759\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0763 - accuracy: 0.9724\n",
      "\n",
      "Epoch:  370\n",
      "504/504 - 11s - loss: 0.0509 - accuracy: 0.9811 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0566 - accuracy: 0.9794\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0720 - accuracy: 0.9764\n",
      "\n",
      "Epoch:  371\n",
      "504/504 - 11s - loss: 0.0494 - accuracy: 0.9823 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0385 - accuracy: 0.9866\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0519 - accuracy: 0.9824\n",
      "\n",
      "Epoch:  372\n",
      "504/504 - 11s - loss: 0.0462 - accuracy: 0.9835 - 11s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0669 - accuracy: 0.9758\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0809 - accuracy: 0.9725\n",
      "\n",
      "Epoch:  373\n",
      "504/504 - 11s - loss: 0.0526 - accuracy: 0.9810 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0468 - accuracy: 0.9834\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0584 - accuracy: 0.9791\n",
      "\n",
      "Epoch:  374\n",
      "504/504 - 11s - loss: 0.0512 - accuracy: 0.9816 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0394 - accuracy: 0.9867\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0511 - accuracy: 0.9832\n",
      "\n",
      "Epoch:  375\n",
      "504/504 - 11s - loss: 0.0471 - accuracy: 0.9825 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0354 - accuracy: 0.9882\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0495 - accuracy: 0.9831\n",
      "\n",
      "Epoch:  376\n",
      "504/504 - 11s - loss: 0.0464 - accuracy: 0.9835 - 11s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0437 - accuracy: 0.9841\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0572 - accuracy: 0.9804\n",
      "\n",
      "Epoch:  377\n",
      "504/504 - 11s - loss: 0.0476 - accuracy: 0.9828 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0463 - accuracy: 0.9838\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0602 - accuracy: 0.9789\n",
      "\n",
      "Epoch:  378\n",
      "504/504 - 11s - loss: 0.0515 - accuracy: 0.9816 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0429 - accuracy: 0.9845\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0565 - accuracy: 0.9802\n",
      "\n",
      "Epoch:  379\n",
      "504/504 - 11s - loss: 0.0462 - accuracy: 0.9837 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0402 - accuracy: 0.9861\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0550 - accuracy: 0.9811\n",
      "\n",
      "Epoch:  380\n",
      "504/504 - 11s - loss: 0.0459 - accuracy: 0.9834 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0686 - accuracy: 0.9752\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0840 - accuracy: 0.9712\n",
      "\n",
      "Epoch:  381\n",
      "504/504 - 11s - loss: 0.0506 - accuracy: 0.9816 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0510 - accuracy: 0.9816\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0656 - accuracy: 0.9780\n",
      "\n",
      "Epoch:  382\n",
      "504/504 - 11s - loss: 0.0482 - accuracy: 0.9831 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0499 - accuracy: 0.9820\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0627 - accuracy: 0.9777\n",
      "\n",
      "Epoch:  383\n",
      "504/504 - 11s - loss: 0.0488 - accuracy: 0.9827 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0472 - accuracy: 0.9825\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0629 - accuracy: 0.9779\n",
      "\n",
      "Epoch:  384\n",
      "504/504 - 11s - loss: 0.0464 - accuracy: 0.9832 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0406 - accuracy: 0.9857\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0534 - accuracy: 0.9815\n",
      "\n",
      "Epoch:  385\n",
      "504/504 - 11s - loss: 0.0476 - accuracy: 0.9829 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0534 - accuracy: 0.9807\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0674 - accuracy: 0.9763\n",
      "\n",
      "Epoch:  386\n",
      "504/504 - 11s - loss: 0.0478 - accuracy: 0.9824 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0381 - accuracy: 0.9869\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0509 - accuracy: 0.9828\n",
      "\n",
      "Epoch:  387\n",
      "504/504 - 11s - loss: 0.0481 - accuracy: 0.9829 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0604 - accuracy: 0.9780\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0780 - accuracy: 0.9727\n",
      "\n",
      "Epoch:  388\n",
      "504/504 - 11s - loss: 0.0514 - accuracy: 0.9810 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0525 - accuracy: 0.9804\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0689 - accuracy: 0.9755\n",
      "\n",
      "Epoch:  389\n",
      "504/504 - 11s - loss: 0.0488 - accuracy: 0.9822 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0621 - accuracy: 0.9775\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0788 - accuracy: 0.9733\n",
      "\n",
      "Epoch:  390\n",
      "504/504 - 11s - loss: 0.0488 - accuracy: 0.9824 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0459 - accuracy: 0.9835\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0616 - accuracy: 0.9781\n",
      "\n",
      "Epoch:  391\n",
      "504/504 - 11s - loss: 0.0467 - accuracy: 0.9834 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0382 - accuracy: 0.9865\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0517 - accuracy: 0.9826\n",
      "\n",
      "Epoch:  392\n",
      "504/504 - 11s - loss: 0.0461 - accuracy: 0.9835 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0478 - accuracy: 0.9820\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0650 - accuracy: 0.9767\n",
      "\n",
      "Epoch:  393\n",
      "504/504 - 11s - loss: 0.0466 - accuracy: 0.9834 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0571 - accuracy: 0.9788\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0749 - accuracy: 0.9732\n",
      "\n",
      "Epoch:  394\n",
      "504/504 - 11s - loss: 0.0463 - accuracy: 0.9837 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.0479 - accuracy: 0.9838\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0611 - accuracy: 0.9790\n",
      "\n",
      "Epoch:  395\n",
      "504/504 - 11s - loss: 0.0493 - accuracy: 0.9823 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0425 - accuracy: 0.9847\n",
      "for testing\n",
      "504/504 [==============================] - 5s 9ms/step - loss: 0.0560 - accuracy: 0.9800\n",
      "\n",
      "Epoch:  396\n",
      "504/504 - 12s - loss: 0.0484 - accuracy: 0.9833 - 12s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0470 - accuracy: 0.9826\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0639 - accuracy: 0.9774\n",
      "\n",
      "Epoch:  397\n",
      "504/504 - 11s - loss: 0.0459 - accuracy: 0.9838 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0421 - accuracy: 0.9846\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0552 - accuracy: 0.9799\n",
      "\n",
      "Epoch:  398\n",
      "504/504 - 11s - loss: 0.0485 - accuracy: 0.9827 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0362 - accuracy: 0.9873\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0503 - accuracy: 0.9820\n",
      "\n",
      "Epoch:  399\n",
      "504/504 - 11s - loss: 0.0454 - accuracy: 0.9840 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0478 - accuracy: 0.9827\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0645 - accuracy: 0.9767\n",
      "\n",
      "Epoch:  400\n",
      "504/504 - 11s - loss: 0.0454 - accuracy: 0.9838 - 11s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0382 - accuracy: 0.9864\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0541 - accuracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for x in range(400):\n",
    "    print(\"\\nEpoch: \",x+1)\n",
    "    history = model.fit(X_train ,y_train ,batch_size=BATCH_SIZE ,epochs=1, verbose=2, shuffle=True)\n",
    "    if history_dataframe.size == 0:\n",
    "        history_dataframe = pd.DataFrame.from_dict(history.history)\n",
    "    else:\n",
    "        history_dataframe = pd.concat([history_dataframe,pd.DataFrame.from_dict(history.history)],axis=0,ignore_index=True)\n",
    "    \n",
    "    print(\"for training\")\n",
    "    train_eva.append(model.evaluate(X_train,y_train))\n",
    "    print(\"for testing\")\n",
    "    test_eva.append(model.evaluate(X_test, y_test))\n",
    "    \n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec37c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.189218</td>\n",
       "      <td>0.435004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.975108</td>\n",
       "      <td>0.605066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.724955</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614860</td>\n",
       "      <td>0.739676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.545470</td>\n",
       "      <td>0.775561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.048405</td>\n",
       "      <td>0.983305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.045910</td>\n",
       "      <td>0.983801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.048493</td>\n",
       "      <td>0.982701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.045375</td>\n",
       "      <td>0.984034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.045354</td>\n",
       "      <td>0.983832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy\n",
       "0    1.189218  0.435004\n",
       "1    0.975108  0.605066\n",
       "2    0.724955  0.693700\n",
       "3    0.614860  0.739676\n",
       "4    0.545470  0.775561\n",
       "..        ...       ...\n",
       "395  0.048405  0.983305\n",
       "396  0.045910  0.983801\n",
       "397  0.048493  0.982701\n",
       "398  0.045375  0.984034\n",
       "399  0.045354  0.983832\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c8463e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eva_df = pd.DataFrame(train_eva,columns=['loss','accuracy'])\n",
    "test_eva_df = pd.DataFrame(test_eva,columns=['loss','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cc95506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(df,title,xlabel,ylabel,values=['loss'],legends=[]):\n",
    "    \n",
    "    for value in values:\n",
    "        epoch_count = range(1, len(df.index) + 1)\n",
    "        plt.plot(epoch_count, df[value].tolist())\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    if legends==[]:\n",
    "        legends = values\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35c781fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8zUlEQVR4nO3dd3hTZf8G8DtNujfdhVLKbNnQMsoQBGUjQwUXiIKKoIjoq+JCcaC+P3lREVzgREBFFBWBInvIKC2z7NK96d5Nzu+PpzlJ2hQKpEmb3p/r6mVzcpI8p6nk7vdZCkmSJBARERFZCRtLN4CIiIjIlBhuiIiIyKow3BAREZFVYbghIiIiq8JwQ0RERFaF4YaIiIisCsMNERERWRWGGyIiIrIqDDdERERkVRhuiKhRUSgU9fratWvXLb9WSUkJ3njjDZM8FxE1HipLN4CISN/BgwcNbr/11lvYuXMnduzYYXC8c+fOt/xaJSUlePPNNwEAQ4cOveXnI6LGgeGGiBqV/v37G9z28fGBjY1NreNERHVhtxQRNTkVFRV4++23ERoaCnt7e/j4+OCRRx5BVlaWwXk7duzA0KFD4eXlBUdHR7Ru3Rp33303SkpKcOXKFfj4+AAA3nzzTbm7a8aMGRa4IiIyJVZuiKhJ0Wg0mDBhAvbu3YsXXngBAwYMQEJCAhYtWoShQ4fi6NGjcHR0xJUrVzB27FgMHjwYq1evhoeHB1JSUrBlyxZUVFQgICAAW7ZswahRozBz5kzMmjULAOTAQ0RNF8MNETUpP/30E7Zs2YINGzZg8uTJ8vEePXqgT58++Oabb/Dkk08iOjoaZWVl+O9//4sePXrI5z3wwAPy9+Hh4QCAVq1asduLyIqwW4qImpQ///wTHh4eGD9+PKqqquSvnj17wt/fX5751LNnT9jZ2eHxxx/Ht99+i8uXL1u24URkNgw3RNSkZGRkIC8vD3Z2drC1tTX4Sk9PR3Z2NgCgXbt22L59O3x9fTF37ly0a9cO7dq1w0cffWThKyCihsZuKSJqUry9veHl5YUtW7YYvd/V1VX+fvDgwRg8eDDUajWOHj2KTz75BPPnz4efnx/uu+8+czWZiMyM4YaImpRx48Zh3bp1UKvV6NevX70eo1Qq0a9fP4SGhmLNmjU4duwY7rvvPtjb2wMASktLG7LJRGRmDDdE1KTcd999WLNmDcaMGYNnnnkGffv2ha2tLZKTk7Fz505MmDABkyZNwmeffYYdO3Zg7NixaN26NcrKyrB69WoAwB133AFAVHmCg4Px+++/Y/jw4WjRogW8vb3Rpk0bC14hEd0qjrkhoiZFqVRi06ZNePnll/Hrr79i0qRJmDhxIt577z04ODigW7duAMSA4qqqKixatAijR4/GtGnTkJWVhU2bNmHEiBHy861atQpOTk6466670KdPH7zxxhsWujIiMhWFJEmSpRtBREREZCqs3BAREZFVYbghIiIiq8JwQ0RERFaF4YaIiIisCsMNERERWRWGGyIiIrIqzW4RP41Gg9TUVLi6ukKhUFi6OURERFQPkiShsLAQgYGBsLG5dm2m2YWb1NRUBAUFWboZREREdBOSkpLQqlWra57T7MKNdlO9pKQkuLm5Wbg1REREVB8FBQUICgoy2By3Ls0u3Gi7otzc3BhuiIiImpj6DCnhgGIiIiKyKgw3REREZFUYboiIiMiqNLsxN/WlVqtRWVlp6WbQDbC1tYVSqbR0M4iIyMIYbmqQJAnp6enIy8uzdFPoJnh4eMDf359rGBERNWMMNzVog42vry+cnJz4IdlESJKEkpISZGZmAgACAgIs3CIiIrIUhhs9arVaDjZeXl6Wbg7dIEdHRwBAZmYmfH192UVFRNRMcUCxHu0YGycnJwu3hG6W9r3jeCkiouaL4cYIdkU1XXzviIiI4YaIiIisCsONlRg6dCjmz59v6WYQERFZHMMNERERWRXOljIRSZJQqZYASLBTcZYOERGRpbByYyJVagln0wtwLr3I0k1Bbm4upk+fDk9PTzg5OWH06NG4cOGCfH9CQgLGjx8PT09PODs7o0uXLti8ebP82AcffBA+Pj5wdHREhw4d8PXXX1vqUoiIiG4YKzfXIUkSSivV1z2vUq1BWfV5xeWVJpm142irvKnnmTFjBi5cuIBNmzbBzc0NL774IsaMGYMzZ87A1tYWc+fORUVFBfbs2QNnZ2ecOXMGLi4uAIDXXnsNZ86cwd9//w1vb29cvHgRpaWlt3wtRERE5sJwcx2llWp0fn2rRV77zOKRcLK7sbdIG2r279+PAQMGAADWrFmDoKAg/Pbbb7j33nuRmJiIu+++G926dQMAtG3bVn58YmIievXqhYiICABAmzZtTHMxREREZsJuKSsTFxcHlUqFfv36yce8vLzQqVMnxMXFAQDmzZuHt99+GwMHDsSiRYtw4sQJ+dwnn3wS69atQ8+ePfHCCy/gwIEDZr8GIiKiW8HKzXU42ipxZvHI656n1kiISysAAHQOcIONjWm6pW6UJEl1Htd2cc2aNQsjR47EX3/9hW3btmHJkiX48MMP8fTTT2P06NFISEjAX3/9he3bt2P48OGYO3cu/u///u+WroWIiMhcWLm5DoVCASc71XW/nO1UcLBVwsFWCUc7Zb0ec72vmxlv07lzZ1RVVeHQoUPysZycHJw/fx5hYWHysaCgIMyePRu//vornnvuOXz55ZfyfT4+PpgxYwZ++OEHLFu2DF988cWt/RCJiIjMiJUbU9HLIXUUT8yiQ4cOmDBhAh577DF8/vnncHV1xUsvvYSWLVtiwoQJAID58+dj9OjR6NixI3Jzc7Fjxw45+Lz++usIDw9Hly5dUF5ejj///NMgFBERETV2rNyYSGPa0ejrr79GeHg4xo0bh8jISEiShM2bN8PW1haA2P187ty5CAsLw6hRo9CpUyesWLECAGBnZ4eFCxeie/fuuO2226BUKrFu3TpLXg4REdENUUh1DdKwUgUFBXB3d0d+fj7c3NwM7isrK0N8fDxCQkLg4OBww899IjkPABAW4AZbJXOjJdzqe0hERI3TtT6/a+InsAkpqus3zSsuEhERNS4MNyakG//LdENERGQpDDcNgNGGiIjIchhuTEiu3DDdEBERWQzDTQNgtiEiIrIchhsTkgcUW7gdREREzRnDjQnpuqUYb4iIiCyF4aYBMNoQERFZDsONCbFwQ0REZHkMNyZ0E/tcEhERkYkx3JgUBxQTERFZGsONCXFAsaHKykpLN4GIiJohhpsGYKlos2XLFgwaNAgeHh7w8vLCuHHjcOnSJfn+5ORk3HfffWjRogWcnZ0RERGBQ4cOyfdv2rQJERERcHBwgLe3NyZPnizfp1Ao8Ntvvxm8noeHB7755hsAwJUrV6BQKPDTTz9h6NChcHBwwA8//ICcnBzcf//9aNWqFZycnNCtWzesXbvW4Hk0Gg3ef/99tG/fHvb29mjdujXeeecdAMCwYcPw1FNPGZyfk5MDe3t77NixwxQ/NiIisjIMN9cjSUBFcb2+bCpLoKgsgVRev/Ov+3WDFaDi4mIsWLAAR44cwT///AMbGxtMmjQJGo0GRUVFGDJkCFJTU7Fp0yYcP34cL7zwAjQaDQDgr7/+wuTJkzF27FjExMTgn3/+QURExA3/uF588UXMmzcPcXFxGDlyJMrKyhAeHo4///wTp06dwuOPP45p06YZhKqFCxfi/fffx2uvvYYzZ87gxx9/hJ+fHwBg1qxZ+PHHH1FeXi6fv2bNGgQGBuL222+/4fYREZH1U0hS8+pDudaW6WVlZYiPj0dISAgcHBzEwYpi4N1AC7QUwMupgJ3zTT88KysLvr6+OHnyJA4cOIDnn38eV65cQYsWLWqdO2DAALRt2xY//PCD0edSKBTYuHEjJk6cKB/z8PDAsmXLMGPGDFy5cgUhISFYtmwZnnnmmWu2a+zYsQgLC8P//d//obCwED4+Pli+fDlmzZpV69zy8nIEBgZi5cqVmDJlCgCgV69emDhxIhYtWlTrfKPvIRERNXnX+vyuiZUbK3Lp0iU88MADaNu2Ldzc3BASEgIASExMRGxsLHr16mU02ABAbGwshg8ffsttqFntUavVeOedd9C9e3d4eXnBxcUF27ZtQ2JiIgAgLi4O5eXldb62vb09HnroIaxevVpu5/HjxzFjxoxbbisREVknlaUb0OjZOokKSj1czipGcUUVWns6wt3JzjSvfQPGjx+PoKAgfPnllwgMDIRGo0HXrl1RUVEBR0fHaz72evcrFArULPIZGzDs7GxYafrwww/xv//9D8uWLUO3bt3g7OyM+fPno6Kiol6vC4iuqZ49eyI5ORmrV6/G8OHDERwcfN3HERFR82TRys2ePXswfvx4BAYGGh2waszu3bsRHh4OBwcHtG3bFp999lnDNlKhEF1D9fiS7Jwg2TpBquf51/26gYVzcnJyEBcXh1dffRXDhw9HWFgYcnNz5fu7d++O2NhYXL161ejju3fvjn/++afO5/fx8UFaWpp8+8KFCygpKbluu/bu3YsJEybgoYceQo8ePdC2bVtcuHBBvr9Dhw5wdHS85mt369YNERER+PLLL/Hjjz/i0Ucfve7rEhFR82XRcFNcXIwePXpg+fLl9To/Pj4eY8aMweDBgxETE4OXX34Z8+bNw4YNGxq4pfUjzwS3wGt7enrCy8sLX3zxBS5evIgdO3ZgwYIF8v33338//P39MXHiROzfvx+XL1/Ghg0bcPDgQQDAokWLsHbtWixatAhxcXE4efIkPvjgA/nxw4YNw/Lly3Hs2DEcPXoUs2fPhq2t7XXb1b59e0RFReHAgQOIi4vDE088gfT0dPl+BwcHvPjii3jhhRfw3Xff4dKlS/j333+xatUqg+eZNWsW3nvvPajVakyaNOlWf1xERGTNpEYCgLRx48ZrnvPCCy9IoaGhBseeeOIJqX///vV+nfz8fAmAlJ+fX+u+0tJS6cyZM1JpaWm9n0/f5awi6XhSrpRTVH5Tj79VUVFRUlhYmGRvby91795d2rVrl8HP9cqVK9Ldd98tubm5SU5OTlJERIR06NAh+fEbNmyQevbsKdnZ2Une3t7S5MmT5ftSUlKkESNGSM7OzlKHDh2kzZs3S+7u7tLXX38tSZIkxcfHSwCkmJgYgzbl5ORIEyZMkFxcXCRfX1/p1VdflaZPny5NmDBBPketVktvv/22FBwcLNna2kqtW7eW3n33XYPnKSwslJycnKQ5c+Zc82dwq+8hERE1Ttf6/K6p0cyWMjYbp6bbbrsNvXr1wkcffSQf27hxI6ZMmYKSkhKjlYTy8nKDacQFBQUICgqq/2ypG3AluxgFZZVo5emEFs4mGHNDsqSkJLRp0wZHjhxB79696zyPs6WIiKyT1c6WSk9Pl9c/0fLz80NVVRWys7ONPmbJkiVwd3eXv4KCghq8nY0kL1qFyspKJCYm4sUXX0T//v2vGWyIiIiAJhZuAFHh0acNEjWPay1cuBD5+fnyV1JSUgO2rcGeutnav38/goODER0d3fCDx4mIyCo0qang/v7+BoNRASAzMxMqlQpeXl5GH2Nvbw97e3tzNE/Guo3pDB06lJUwIiK6IU2qchMZGYmoqCiDY9u2bUNERES9Zu40NIV2V3B+FhMREVmMRcNNUVERYmNjERsbC0BM9Y6NjZVXr124cCGmT58unz979mwkJCRgwYIFiIuLw+rVq7Fq1So8//zzJm3XzVYKdN1STDeWwioPEVk1SQIy44CEg8bvrywDKqrXIKsoBk78BBxfD5TmGj/fmIJUIC+x9vGrl4Gq8trHa9Ko6/9aDcSi3VJHjx412PxQuy7Lww8/jG+++QZpaWly0AGAkJAQbN68Gc8++yw+/fRTBAYG4uOPP8bdd99tkvZoqz8lJSX1Wjm3Lvx4tRztwoKNoZJH1ChJkvhLTJKA7PNAi3aA0gwfBZIE5FwCnL0B++qZLjb1/Pu6sgw4/zfQcTRg6wCU5QOpsUDpVcCjNdAyvP5tUCiA/GQgZg3QsjfQ4c7a56mrDH8mmWeBqjIgsKd4DnUlEP0N0OEOwKMNoKkCVNUzZJOOAFteAq5eAka9B+QnAblXxH3FOYDSFpiwHHBwr37uOGDba4C9KzDwGfEa6aeAmB+AIS8ATnpb5pQVAOseAK7sFbfHLRMLvpblA53GAE5ewJe3A8XZwOy9wE8PA0n/inPdWorz1eXAhSjxencuBmyU4v6rl4E/FwBBfYHDX4qA8kys7vVjfgB+nyveg/vXivuTD4v7/LsBuz8AYr4HBi0ALmwDOowABjxtscGojWYquLlcbypZWloa8vLy4OvrCycnpzoHKhuTnleK/LJKeLvYw8vFvON8mjtJklBSUoLMzEx4eHggICDA0k2ixkCjBipLxD/kdZ6jAUpyABcf07ymJAEp0eKv31YR4q/nogygzaC6H1NeJM5p0bb2h0FxNqCyr/sairMBpR0Q9br4IL1vre6DVl9pHrBhpgg0D/0KnN8KbHsFGLoQ6D8H2Pc/wC0Q6PtY/a7z3N/AifXA4OfEh5vW5V3iw75VX/FaLcOBrHPAT9OB7HOArbNon70bMPZD4OI/gFuAaF/vaUB5IeDsI9qi9cd8IPproM8sIGw8sH4aUF5QfacCeGC9eM0OIw1DiUYD7PkvkHEK8OsK7H4P8AwBcuMBSQMo7cUHtas/4OgpQoWNElj/kHi/irPEh35Zvni+dsOB1BjAM1j8FxA/e3UF4NVBtO/8FuDyTnGfykGEopoGPgO0vR2oLAV+flg8HhBb7sw9DHzaV/ze9nwQaNUHCLkNcA8Cvh0HJB0y/n60jAA6jgR2viNu26jEzwQAXAOAwrTajxkwT7xHSlsg6bD4HdQ3bhkQ8Yj4uXw1XHctU74DDizXhRt7N733A7pjcw8Zvo+36EamgjPc1CBJEtLT05GXl3fDz51bUoHicjXcHFVwc2DlwBI8PDzg7+9/Q6GULESSgIqiawePa6koEf8wB/bUHVNXAr8+BihsgHbDgL0fir+aH/4TaDNQnFOYDjh4iAoAAPzzFrD3/8Rf2f2f1P11D4jHx+8BRrwtPoB+fkT8NR39jfjwHvWu7rU1avHBuPNdYPf74phbS/GBUJIDTNso2gQARVmArSPw3V2iLXkJQM5FEW4e+hVoESI+bKJeBxIPAipHoP1wwMUPaHe7aNfg54GDy8X9+ka+C6SdAO5YBMSuAXy7AFf2Aac2AEXVEzKcvESbtFq0E5UGAHhiD+DfHcg4DWSeAdoOFR/Sm54SwWDYa+K5fpstAoKtE9B5gqgqqMuBi9vF87j4iQ/LiEeBC9uB/ERAoQSka3RZuLcWAc3ZG5j1j/j5XdoB/HivuF9ZHdrUFeK+ghTDx494B7B3EcHBozXw+1NA7A91v56+67XtVrS/AwgeKK5FW3XR126Y+J3IOCl+dyWN4f0O7oBbKyDztPj+wV9EWDQWWGp6cIOoxux6T1RW7Jzr9zgt10CgMFXXDm3QA0RQVVcAmkrxfhRni98BhQ3wwE/Gq2K3gOHmGur7w1Gr1UY3hryWj7afx6bjqXioXzAeGRRyq02lG2RrawulUmnpZjRt+h/sdUk4COz/CBj9HuDZxvg5FSWAXfXGr8nRgFdb8SGuUAA73gHi/hAfQslHgAkrgF4PGrahokj8w6nttshPEQEk7k/ApxNw34/A9xNFhWTkEvGPb9hdQNZZYNPTtdsT0EOMOSgvFP918hLhBBDdHVrDFwFHVwOh44ABTwH/6yKOK5SiClPzr+bQceKv7/ICIP0k0Hs6cPgL4z8Trw7AkweA42uBP56p/qDIq31ezwdFF8P6h2DyTm7XQPGc1/pwCxsP+IQBe6q3X6lZfQgdJ6ozFUWimpBfz+U1WrQDZm4TgfTKPl2FAQDaDK79oe/VXvzcs88ZaeNdwN1fiQ/aj3sDFYWG97v4A13vBv791PC4Xzdg2KuAX2cRhD8bJCokNrbiA1pfywhRvelxn6h87P0/8Xur7WJSOYoQ6t8NGP66+H9C+977hIqKxaUd4vaTBwC/LqKStKK/4TW1HgBM/010Fa1/ENc19QfxHiX+C+z/GBj0rAhw0d/orjHjpPg+eBDwyF+Gj5ckIPUY8OUw3bE7F4swGnKb+N2EAriw1fBxbi2BR7cAax/QPf+ET8X1X9opfvclDRC1SATxrpOvfy03iOHmGm7kh3Oj3th0Gt8cuIK5t7fDf0aGmvS5qZlKOiI+uDtPMDxeM4Ro1EDJ1fp3reQmiH+IPNuI51FXApvmAQn7gCnfi2pIaqy439EDOPGz+ACPeFSUp1OixYfw2KXAvqXVJfERomvhl0eAhAPiH+H8JODPZ8U/4BVFQPqJ2m3xCQPmHBTtqKoQoSVhv/gg7j0dOPx5/QZD1vyL10Yl2qUdc3CjIp8SlZGbMehZwLezqCIBuq4L99aiglGTWyvxAbnxccPjnScCo5aIQHB6o+4DTN+MzeJao16v+1q92gN3viWqPgWpwJ7/A9JiqweIVgeXOxeLDyZIuqpLTfohILA3MDMKiN8lqkxOXqILLqA7cGSV6Mrw7wac+0tUf8Z/LLpztLa9CsTvBaZ+L6os+hUvW2egslh3rouf6P7a9z8RbEYt0Y0VubgdOPRF7Q9jrTH/J4L1iXWim8VDbyHX3Cvi/x07ZzGA9vhaEW6HvgwMfdH48+VcEr/zA+aJoK1Vmge8X319/ecAvmEiaPt3F+NftLIviMpXzwfF2J/AXqKKqNGI/7fST4hxK/auQNRrQPf7xP15SeLfgfCHjbfps8GAdwdgxp+iG+9CFDBzq2hHTZIEfNxTXP+It8XYmJp+fkRULbtPAdKOAyPeEl2MxTmiiufUAhj/Sf3HTZkAw801NGS4eevPM1i1Lx5PDGmLhaON/EKR9aosEx/knm1E//X1JB8Vfz0HD9QN2Mu5JP76i3hE/INXWQa8U70i97DXgHObxT+I6SeAkxuA1v1E+b2qFDi3BUg5Kv5q6zJB/ONq52z8taO/qf7rDEDvh4HxH4kP4ZM/i2NuLYFu9wL7l4kPmSH/Aba/CUASf7Wf/VOcp7AR7b+yV3RNTNsoApKxv7Rr8mwjPrj3LxO3O08QlYzLu0UXjTFB/UX15HqBw8VfdK1IGjGOQvszBIDJX4m/KuP+ENerrRZEzBR/wR9fW/v5Ok8Azvxe9+v5dhbdCtp2ObYA5p8QP7s/5oljHUYAP00z/vg5/4qqhsoOWD1K183UMkL8paz/+3T4S+Do1+JD7MxvosJzf3WbD64Ati6s/fwP/yHCpbFBwyd+Bn6dJbquntwvPiC1f5UDwPxTokKw8x2g+1Tx4bb+IRHUHtkMBA+o++ei0YjAWpAqqhjXqwhWlonfh3bDxe/H1oVAUSYwcQXg3kqcc63K4hsekCtdNioRaPrPFt139e2m1mjE9ft1u7kP7ZO/iN+hiStF2Dv2LRAyBPBqV7/HV5SIcTIObuJas86KKlB92l9WILo6lbbVg54rxFitumScEWNmek3TBcWatF2tjQTDzTU0ZLhZsjkOn++5jFmDQvDquM4mfW5qAAVpog+63xO6mQuA+OArSAX6PCb+qj3zuxif0HmC4V99Wuf+FrMS1OVA+AzxV7utk/gHtjRX9w+bRiMCiIMHsHKA+AvYwQN45rgY3/DNWACS+CCa8ZcIM/UpUxvTeSIw8h3x15dbAHDvt+IfyKxzohxdUSTOc3AXf4HueEv8Ve7qX/9uhrq4BooPUmNTSe1cge73ivEtKnsxMDRuU+3zxn8sxpXkJYifx5RvARdfcd+pDUDOZaDzXaKCtGUhcNcn4sMk9woQ1A/wbq97rg/DdGMG3sg3fJ2/XxTP9+hWMZ4kJRo4tFIcA0S3y+O7gf+2NXycgwfQ8wHxYTLsNfGz3bsU+OdNYNT74kO1ph1vAwc+ASLniu6O7yeLts7UqzhknhWv795KBC79mTL6NBpRqWgzSDdm6fxW4Mcphuc5egL/uVz3B7UkibAa1E/8fH9/Svw/AYjfjRcTan+w5lwSv9etIow/p6Uc/hLY/Lyo8Ax5Ufz/14g+mOnWMdxcQ0OGm/e3nMXKXZfwyMA2WDS+i0mfmwBkXwTyroj++Wv9RXI95YWihPzlMKA4E+g/V4SWdsPFGIhVIwBIolSr0ej67V38gEmfi6mbds6iy0ShMPxrW8uzjfgATD8pyrmZceL7tNja7Wk3HLj0j+Gxaw1u7Ho3EDoWOPipCCTO3mLtiWGvir70v56rPSBx2kbRL35wubivVV8xg6SyRHfOuP+JCsS340UwaRkuZoToP5fSXoS4treLGSHuQaJMvuMd8TML6gfc+42YZbL/YzHWxbczsOZuwNm3uqKht8xC0hHgh8mi+6Lt7eI5fMPEmIL8ZODsX2LMg374vFFX9okQM+b/gODI659/agPwy6PVP5NlopL2RvXrO/uKn3Xfx4Dhrxk+TpJEKHZvWfdz6/8lnHFGhMm6AsyNyrkEfFJj77Wwu0S3T30dWQX8JZbkMDpeozGTJPFHgm9ns3aVkPkw3FxDQ4abD7edwyc7LmJ6ZDAWT+hq0udultRVwM63xSJUrfuLD+fSq+IDdeY2ER52vye6MnrcJ2a67PmvmCky4VPxAZuwX1QxHPTe66/HiOPG+IQBWXH1a1/4I+JDbmV1aV5/VsHN6PGA6NvOPG38foUN8HKqYTgw5ve5Yk2KurQbDtz1sehC0oaq1pHAI39XdyOkifEd3aeK4KENZLe/Irqrci6Jrp3MOFGVUtmLQcaSWnRTGSuhX94luru8O1zvp2B5pbnA8j5iOvLju0V3UdIRMR5kzAdiRlNjpK4E3vYT70PIEFF1HPnujVVYUo6JdVIAoO8T4nqJGokb+fxuUntLNXZKG/GPulrTrPLizZEkMRCwVR8xYBUQgxxTjok+dkcP0V+973/iPm03ASC6TQ59JvroD38ujmWcErNRdrwtbut3dSQcAG5/Gcg6L8r9dQUbQAQbB3cgdLxuCqmTtxjwuu1V0WbtFNror8UXICoePR8U64gYEzxQhKzTv+qqPHe8CWxfJL63cxWVk9g1ur+cvdqLacvaLiSvDtcPNgBwx2Ix9MCnE9B2CPD5EACSqLpMWikqP4CoYmjDjbYKBYhurMg54vugfrpw499dzA5pUT0T0E+v6/V6FZG2Q6/f7sbC0ROYFysqLNr1YoL6AA/9YtFmXZfSVlQMr14CetwP9Lz/xp/Dr4tu0LAfq8/UdDHcmJBS0YzDzclfxKqUo967dpldu/Jn3B9igKVniJjSefJnEVgA4J/FwLilYoGwmrSDO/d/ZHhcUyW6Pow5vtb4QNG6aMeraMNNt3vFeITJelN8z20B/v6PblxJn8eATqNFEMk4owtdfWaJD4nu94mp0R3uEN1hwQOBbvfowk3IYDEjotc00bXRfrgYrJmXCCyrXiCtvh82zl7ARL0psHd9LNZQ6fOY4ZihNoP1rrnGbCytoL66awnoUb/Xtwb2LpZuwc2JnCMGm3cceXOPV9mLNVku7xS/k0RNFMONCSmVzSTcFKQCGx4Tfxn2ekiUw7UVi8I04P51wNV4cSw3QXxQT/pMzLg5vg6492vdCp658WL6o76jq0XFIemQ6IqZe1jM0LBzASZ9IdZNKUgW57a/Q0xh/WKobpXPiEfFrIPIOcDm/xhf0TN0nBh0+OWw2utbdJ0sBmnO2Awc/1Es2lZTp1Hiuq5eFitxulWviBw+QyzGdeRLMVal32zDrpgWbYHnqlcE1e++6TJJ/FdlZziWw62VGJxcWXLzf0n3nm78eOv+wF3LRZvqmlnVZrD4uXu0FuNDqHHrM0t83Yp7VouQrh3ATdQEMdyYkFVWboqzxQBK/UGSG2eL9VAS9olwo7/4Vvwe4N0ay22f/RP4JEK3Ouqae3R7ywCiDN4qQlQzci6KKa4/VX8gh9wmwsFcvYAyaglw4GPAu5MIAq7+Yr2Ng8vFOImR7+qCw9CXxKyUsPFiQPCRL8XxNoPFehxe7cR0S33B1cvktxmoW9XWGKWt4ToXWi4+ohpVUWJ8jIn+0viP7xZTu7vda/w1bGzEtPCE/aILz9R61zE9WcvVT3TJ2TpZbI8YMjM7J90CjERNFMONCcljbpryGO3CDLESZ7vhYvDu5v8AUADzYsRKrrFrgcQDuvOryoFTv4rvVQ6iAqAdk+ITKmbw/P6ULthoafchuWe1CBOu1WuRpMaIcKMV+VTtNna+S3zpC+wpAkVN7YYBL1wW4yhSjunCjTYoeHcQ4cargxiX49XeNJsIdq2ji6ymwJ6G2wcYM3GFmAXSdsitturmeLS2zOsSEd0khhsT0oabqsZYudFodJvH6U+TvLJPzHqJeFRMj/7hbrGIVc0l5D/sWHt6MQCsHAjkXBDf379OtwJq8lFRdXH0EANyv7tLdBsNWiBW9wREl1PYXYaLlAX2EkEj54Joa/s7bv3atWOAWvYWY0vKC0XVBgC8O4r/+nVpkOXCTcKzTd3bHBARUS0MNyakqg43msYYbg4uF0t5j3xXBJm9H4oqxdZXgJJsYMtLYpxJaa5u6XNV9WqX5QW6YBPxqBgwq13mPeeCWCwr4lEx/RQQq5HqV1aCq6cZ5yWKcKENN5LG+Gq+934j2jt0oWm7QhQKsZutvl7TxNTmgfNM9zpERGRRDDcmZNOYKzdR1YNUt74sZjalHjO8X1Mlgo1vF+DuL0WVxTVADGT9uJc4p81gMV0ZEPsFHV0tvu92LzDmv9d+/VYRuvU2QoYA8bvFkvTG+HcVA5DNoUWIWP2WiIisBsONCTWKyk15IXBguViDpPMEMYbmYpThOTWDDSBm63QaK/5bc8xJyG1iY7yRerv4+uqtcVLXNOK6TPlWbHRnbAM4IiKiW8RwY0I2CgtWbjRqsf7LjrfE9GQogAfWi71s9GczaTm4A2XV++yMfFfsd1OX+9eL0KQd9AvoxqoA1cvm3wBHz7p33CUiIrpFDDcmpKpe50Zj7tlSqbFiTZmci3oHJbE/jnZ125qmrgG+HSe+D7nt2s9vbGpoyG3AHW+Ibixbh5tsOBERkekx3JiQXLlRmzncHPpcBBsHD7G/Up9ZwIr+Ym8lrVHviVlLv84Sa9O0GSQWmFNXiFlJN0qhELtfExERNTIMNyakqp5i3WDr3OQliVVDizIBpZ2umyjzjPjvXZ/oZimF3Kbbj6ndMBF6AMCno1h/RqEARr/fMO0kIiKyIO4Lb0LK6p9mg6xQHL8HWNZV7OS8or/4fs9/xVibrHPiHP1BvvpdTYG9dd8H9BD7xxAREVkpVm5MSKmt3DREuPnrOfHfE+t0x3a8LfZuqioVOz5rd2sGDMON/u7NREREVo6VGxNq0MpNUYbx4zHfi/96dwRslLrjniFAywjAscWNz2YiIiJqwli5MSFt5cYkU8ETDorVfyuKgd0f6KZta0U+BZRcFbtWA4BvqOH9CgXw8B9ix2sH91tvDxERURPBcGNC2l3Bb3kRv4Ofim0RcI3n8ekkVvrVhhsXv9rncGdfIiJqhtgtZUK6jTONbDBZX2X5wLbXAEiAfXXFJbBX7fN8QgHPYGDIS4CdK9DzwZt/TSIiIivCyo0J6Rbxu8knKMwA0o4Dkhpwbw3MPQQUpYvxM3kJQNwfwLZXxbnaFYJvXwgMfcm0G0wSERE1YQw3JqTbfuEmKjcZp4EvhopF9QCgZS/RrdSirbjt2UY31ds1AHD00D2WwYaIiEjGcGNCuo0zb+BB5YVil+6MU7pgAwABPWufG3IbEDETaDPwltpJRERkzRhuTOimxtxsfRk49l3t48bG2ShtgXFLb7J1REREzQMHFJuQNtyo65ttCtONBxsACOxpkjYRERE1N6zcmJAu3NQz3Rz63PB2QE/RNeXVHnD0NG3jiIiImgmGGxPShZt6TJeqqtBVbSZ/Kao4oWMBr3YN2EIiIiLrx3BjQtpF/OoVbs79BZRkAy7+QJfJgJJvBRERkSlwzI0JyZUbqR7hJmaN+G/vaQw2REREJsRwY0L17paSJCD5sPg+bHwDt4qIiKh5YbgxIVV9w01eothmwcYW8AkzQ8uIiIiaD4YbE7Kx0W2/IF2rayr9hPivbyigsjNDy4iIiJoPhhsT0lZugOtUb9JPiv/692jgFhERETU/DDcmZKMXbqquFW7Sqis3Ad0buEVERETND8ONCelXbjR1dUtpNEBqjPjen+GGiIjI1DgH2YRsFNep3CQcBK5eAorSAXs3brFARETUABhuTMhgzI26RrjJTQC+Hg2g+niXiYCto9naRkRE1FywW8qElPrhpma3VNY5yMEGALrfZ55GERERNTMMNyakUCigzTe1ZksVJOu+D7sLaB1pvoYRERE1Iww3JqayET/SWmNuClLFfyNmAlO/B2z4oyciImoI/IQ1MXkLhppjbvJTxH/dW5q5RURERM0Lw42JaQcVV2o0hndou6XcGG6IiIgaEsONiamUdewvpa3cMNwQERE1KIYbE1MpxY+0Uq1XuZEk3ZgbdksRERE1KIYbE7Ot7paq0h9zU5oLVJWK710DLdAqIiKi5oPhxsS0lZsq7ZibwnRg1Z3ieycvwNbBQi0jIiJqHhhuTEw75qZSW7k5uhrIuSi+92htoVYRERE1HxYPNytWrEBISAgcHBwQHh6OvXv3XvP8NWvWoEePHnByckJAQAAeeeQR5OTkmKm112erXedGG27ST+ruHP66BVpERETUvFg03Kxfvx7z58/HK6+8gpiYGAwePBijR49GYmKi0fP37duH6dOnY+bMmTh9+jR+/vlnHDlyBLNmzTJzy+smV2603VLpp8R/H/4TaDfMQq0iIiJqPiwabpYuXYqZM2di1qxZCAsLw7JlyxAUFISVK1caPf/ff/9FmzZtMG/ePISEhGDQoEF44okncPToUTO3vG7ymBu1BJTmAfnVQc2/q+UaRURE1IxYLNxUVFQgOjoaI0aMMDg+YsQIHDhwwOhjBgwYgOTkZGzevBmSJCEjIwO//PILxo4da44m14tutpQGyDgtDroHAY6eFmwVERFR82GxcJOdnQ21Wg0/Pz+D435+fkhPTzf6mAEDBmDNmjWYOnUq7Ozs4O/vDw8PD3zyySd1vk55eTkKCgoMvhqSrltKAjKqu6T8WLUhIiIyF4sPKFYoFAa3JUmqdUzrzJkzmDdvHl5//XVER0djy5YtiI+Px+zZs+t8/iVLlsDd3V3+CgoKMmn7a7KVu6U0QNZZcdCvc4O+JhEREelYLNx4e3tDqVTWqtJkZmbWquZoLVmyBAMHDsR//vMfdO/eHSNHjsSKFSuwevVqpKWlGX3MwoULkZ+fL38lJSWZ/Fr0qfQX8ZNXJW7YQEVEREQ6Fgs3dnZ2CA8PR1RUlMHxqKgoDBgwwOhjSkpKYGNj2GSlUglAVHyMsbe3h5ubm8FXQ5K3X9BodOHGjasSExERmYtFu6UWLFiAr776CqtXr0ZcXByeffZZJCYmyt1MCxcuxPTp0+Xzx48fj19//RUrV67E5cuXsX//fsybNw99+/ZFYGDjCBC2Sr3KTWF1NYnhhoiIyGxUlnzxqVOnIicnB4sXL0ZaWhq6du2KzZs3Izg4GACQlpZmsObNjBkzUFhYiOXLl+O5556Dh4cHhg0bhvfff99Sl1CLqrqypKkqB4qzxEHuJ0VERGQ2Cqmu/hwrVVBQAHd3d+Tn5zdIF9WCn2Lx67EUvDPUDQ/+Ow5Q2gOvZgB1DJImIiKi67uRz2+Lz5ayNtoBxY5l1QOl3QIYbIiIiMyI4cbEtAOKHUozxAF2SREREZkVw42J2cqVm0xxgIOJiYiIzIrhxsS0lRvn8urKjVuABVtDRETU/DDcmJh2+wXncs6UIiIisgSGGxOzrZ4K7lR5VRxw8bVga4iIiJofhhsT01ZuHKqqN+h0amHB1hARETU/DDcmpt0406kqXxxwZLghIiIyJ4YbExPr3EhwUrNyQ0REZAkMNyamUtrAEeWwlSrEAVZuiIiIzIrhxsRslQp4okjcUNoBds6WbRAREVEzw3BjYiobG3gqqsONoye3XiAiIjIzhhsTUykV8FAUihvskiIiIjI7hhsTM+iW4mBiIiIis2O4MTGVjQ089LuliIiIyKwYbkzMVqmABys3REREFsNwY2KGA4oZboiIiMyN4cbElPoDilm5ISIiMjuGGxOztbHRDShm5YaIiMjsVJZugLVRKRVwVnDMDRERkaWwcmNitkoF3MHZUkRERJbCcGNiKhsbuClKxA0HD4u2hYiIqDliuDExlQ3ghmJxw8Hdso0hIiJqhhhuTMxOqoCdQi1uMNwQERGZHcONidlVFQAAqiQb7ghORERkAQw3JmZXKda4KYQzdwQnIiKyAIYbE7OtEuGmAE4WbgkREVHzxHBjYrYVItzkS06QJMnCrSEiImp+GG5MTFUpxtwUSE5QaxhuiIiIzI3hxsSUFdXhBs6oYrghIiIyO4YbE5PDjeSESrXGwq0hIiJqfhhuTExZrle5UbNyQ0REZG4MNyZmU54PoLpyo2HlhoiIyNwYbkytrDrcwImVGyIiIgtguDE1bbiR2C1FRERkCQw3pqZXuWG3FBERkfkx3JhamW7MDSs3RERE5sdwY2py5caZU8GJiIgsgOHG1MrF9gtFkiMX8SMiIrIAhhtTkiRAXQ4AKIct1BxzQ0REZHYMN6akrpC/LYctKqpYuSEiIjI3hhtTqiqXv62ALcoq1RZsDBERUfPEcGNKBuFGheKKKgs2hoiIqHliuDGl6vE2lbAFoEBJOSs3RERE5sZwY0rVlZsqGzsAYOWGiIjIAhhuTKl6QLFaYQsAKKlg5YaIiMjcGG5MqaoMAKDWVm7KWbkhIiIyN4YbU6oSlRsNww0REZHFMNyYUvWAYklpDwAoZrcUERGR2THcmFL1gGJJJSo3JRxQTEREZHYMN6akXedGW7nhVHAiIiKzY7gxpepuKbByQ0REZDEMN6ZUPaBYoXIAwMoNERGRJTDcmFL1VHCFSnRLsXJDRERkfhYPNytWrEBISAgcHBwQHh6OvXv3XvP88vJyvPLKKwgODoa9vT3atWuH1atXm6m111G9iJ+NLWdLERERWYrKki++fv16zJ8/HytWrMDAgQPx+eefY/To0Thz5gxat25t9DFTpkxBRkYGVq1ahfbt2yMzMxNVVY2kQlI9oFhpK7qlSrjODRERkdndVLhJSkqCQqFAq1atAACHDx/Gjz/+iM6dO+Pxxx+v9/MsXboUM2fOxKxZswAAy5Ytw9atW7Fy5UosWbKk1vlbtmzB7t27cfnyZbRo0QIA0KZNm5u5hIZRPaBYaecIACipVEOjkWBjo7Bkq4iIiJqVm+qWeuCBB7Bz504AQHp6Ou68804cPnwYL7/8MhYvXlyv56ioqEB0dDRGjBhhcHzEiBE4cOCA0cds2rQJERER+OCDD9CyZUt07NgRzz//PEpLS+t8nfLychQUFBh8NZjqyo3KTnRLSRJQVsWuKSIiInO6qXBz6tQp9O3bFwDw008/oWvXrjhw4AB+/PFHfPPNN/V6juzsbKjVavj5+Rkc9/PzQ3p6utHHXL58Gfv27cOpU6ewceNGLFu2DL/88gvmzp1b5+ssWbIE7u7u8ldQUFD9LvJmaMONrQMU1cUazpgiIiIyr5sKN5WVlbC3F9WJ7du346677gIAhIaGIi0t7YaeS6Ew7LKRJKnWMS2NRgOFQoE1a9agb9++GDNmDJYuXYpvvvmmzurNwoULkZ+fL38lJSXdUPtuSPWAYoWtPZxslQA4Y4qIiMjcbircdOnSBZ999hn27t2LqKgojBo1CgCQmpoKLy+vej2Ht7c3lEplrSpNZmZmrWqOVkBAAFq2bAl3d3f5WFhYGCRJQnJystHH2Nvbw83NzeCrwVRPBYfKAU72YjgTKzdERETmdVPh5v3338fnn3+OoUOH4v7770ePHj0AiDEx2u6q67Gzs0N4eDiioqIMjkdFRWHAgAFGHzNw4ECkpqaiqKhIPnb+/HnY2NjIg5stqnoRPyjt4GzHyg0REZEl3NRsqaFDhyI7OxsFBQXw9PSUjz/++ONwcnKq9/MsWLAA06ZNQ0REBCIjI/HFF18gMTERs2fPBiC6lFJSUvDdd98BEAOZ33rrLTzyyCN48803kZ2djf/85z949NFH4ejoeDOXYlry9gv2cLKrrtxwrRsiIiKzuqlwU1paCkmS5GCTkJCAjRs3IiwsDCNHjqz380ydOhU5OTlYvHgx0tLS0LVrV2zevBnBwcEAgLS0NCQmJsrnu7i4ICoqCk8//TQiIiLg5eWFKVOm4O23376ZyzC9Kl24cbavrtxwrRsiIiKzuqlwM2HCBEyePBmzZ89GXl4e+vXrB1tbW2RnZ2Pp0qV48skn6/1cc+bMwZw5c4zeZ2zmVWhoaK2urEZDb1dwbeWmiOGGiIjIrG5qzM2xY8cwePBgAMAvv/wCPz8/JCQk4LvvvsPHH39s0gY2KXrdUi7ygGKGGyIiInO6qXBTUlICV1dXAMC2bdswefJk2NjYoH///khISDBpA5sUvQHF2nDDyg0REZF53VS4ad++PX777TckJSVh69at8irDmZmZDTvVurHTmwruLIcbDigmIiIyp5sKN6+//jqef/55tGnTBn379kVkZCQAUcXp1auXSRvYpFQv4geVHVwctOGm0oINIiIian5uakDxPffcg0GDBiEtLU1e4wYAhg8fjkmTJpmscU2O3oBil+rZUlzEj4iIyLxuKtwAgL+/P/z9/ZGcnAyFQoGWLVvWewE/q6VfubG3BcAxN0REROZ2U91SGo0Gixcvhru7O4KDg9G6dWt4eHjgrbfegkajMXUbmw6DMTeiclNUxnBDRERkTjdVuXnllVewatUqvPfeexg4cCAkScL+/fvxxhtvoKysDO+8846p29k0yLOl7OHqoF2hmOGGiIjInG4q3Hz77bf46quv5N3AAaBHjx5o2bIl5syZ03zDjbzOjR2ctYv4sXJDRERkVjfVLXX16lWEhobWOh4aGoqrV6/ecqOaJEnSdUsp7fVmSzHcEBERmdNNhZsePXpg+fLltY4vX74c3bt3v+VGNUlqvSnfeisUM9wQERGZ1011S33wwQcYO3Ystm/fjsjISCgUChw4cABJSUnYvHmzqdvYNGi7pIDqcCNyY0mFGmqNBKWNwkINIyIial5uqnIzZMgQnD9/HpMmTUJeXh6uXr2KyZMn4/Tp0/j6669N3camQTuYGACU9vIKxQAHFRMREZnTTa9zExgYWGvg8PHjx/Htt99i9erVt9ywJkc73sZGBdjYwF4hwVapQKVaQnF5FdwcbC3bPiIiombipsMN1eDqDzx3Tl6lWKFQwNlehbySSjFjyt3C7SMiImomGG5MxUYpAo4eF2244aBiIiIis7mpMTdUP5wxRUREZH43VLmZPHnyNe/Py8u7lbZYHW24KWa4ISIiMpsbCjfu7tceOOLu7o7p06ffUoOsiXYhv0KuUkxERGQ2NxRumu0075vkzMoNERGR2XHMTQNy5ZgbIiIis2O4aUDancEL2C1FRERkNgw3DcjDyQ4AkFtccZ0ziYiIyFQYbhqQpzbclFRe50wiIiIyFYabBuTpJLZcyCth5YaIiMhcGG4akNwtxXBDRERkNgw3DcjTWVRu2C1FRERkPgw3DUg75iavpAIajWTh1hARETUPDDcNyKN6zI1G4irFRERE5sJw04DsVUo42SkBcNwNERGRuTDcNDBPDiomIiIyK4abBuYhTwfnoGIiIiJzYLhpYKzcEBERmRfDTQPTVm44HZyIiMg8GG4amP50cCIiImp4DDcNzFOu3DDcEBERmQPDTQPzcrEHAGQXMtwQERGZA8NNAwtwdwAApOaXWrglREREzQPDTQNr6ekIAEjNY7ghIiIyB4abBtbSQ4Sb7KIKlFWqLdwaIiIi68dw08DcHW3hXL0FA6s3REREDY/hpoEpFAoEVldvUhhuiIiIGhzDjRlw3A0REZH5MNyYgVy5yWW4ISIiamgMN2bQUu6WKrNwS4iIiKwfw40ZaMMNu6WIiIgaHsONGXi5cGdwIiIic2G4MQM3B7G/VEEpdwYnIiJqaAw3ZuDuWB1uyqos3BIiIiLrx3BjBm7V4aaovApVao2FW0NERGTdGG7MwM1BJX9fyOoNERFRg2K4MQOV0kbegiGf426IiIgaFMONmbjJ424YboiIiBoSw42ZyIOKS9ktRURE1JAsHm5WrFiBkJAQODg4IDw8HHv37q3X4/bv3w+VSoWePXs2bANNRDsdnN1SREREDcui4Wb9+vWYP38+XnnlFcTExGDw4MEYPXo0EhMTr/m4/Px8TJ8+HcOHDzdTS28du6WIiIjMw6LhZunSpZg5cyZmzZqFsLAwLFu2DEFBQVi5cuU1H/fEE0/ggQceQGRkpJlaeuvcHMWMKVZuiIiIGpbFwk1FRQWio6MxYsQIg+MjRozAgQMH6nzc119/jUuXLmHRokX1ep3y8nIUFBQYfFmCbswNww0REVFDsli4yc7Ohlqthp+fn8FxPz8/pKenG33MhQsX8NJLL2HNmjVQqVRGz6lpyZIlcHd3l7+CgoJuue03g2NuiIiIzMPiA4oVCoXBbUmSah0DALVajQceeABvvvkmOnbsWO/nX7hwIfLz8+WvpKSkW27zzXDjFgxERERmUb/yRwPw9vaGUqmsVaXJzMysVc0BgMLCQhw9ehQxMTF46qmnAAAajQaSJEGlUmHbtm0YNmxYrcfZ29vD3t6+YS7iBrBbioiIyDwsVrmxs7NDeHg4oqKiDI5HRUVhwIABtc53c3PDyZMnERsbK3/Nnj0bnTp1QmxsLPr162eupt8U7RYMu89nYe6PxyBJkoVbREREZJ0sVrkBgAULFmDatGmIiIhAZGQkvvjiCyQmJmL27NkARJdSSkoKvvvuO9jY2KBr164Gj/f19YWDg0Ot442RtnIDAH+dSMNrYzvD393Bgi0iIiKyThYNN1OnTkVOTg4WL16MtLQ0dO3aFZs3b0ZwcDAAIC0t7bpr3jQVLT0doVAA2oLN+YxChhsiIqIGoJCaWf9IQUEB3N3dkZ+fDzc3N7O+9v6L2Vj8xxmcyyjEa+M6Y+agELO+PhERUVN1I5/fFp8t1ZwMbO+NkV3EYOmLmYUWbg0REZF1Yrgxs/Z+rgCACxlFFm4JERGRdWK4MbMOvi4AxJibZtYjSEREZBYMN2YW4u0MG4VYzC+rsNzSzSEiIrI6DDdm5mCrRIC7IwAgJa/Uwq0hIiKyPgw3FuDtKlZMZuWGiIjI9BhuLMBXG26KGG6IiIhMjeHGAnxYuSEiImowDDcW4OMiwk0mww0REZHJMdxYACs3REREDYfhxgIYboiIiBoOw40FMNwQERE1HIYbC9COuckqKucqxURERCbGcGMB2spNRZUGBWVVFm4NERGRdWG4sQAHWyVcHVQAgKzCMgu3hoiIyLow3FiItnqTWcBxN0RERKbEcGMhLT3E/lKJV0ss3BIiIiLrwnBjIe18XAAAl7KKLNwSIiIi68JwYyHtfLXhptjCLSEiIrIuDDcW0s7HGQArN0RERKbGcGMh7asrN0lXS1BWqbZwa4iIiKwHw42F+LjYw9VBBY0EJORwUDEREZGpMNxYiEKhkAcVn88oxK/HkpFRwDVviIiIbpXK0g1ozsICXBGblIen18YAAAZ38Mb3M/tZuFVERERNGys3FjQ81M/g9t4L2RZqCRERkfVguLGgQR284WirlG+rbBQWbA0REZF1YLixIAdbJYZ09JFvqyUJm46n4uiVqxZsFRERUdPGcGNh84Z3wIB2XgAASQLmrY3BPZ8dtHCriIiImi6GGwvrHOiGHx/rj9YtnAyOc+0bIiKim8Nw00gEuDsY3M4q5G7hREREN4PhppGoFW6KGG6IiIhuBsNNI+Hv7mhwO5uVGyIiopvCcNNIONspDW6zckNERHRzGG4aCVcHw8WiOeaGiIjo5jDcNBJT+gRhVBd/tPQQ3VPZrNwQERHdFIabRsLJToXPpoVj9pC2AIxXbiRJwk9HknAyOd/czSMiImoyGG4aGR9XewBAdlFFrftOpRTghQ0n8J9fjpu7WURERE0Gw00j4+0iwo2xyk1KXgkAIC2/zKxtIiIiakoYbhoZXeWmHL/HpmDWt0eRX1IJAMgpFtWc/NJKVKk1FmsjERFRY8Zw08how01JhRrPrIvF9rgM/BydBADI0euqyiuttEj7iIiIGjuGm0bGyU6FOzv7GRw7lSIGEF8t1oWb3OLaY3KIiIiI4aZR+nBKD4T6u8q3f4tNxbAPd+GX6GT5mDboFJZV4siVq5AkyeztJCIiaowYbhohNwdb/P7UQOx94Xb52OWsYhSVV8m3c0tEuHnoq0O497OD2HE20+ztJCIiaowYbhope5USQS2c6rz/arEYVHy8es2bP46nmqtpREREjRrDTSP3xG1tjR7PLanAiRTdYn6+bg5GzyMiImpuGG4auWfv7Ij1j/evdfxqcQUOXsqRb+cYWfSPiIioOVJd/xSyJAdbJfq19YKznRLFFWr5eG5xBeLSCuTb3IuKiIhIYLhpIlwcVAbh5mJWEU6nMtwQERHVxG6pJsLNwdbg9onkfKg1uunf7JYiIiISGG6aiBbOdkaPPzKwDQAgp7i81lo3ybkl8gKAREREzQXDTRPx5oQucLZTYnKvlvIxO5UNZgxoAwCoVEvI19uSQZIkDHp/J8Z9sg9p+aXmbi4REZHFcMxNExHq74YTb4yERpJwKjUfCijw5oQuCPZyhquDCoVlVcguqoCHk6jw6O8cfi69EAHujpZqOhERkVkx3DQhShsFlFBg6/zboFAo5OM+LvbV4aYc7X1dAMCgO+oq96EiIqJmxOLdUitWrEBISAgcHBwQHh6OvXv31nnur7/+ijvvvBM+Pj5wc3NDZGQktm7dasbWNg76wQYAvF3ETuL6M6ZO6c2k0q/iEBERWTuLhpv169dj/vz5eOWVVxATE4PBgwdj9OjRSExMNHr+nj17cOedd2Lz5s2Ijo7G7bffjvHjxyMmJsbMLW9cvF1FV1RWoV640avcpOZxzA0RETUfFg03S5cuxcyZMzFr1iyEhYVh2bJlCAoKwsqVK42ev2zZMrzwwgvo06cPOnTogHfffRcdOnTAH3/8YeaWNy7BXs4AgE92XMTpVBFqjIWbsko11hxKYNghIiKrZrFwU1FRgejoaIwYMcLg+IgRI3DgwIF6PYdGo0FhYSFatGhR5znl5eUoKCgw+LI2jw1ui24t3XG1uAIvbjiB+OxiZOpVcbTdUit3XcIrG09h6hcHLdVUIiKiBmexcJOdnQ21Wg0/Pz+D435+fkhPT6/Xc3z44YcoLi7GlClT6jxnyZIlcHd3l7+CgoJuqd2NUQtnO3zzSB842ylxKqUA89fHAgD83MRYnNS8UkiShG1nMgAASVdZuSEiIutl8QHFNQfHSpJU65gxa9euxRtvvIH169fD19e3zvMWLlyI/Px8+SspKemW29wYebnY49FBIQCA40l5AICZ1bcLyqoQ+toWXMwslM+XJAk//JuA1fvizd5WIiKihmSxcOPt7Q2lUlmrSpOZmVmrmlPT+vXrMXPmTPz000+44447rnmuvb093NzcDL6s1RND2qGNl5N8e3LvVvL35VUaVKp1KxhfyirCq7+dwuI/z3BfKiIisioWCzd2dnYIDw9HVFSUwfGoqCgMGDCgzsetXbsWM2bMwI8//oixY8c2dDObFBd7FdY+3h/hwZ54bHAIvF3s0dLD+OJ9f5/UhcqkqyXmaiIREVGDs+gifgsWLMC0adMQERGByMhIfPHFF0hMTMTs2bMBiC6llJQUfPfddwBEsJk+fTo++ugj9O/fX676ODo6wt3d3WLX0ZgEuDtiw5O6cPj+3d3x2u+nEJ9dbHDe5lO6cJOcW4perT3N1kYiIqKGZNExN1OnTsWyZcuwePFi9OzZE3v27MHmzZsRHBwMAEhLSzNY8+bzzz9HVVUV5s6di4CAAPnrmWeesdQlNHqDOnjj82nhtY7HpelmjSXn3vgAY0mSsHpfPA5czL6l9hEREZmaQqq5lbSVKygogLu7O/Lz8616/I2+8io1Or26pc77H+jXGu9O6lbn/WWVapRUqA12Jj965Sru+UxMKY9fMqZeg8CJiIhu1o18flt8thQ1PHuV8pr3bzyWgjf/OF3n2JunfozBoPd3ICFH17WVUaAbhHwl58bG7BSWVWLt4UTueUVERA2C4aYZGtDOy+B2aaUaX++/giH/3YnY6mnkfxxPxdn0Amg0ErbHZaCkQpyjlVuiCybHEnJv6PVf3ngKC389iWer1+MhIiIyJYabZqJzgCjh9WrtgTWz+mH94/3x7aN9Dc7RSMCi30/hwMVsPL02BrO/j0Zqvm48jv62DforIB9LvLFw88fxVADA7vNZN3wdRERE12PR2VJkPp9PC8fq/fF4dGAIFAoF+rX1QqVaI98fEeyJM2kFOJ6cj//8cgKA6G7aeTZTPudYYq68yGJWoW6n8egbrNw0NvVdOJKIiJoGVm6aiaAWTlg0vguCWugW+bNV2sgVnVfGhuHx29oCAFL0KjTfHkyQv88uqsCxxDys3HUJMYl58vFzGYVIz9eFHWOSc0twIjnvmudYwuH4qwh/ezt+j02xdFOIiMhEWLlp5r5+pA+uFlcgLMANbb1dsGz7BYP7L2YWGdy+e2XtTU0lCfj5aBKeHt6hztcZ9P5OAMDW+bfJxxpDsWTfhSxcLa7AzrOZmNCzpaWbQ0REJsDKTTPn5+aAsOrqjbuTLcZ2CzB63gP9WsNOVfvX5ZGBbQAA644kQaMxvqpARoGuqrP5ZJr8vY1CYfQxKXml+L+t55BfWlnv67hZ2tfIM8NrERGReTDckIF3J3XDg/1aY331Ng5a94S3wvL7e6GTn6vB+TMGtIGLvQopeaWISy+o+XQAdBt5Aobjc9QaCQVltUPF7O+jsXznRTz3U+ytXYwRG6KT8fV+3WahBWVVAIC8EoYbIiJrwXBDBtydbPHOpG7o19YLKx7sjUB3B7RwtkMnP1eM6OKPrc/eBkdb3bo5LT0c0bWlqPycTtWFm9IKtfz9cb2xNvtqrGicY2Stm5Mp+QCA7XGZte67FeVVajz383G8+ccZeeZXQXXFxhxVIiIiMg+GG6qTn5sDohYMwa7/DIWzvW54lr+7g/y9SmmDLoFiX68z1eHmm/3x6PbGVizbfh4AcCI5v87XOHAx22Axv+LyKoP780pMt9Bf0lXdQOmcIvG82sqRKV+HiIgsi+GGrsnZXgU3B1uDY228nAxudwkUlZtTKfn4cs9lvPHHGVRpJKw5lIiSiiqDbqmaXvv9NB786hC0u4DoV38A4N/LV01wFYL+CszZRWKdnoJSEabySyvrHDNERERNC8MN3bDXx3eBh5MtnqmeHdW1pajcHEvMxTub4wAANgogq7AcUz4/iIKyKrjYG07M0w9IcWkFOHgpB4CuS0rr38s59W6XRiPhWlulJeqFmyxtuKmu3Ggk4I0/TmP7mYx6v545NbMt4IiIbgnDDd2wEG9nxLx2J569syMAoK23M+xVNtAWPubf0QETe4lp1adSRCXmkwd6yY93sVehT5sWBs/5wFeHcMfS3Th6RVRq2vo4AwBOpxrv0vr1WDImLN8nD1Auq1Rj3Cf7cNfy/VDXqMBoQ0+C3h5YWdUrLOuPtfnuYAJe+/3UDfwkbt2hyzn4dOfFa1aN3t9yFhFvb0da/o3v3k5E1Bwx3NBN0V/RV6W0Qd8QEVaeu7Mj5t/REZOqw43KRoEXR4Xi9k6+uCPMDwDw1sQucHe0rfWcFzOLsOV0OgDggb6tAQBn0wprVS1+PJSIBT8dx/HkfKyunvn07YErOJNWgJMp+QYVmlMp+ej02t/4ZMdFw8pNYTkq1RqU6A18BoC0/LJa435uxuWsIpytY/aYvtd/P43/bj2Hw1fq7n6LOpOBnOIKi6wEXaXWYMbXh7GkuiJHRNQUMNyQSSy/vzc2zxssL+Q3uIMP1j3eH/88NwRPDm0HAHj/7m74ZXYkJvVqhTS9FY1/nh2J4OpuKkkCnOyUuL9va9gqFSgsr0Li1RI8//NxvLHpNABg/dEk+bGHLueguLwKK3dfko/pLzy4bPt5VKolLI06b7CreXZROQrLjIeY+Oxio8frS62RcM9nBzHp0wMoNDLVXZ92Nei6dmQHdIOdtYOgzel8RhF2ncvCNweumP21iYhuFsMNmYS7ky06Vw8s1urf1gvBXs7ybS8Xe0RUd0eN6y4WC+wR5IE+bVrglTFh8nmD2nvD2V6F9r5iTZ0fDyXil+hkfHPgCuKzi3E2TVcRyS6qwI+HEg3WqVkadR6zv49GVmE5ivSqMBf0Qk9WYbk8DbymKzm3Fm7S8ktxtbgCpZVqXMmuO7SUVFTJ7UurY/sKSZKQW31txqbNNzTtTLbyKg3KKtXXOZssTZIkvk9EYLghCxnZxR/rHu+P76p3Jh/Q3hu2StHVNSzUFwAQFiDCzed7LsuP++HfBJRXaeBsp8SAdl4AgI93GG4ZEZdWgC2n0zFnTTTOphcaff1D8Vfx9NoYo/ddqWflJi2/FHvOZ9XqNtPv/krKrR1uCssqodZIyCzQ7axe13iagrIqeQxRTlG50XMaUk6x7jW50GHj98am0+j9VtQ1K4HXo9FIBn8UEDVFDDdkETY2CvRv6yWPvXGxV2HmoLboEeSB0V1FVSfM363W41btE2NsugS6Y1AHbwCQu5eGdPQxOPfIldxaH8j9QnQDmWvOzNKKr662rNx1CT3e3Ia4tNpjZ36LScHwD3dj+urDeHptDCqqdDusJ+oNXE6s8SGTnl+Gvu/8g8e/O2qwLUVdlRv99XdM3S1VqdZg7o/H8NXey3Weo/+auVwLqNE7FH8VJRXqOn+362PeuhhEvB3FAezUpDHcUKPx0uhQ/D53INydROAZHuZb57mdA90wscZGl1P7BBk9193RFg/1b413JnXF59PCr9uO+OwiZBeV4/0tZ5FfWomv9sYb3H/gYjYW/BQrD0b+80QafolORn5pJaZ8fhCv/KabcbXmUAJe/OWEPPZm/ZEklFaq8c/ZTGQU6lVu8kS4ScsvxcD3duB/Ueeh1ui6pAAYLHZ4Iw5cysYfx1NrVZj2XsjCXyfS8PZfcbVmmBl7TVZuGj9t0K+ry7U+jl7JRVmlxmioJ2oqGG6o0Wrr44LN8wajb5sWeOK2tgYbd3Zt6Y5AD0eD/a+03VmAmLXl5WwHABjfIwBvT+yGB/sFG52lVVN8drFcIQKAS1m6sTpxaQWY++MxaCRgcq+WmFM9WDo6IRdrDyficPxVg6CQdLUU648mYc2hRACGU89P6f11nVr9V/LX+68gJa8UH/1zAaM/2oOJn+6Xz8kuvvFuqUq1Bo99exRPr43BZ7sNKzT6A6rjs4tqPhSA4Tif/NLmW7lJyCnGB1vOyos/Nlba7iRje7bVhyRJcqDNLWaYpaZLdf1TiCync6AbfpodCQAIDXDFqn3xqFJLuL2T6IL64J7uuP+LfzGpd0s42Coxpps/jifl46H+wZjaNwhrDyUZVHT0p7AbY6tUILekEt/qzQ46lZKPkooqHL2Si3nrYpBXUonurdzxzqRu8l5Zp1LyYaeq+7l/+DcBGknCngtZ8rFD8brp34VlYnCxfvfW+QzDwHEjlZuLmUVo3cIJl7OLUFxdYXp/y1kM7eQj7wKfpVc5OpVSIA/gNnxN3Tm5zbhyc98X/yItvwwXM4vwxfQISzfHKEnSjZUpKK1CRkEZfF3tr/s7r6+ovAoVavE7mMf91qgJY7ihJmNSr1aY1KuVwbF2Pi44/Mod8u0VD4ZDkiT5H/Rn7uhQ63l+mNkPp1Lz8d7fZw2OD2rvjUq1Rh634GynhKOdCtlF5Vi2/QK+2nsZGgno1doD3zzSF452SnSrXp35Qmah/KFgTHJuKT7Ycs7gWM1tKdLzS5FsZACyVl5JJSrVGtgqr11wPXAxGw98dQhDO/lgfPdAg/t+i02Rw43+OJ9TKfnywov62C0laH9WBy7Vf8Vscyur1MhVw1X74rF850W8NbErpvUPrvdz6FdruN8aNWXsliKrc72/VAd18MbsIe3k271be2D7giH46uEI3KY3KHlQB28Mai9mZH2xRwSbu3oEYu1j/eXuLT83e3i72EEj3fr6OGn5Zbh8nefIrUf1Zmv1Qoi7zmXh010X5XYCwF8n0uSxN+n64aaOlaD1BxTnlVZg1b54jPzfHmQWGB8AbS3OZxQanXGkUta/CmJu+msqlVZPBz+ZnHdDz5FjUKljuKGmi+GGmq1VD0cg1N8Vb97VFe19XeBgq8Tg6hlYADC0ky+eHNpeDgah/q744J7ucLBVyucoFAp5by0AcLZTwsfVHr1aeyCiejzQksndMKabf53tcLITz7fnfJbBTCtjHlp1CBcyjE9v19LfbPRylghLT93eHk52SiTnlmLLKRF+9GfDnE4pMLoFhP6Ym6tFFXjrzzM4l1GI7/9NuGYbalJrpCazMWl2UTlG/G8PBn+ws1abC0orMePrw/jrRJqFWle3QiPTt290hp1+pa45d0NS08duKWq2hof5YXj1lhBaXQLd0bqFE3KKyjEs1Bd+bg7Y9uwQbD2VjttDfQ2CjVafNi2w65wYSzO+RyBeHdcZtkoFSivUyCupRBtvZ9zftzXOphdgxuojaOfrjP0Xdd0bc4a2w/9tO48vq2dl2ats8OrYMOw+n43tcYYbeZ7PKMLzPx/HD7P64d7PDiKohRMGtffGqZR8vDWxK4rLq3CuOvy09HCUV0DuHeyJCT0DsfZwEub8eAwrH+xtULkpLK/C6dQCdGvljktZRfhf1Hk8PayDwQDovRey5e8r1ZLe9xo89t1RuDrYYtnUnlDaiOqGWiNBaaNATnVYiGjjic+n1T1eJSGnGM72Kni72Nd5jjlc0lvs8XxmITrqjUXSSKIiVlBaibHVC1E2FkVGVty+0YUfDbshWbmhpovhhkiP0kaBX56MRFmFBn5uDgDEVPIpdUwzB4CZg0LQ1tsZDnZKRLb1kgOQvUoJDyc7+bxQfzf8+/JwJOeWYND7OwEA3Vq6Y/aQdjh8JRd7zouA5OZoi2mRbdCqhVOtcAMAx5PzsXznRZxNL8TZ9EJEVe9k3r2VO5Krw0wnP1e8MjYM01cfhpOdEh18XbFofBeUV2rwa0wK3t18FqnV4aZHkAeOJ+Vh9/lMdGvljv9uOYctp9Nrrb2TrtcVlZ5fKo9tOp6UJ4e7sABXzBnaHj8dTcLrv5/C08M6oI2XM3KKKxB1JgPF5VVw1tshvqJKgy2n01FZpcGLG07A390B2xcMMRoizUX/OqMTcuFjJGxlFDS+WVPGFt7LucEZdk15jJUkSbiUVYy23s6wsWm83YdkHgw3RDX4ujrc0PkOtkqM7lb/v+JbeTphQfWO6o8NbguV0gbv390NkUt2AAD6tBHdWW30tq7Qr8IAwOe7ay+899rvp+Xvb+vojds6+uDzaeFwc7CVp9G/NbErdp7LlBcXVNoocE/vlnJAmRbZBjvOZgLANTfq/C02Fb8fT0WvIA8EtXCSj/8v6jyGdvTFN/uvoKxSg/9uPQfX6jCjkYATyfmIrF5ZGgA2xiTjxQ0n5dvJuaVYezgRjwwMQVmlGqdS8hEe7HlDM35qOpNagGXbz+P5kZ3Q0a/2jLCaknN1P+foK7kGyw1oZRSUITm3BLZKGzkEX0+lWoPUvFJ5SxK1RoKN4vpjxOrL2F5pt9It1dTCzU9Hk/DihpN4cVSovJ8dNV8cc0NkAfOGd8C84R3gWD3eJsDdEftevB0zBrTB08PEDK+WHo7y+c+P7IgvpoVj9Qxdt47KRiFvWaHv9XGd8dyITgDENhf6YcLZXoUn9AZTqzUSbq9eH+hYYi5GLdtTa9ZXXZ+9kgQcS8zD77GpAMQq05VqCfPWxRjsiK4/FiS2xgyxYwmGtwFgxa5LKKtU460/z+Cezw5i9f4rxhtQ3X79gdxllepaixU+9eMxbDuTgUe+PlLr8dlF5Th02XAGlH6IjE7MRXZh7YBQpZEw6P2duO2DnQbT90sr6t7X6f+2ncOQ/+7C+iOJSMkrRc/F2/CSXrC7VcYqNyUV6mu2qaac4qa7IvXxZDEoPjap7lBOzQfDDVEj0crTCW/c1UWeqm2nssEdYb4IcHfAiM7+GNHFH8NC/fDto33Rp40nXhodijWz+uOHmf1wb3grqGwUWPlgbzw6KOSa3TqzBoXIr9EvpAVaeTpheKgvNJJuynNQC12wemRAiPx9hJEqhtbSKT3g5WyHi5lF0EiAl7NdrWD0/pazmLbqkLy5o3aW1rzhHRD7+p1o6eGIrMJyrNoXLy98+NafZww2g8wuKsenOy9i6bZzWLHzIm7/v11YezgR/8RloM/b2/HYd0cNXlM7A00/tGjd+9lBTP3iXxyoXq/oSnaxwd5iCTklBkGtpvIqDU5Uz0j66WgSwl7fUudgY2217cUNJ7Es6jwKy6qw/miSQRi7nFWEVzaexKLfT113cHlNRXUs3HcjXVP6s/FKKtQor2o6m3CmVb+/iVe5bQSxW4qoUftyegTUGgkqvbVthnT0qbWPVmQ7L7wyNsxgjE9dVEobbHgyEt8dTMCg9mJ22OfTwvH3qXTEJObB29UO0/oH47dY0e3UxtsZq/eLwc4zB4XgqF531bBQX7kba0gnH7wzqRtm/xANAJjYqyWizmTU2l9r74Vs3P/lv0jJLUVm9UKC94a3goeTHeYNb48XN5zEf7cargk0d80xvDQ6FCHezpjy2cFaU+YX/qqrgGyPy0RZpVoOeDYK0SUGwOB4fkmlXPXZejod5VUaPPJN7eqO9vrqsv9iDiLatMALv5wAADz7U6w82Fi72WlJjerJz9HJ8vcZBeXwdxddWx//cwG/VVfCUvPL8GWNBQPzSioQk5QHHxd7zPr2KF4Y1QmTe4u1n+ra7DKnqAKtPJ2M3ldWqcZ/t57DuO4B6NXas9YA5LySSvi5GQblf+IykJpXimmRbYz/QCxEG8wTc4oN1roi48qr1MgsKDfoVrYmrNwQNWIKhcIg2NRFaaOoV7DRcrJTYfaQdvI0dpXSBuN7BOL18Z0xZ2h7uDrYYlr/YHRt6Q4XexWmRLTCqC7+uLOz4eyytyZ2hb+bA+7vGwR7lRKjuvpjxoA2sFfZYFKvlugR5CGfqz8LKiYxTw427o62aOUpKkWTe7dCWx/dWCNfV/GYf85m4t7PD2LNocTrrgUEQN4NXr/LCIDBfkna1aUBoLhCjbWHEw3O7dFK/Gyut3DfuiOJOHJFN/1eWf2hWlapxrhP9mHcJ/uw+3xWXQ+XZ7cBwAm9LTl2ncusNWPp5Y0n8cjXR/DE99FILyjDusNJ8n3GpoID167cfLnnMlbti8ekFQcA1F4FO7ekApkFZXjoq0P46WgSknNLMPPbo3jt99MNuveUJEl44vujGPPRXpRU1G+H8tTqyk1xhRqD3t+Jt/48U6uLsqEVllXKlTxLKqtUX3NBUAB4Zm0sBn+ws9ZiotaC4YaIruuDe3rgs2nhBkGrhbMdWno44uDCYVgyubt8/I27uuDM4lHo2tIdr44NQ88gD7w9sSv+eW4Its6/rdZzazS6v7JtlTZY+1h/TOwZiC6Bbvht7kD88dQg9AzyQF5JJRZtEoOmvV2MB7lerT0A6HZ8T8krhf5SNY9+cwTRCVdxMbMQL/xyXD5+PCkPu2oEkJpBri5p+WW497OD8u0qjQaVag3OpBUgLb8Mafll8nYeMwa0qfV47bpFReVVciXJz80elWoJf53UdXGVV6mx82yWfF0AcDo1X16Lx9hUcADIvsag4vgcXVDMLCyTK03asVx5JZV4eeNJ7LuYjRd+OYFPd16Uzz91jZ3H3/v7LKZ+fhDFdQQuYyrVGizbfh7Hk/Lw54k0bD2dgTNpBQZLENSluLwKBXrXn5JXilX74vFbbEq9Xx8QXZ4fbjsn/xxu1Kxvj+Ku5fux+3wWJEnC8aQ8g8UVr2fzyTQsjTqP8io1DsdfxfAPd8mzKG/EI18fwaD3d+L8NdbE2lK92OcXe2pPTgBEF+XlrCJkFZbjq72Xb2jsVmPAbikiuiEf3dcT7/wVJ3eZGCv/a9e68XNzwG9zB8rH3R1tMaFnILafyYC/uwMuZRXjkUEhBo/1c3PAsvt6ybcDPRzx7aN9MXrZHnn6+stjwvDKxlMorVTLCy/OHtIOBy5lIyYxD6/9dgqxiXm4s7PhzvK5JZWYtuowKqo0qNJLPRcya28cGh7c4oZ+LlqVajHIOTYxr9bzj+jshz9PpBoEjrf/isO+i9l4/La2kCQgwN0Bjw4MwTub4/DTkSQ80Lc1yqs0OHg5R155WKu4Qi0HlJN1hI1LWUV1btuhP8Nq7aEkFFeoYatUINTfDSdT8nExswjb43Tdcmv1KkVn6qjcFJdX4au9l1GlkbDvYjb6t/UyumHt8aQ8nE0vwJSIICgUCmyKTcWy7Rew5VS6QTfettMZcLVXIbKdV51dTfoLUup7848zGNst0GDT3Wt5ePVhnE4twNn0wlpdgtej0UjyfnHrjyTCVqnAA18ewrjuAVj+QO/rPl6SJLz4ywkUllfh30s5CPF2xqWsYvwcnSz/jtd1/ZVqDV777RS6tnTHQ/2DcbB6kPz6I0l4bVznWudX6U0ayKtjQ9zHvjuKowm5UCjE5IGsonIsHB0GAEjOLYGd0ga+9ZwpaAkMN0R0Qyb0bIkJPWvvQ1Vf/5vSU56RtfV0er0qJO6Otnh5bBie+jEGAHBHZz8cvJSDn6OTMWdoe3lGmP5fyRuOJWPDMTG2pWtLN3g62WHvhWz5g9PfzQG3h/oYfGBP7t0S289k4I7OfujS0k0+bqeygcpGUWvszK7nh8LLxQ4zvzmKw3pdU+fSC3G8RveEncoGvYM98fH9vfDQV4fQuoUTrlQPGt51LksOH10C3TG5d0v8d9s5HE/Ox5EruVi0qe5uoKgzGVi2/TzKKo3vbfb57suISyvEt4/0gUKhwNXiCrzzVxz+vZxjMMj64x0XAABhAW7o6OeKkyn5cqXMmDOpuvboj3GJTsiVg+MT34vxV08MaYsXR4bK689IkoQ5a44hJa8Uvm4OuL2TrzyWS9ulqKV9H18aHYoDl3JwV49A3BOu22Nuy6k0gzFX+vJKKnHwcg58Xe2x7XQGZg9tC3tV7cH2MYm5OJaYh9PV16RdO8qYUyn5WPjrSbw6Ngz92upmIl7K0gVkR1sVTlTP3orRC7mA+Ln9cSIVjw1uixbOugpkdlGF3LV4+MpV+fdpR1wGer0VBQeVEnNvb2d0rNPBSzlYdyQJOJIEO70QW9eAcP21nIyt2aTWSPL7oe3Z+/tkOhaODkNWYTlGL9sLVwcV9rxwe726zS2B4YaIzMrGRgEHG/EBcyMhaWy3ABRMqoK3ix3cHGzx1sSueHpYB7T20g2I7N7Kw+hj+4d44dVxnXG1uALPrItB6xZiZpqt0sYg3CwcHYYlk7vBTmlTvbWGG06lFGDZ1J7VH6CGHxatWzjBxkaB72b2xZm0Avx8NAlrDyfh71NpOJ5kWEnp3doDDrZKDGjnjWOv3Ymc4goM/3C3fL/2A7VbS3d4udhjQo9A/BydjCmfHzR4HjuljcF0/ZobwBqz53wWpq8+jBkD2uCHfxOw81ztrg7tppvdW7nj6WEdsO9CNtILyuDmoEJogBsOV1clpkS0wk9Hk3EmrQBHrlzFC7+cQHJuCSLbecNOaWN04cnPd1+Gi50KTw8XyxzEZxfLwSrqTAZu7+SLmETDKdyDO3gbdElpr3PP+SzklVRg1uC2AIDZPxwzes1TI4Kw/mgStpxKx5nUfBxPzoeHky0e1usaXHdYNyOvWC+4ejjVrjRpTV99GFeLK/DoN0dwevEo+bj+QPvk3BLYqUSQS80vRWmFWl724f0tZ7H7fBZW7rqECT0DMXNQCLq38qhzb7riCjVQoQZQicV/nsHd4a3gZGf40a3f/fTChhPy95l1LDaZoreWU3x2scFAe8Aw/Ghpg9Km46koLK9CYXkVTqbko1frumdQWlLjjFxERDUoFAo80K81RnQR+3Q52CoNgg0gurDentgV707qhkXjdeX4tj4uAMQ4oe9n9sM7k7rJlZKOfuK+LoFu8HG1h71KKVchvpwegY1zBmBMtwDcUb1Vh4eTLVztVbgjzE+uRDjYKtG7taccrjafTK819XxAO92+ZR5Odmjn44IP7umO+/sarn7dr63oDps5OAT6C+062SlxV49AvH9PN7g6qHBXD8Md32saFuprMBV/74VszPz2aK1go7JRyAO6AREQ/dwcsOaxfnjitrb4be5APDpQ13X47J0dYatUoLCsCi9tOIH47GJUqiXsOZ9lNNhofbn3slxZ2683SPu3mBRsOZVeq2IzplsAhoUaditqrdh1CRqNZLCFCAC5EjKovTfG9RAz1tYeTpTXwNl5TtfFlnS1BC/9ehIv/XrSINgAouLzf1vP4Zhe4ErPL8POc5nyoOviCjVe/OUENsaI6qD+opeJV0vksCLV2FRXf3D577GpeOGXE5AkCfHZtbtGa6pUS3J3Z0lFFXaey4RGI+FcuvGxNfrVJH36v5tqjST/7CvVGjz14zE8szam1mMyCsqRUVCGTcdT5WP6+9g1NqzcEJFVeah/sPx9jyAP7D2fjQk96w4CS6f0xA//JmDBiI617gtwd0SAu/jgf318Z3Tyd8H4HoFwd7SFg5HujXvCW6FKI+GvE6k4HH8V/dt64VRKPgrKqjCwvVet86dEBImd5qurR+O6B6B/dVdHqL8b3pvcHS9sOAFHWyX2vnA7vKpnnE3q1Qr5pZUoLKtEXFohlDYK+QPr6xl9IEHCbR185DE0c9Yck7fIAIDOAW7ymJkqjYQpEUFYGnVe/MyqA1o7HxcsHCPGWLT0dMS47gHoHOiGAHdHeUzOpeqNWVc82BsnU/KxIy7TYPYXILoESyrUuJxVjP9uPQd/dwd8sEU31b+kQi0vH6BvUHtvTOgZiPMZRZj46X6D+64WV+BkSj6u5BhWO8Z088e94UFo7+sCO5UNvJztDKa3H7yUg9S8UpxOLcBFI+Os9C3feRHLd17EkVfugLeLHR7//qjc1aS1/mgS1h9NQkRwC4PQkpZfZrA20+XsInQOdEO23kBlbVXwbHohDl7KkWcBdm/lXut1AKCtjzMuZxXj8JWrGNDeGx9uO49V++Lx9LD2dQ4cTsgpMTreSr9yA4glCEZ09oOTvQp/6q3TZKMAHuwXjF3nM5F0tRR/nkgzmF118HIOurZ0wy/RyXh9XGdUaSTM+PoIJvdqicdua1vXj9YsFJK558pZWEFBAdzd3ZGfnw83N7frP4CI6CaUVFTBXqXEnvNZuJJTjBkD2tQ5IPSX6GTEJObilbFhtbocDl7KgYu9Ct1auRt9LCCm/oa+tgUAsHX+bejkX3ubiaNXruKe6lldL44KxftbdN1ZBxcOw5D/7oKXsx32vThMHhBel99jU/DMulgAQDsfZ/zz3FD5vh8PJUKChFMpBdgQnYyfZkficlYRFvx0vNbzdGvpbjAQum9IC5xMzkdYgCt+naMbiH7bBzuReLUEvq726NXaA1tPZ+Ch/q1xPCkfJ1Py0SPIAz4udlg8oSsC9Vb2/uHfBLz626lrXgsgqmL/m9oTBy/lICYpr9b06E5+rrVCmzHBXk5Iyy+rtQTBs3d0xDN3dMD+i9l48KtDaOPlhF3/uR2v/34K3x1MwPBQXyhtFNh2JgMvjQ412tW4eEIXvP77aQxo54UfH+uPNi/9Jd9nr7JBeZUG4cGetbZN+ee5IWjn44IqtQZrDyci2MsZm0+mYd2RJIQHe+Jkcn6tVcm17u8bhCWTu+M/Px/Hz9HJcHe0RX5pJTycbJFXUgk7pQ1cHFS4WlyBxwaL6p52A+Bzb48yOr7pVtzI5zcrN0REDUAbUm6vo2tF3z3hrQwGyerT3z6jLg62Sqx6OALx2cVyN1tN4cGeGBbqi9ikPEzu3RLdWrrj8e+P4t1J3RDg7ojtzw6BrUpx3WADAHf1CMTPR5Ox72K2PPZF64F+rQGIGTkvjQqFu5MterRyR5VawrLt5xHo4QhXBxVaONvjnUldkVlQDgc7G2yKTcWYbgFQ2ijgZGf4odirtQcSr5agX1svDGjnha2nM/DDv7p1iZ66vb3Rgen3922NX6KTEZdWgEHtvfFP9YKM+gs7fvpAb4QHe8Lf3QEju/hj2qpDtZ6nPsEGAJZM6oZ3NsfJA5O1LmUVIelqiTxuKdRffDA/MjAE3/+bgH/OZso/9y6BtT+07wjzkyt6xxJz5TV9tMqrNLBT2WBkF79a4eZCRiFaeTpi9vfR2HkuS579BAD39QnCy2PCsPjPM0bXu9Eu/tgnpAV+jk5GfqnoVpw5MAQ/Hk5EWn6Z3E33S3QyugTqAvi+C9kYHla/5RQaAsMNEZEVuN4HiUKhwFfTIyBBTNX3c3PAGb0BsTXHL13vub6YHo7ohFx5leuaVEobuDvZyOdP6ROEKX2Cap2nfd2aIUnfowNDkHS1BI8NDoG/uwNc7VUoqqiCv5sD2vo419kGpY0Cax/rj6LyKjjbK7HvQjY6B7rhs92X5HA0LNRXHuwLAA/2a429F7IxvkcgFtzZESeS8/DGptMoKq/CZw+F40RyPuKzi+WxJ+O6B6CgrAoP9muNAe290cbLuVa42XQ81WCsirayFuLtjOGhvtgelykP6A7xdpYHU786Ngy9gz3Ryc8VjrZKeQPdZ9fH1rrW9j4u6BygCxfeLnbILqrA+iNJSM0rk8da6ffVtPR0RHiwJ36fOxBjPtpba3q/dvXikV388fKvJ+VZcAM7eKNLSzc8+o1uq5PckkqDhTH/OpHGcENERA3Pph5VmfpyslNhcAef659oAj2CPAy6qXb+ZygAw1Wv6+Jop5TDi3Yw+vMjOuF8RhEigj0Ngg0gPsh/nzsQoQGusFcpq8OGD3JLKtDOxwXDw/yQUVCGzMIyTI9sgzHdAgweP7l3S3nxxUHtvXEqNR95JZUG1aKwAF234azBbeW1hHoGeSDQ3RGfPtgbR+Kv4vZOvgbv2dPD2uOlX0/K6+nc2dkPnk62OJ1agJmDQtDRX1e1+8/ITlj460nsPJclB5vFE7ogObcU3x28AnuVEp0DdFWiMd38a4eb6oHm7o62CPRwlLdS6d7SHSqlDZ4f0RGH4q+iS6A7Ptt9yeCxUWcyas3CMieOuSEiIjKhw/FX8c2BeDx+WzuE+ruisKwKbo4qfHcgASdS8vHfe7obfOhvPZ0OpUKB26vH3tSlUq3BuI/3yd1kn9zfC+P1Zs1JkoRhH+5GdmE5Diwchtd+OyXvVebtIsZTOdgqUVReBY0kwc1BN+W9rFKND7acQ8/WHphXPVsq+tU75EHs2uUE7glvhf+7t0etdoW/FYWCsir0bu2Bjn6uGNjeGyO7+Nd7AcX6uJHPb4YbIiKiJqKwrBJRZzJQqdbg3vCgWtW4kooqlFVq0MLZDoVllfhw23lEncnAgjs74u46xnXVtPNcJsoq1Bhdoyp1JbsY/u4ORqsxSVdL8PE/FzAtMrjO9aZuFcPNNTDcEBERNT038vnNRfyIiIjIqjDcEBERkVVhuCEiIiKrwnBDREREVoXhhoiIiKwKww0RERFZFYYbIiIisioMN0RERGRVGG6IiIjIqjDcEBERkVWxeLhZsWIFQkJC4ODggPDwcOzdu/ea5+/evRvh4eFwcHBA27Zt8dlnn5mppURERNQUWDTcrF+/HvPnz8crr7yCmJgYDB48GKNHj0ZiYqLR8+Pj4zFmzBgMHjwYMTExePnllzFv3jxs2LDBzC0nIiKixsqiG2f269cPvXv3xsqVK+VjYWFhmDhxIpYsWVLr/BdffBGbNm1CXFycfGz27Nk4fvw4Dh48WK/X5MaZRERETU+T2DizoqIC0dHRGDFihMHxESNG4MCBA0Yfc/DgwVrnjxw5EkePHkVlZaXRx5SXl6OgoMDgi4iIiKyXylIvnJ2dDbVaDT8/P4Pjfn5+SE9PN/qY9PR0o+dXVVUhOzsbAQEBtR6zZMkSvPnmm7WOM+QQERE1HdrP7fp0OFks3GgpFAqD25Ik1Tp2vfONHddauHAhFixYIN9OSUlB586dERQUdLNNJiIiIgspLCyEu7v7Nc+xWLjx9vaGUqmsVaXJzMysVZ3R8vf3N3q+SqWCl5eX0cfY29vD3t5evu3i4oKkpCS4urpeM0TdqIKCAgQFBSEpKckqx/JY+/UB1n+N1n59gPVfo7VfH2D912jt1wc03DVKkoTCwkIEBgZe91yLhRs7OzuEh4cjKioKkyZNko9HRUVhwoQJRh8TGRmJP/74w+DYtm3bEBERAVtb23q9ro2NDVq1anXzDb8ONzc3q/2FBaz/+gDrv0Zrvz7A+q/R2q8PsP5rtPbrAxrmGq9XsdGy6FTwBQsW4KuvvsLq1asRFxeHZ599FomJiZg9ezYA0aU0ffp0+fzZs2cjISEBCxYsQFxcHFavXo1Vq1bh+eeft9QlEBERUSNj0TE3U6dORU5ODhYvXoy0tDR07doVmzdvRnBwMAAgLS3NYM2bkJAQbN68Gc8++yw+/fRTBAYG4uOPP8bdd99tqUsgIiKiRsbiA4rnzJmDOXPmGL3vm2++qXVsyJAhOHbsWAO36sbZ29tj0aJFBuN7rIm1Xx9g/ddo7dcHWP81Wvv1AdZ/jdZ+fUDjuEaLLuJHREREZGoW31uKiIiIyJQYboiIiMiqMNwQERGRVWG4ISIiIqvCcGMCK1asQEhICBwcHBAeHo69e/daukk37Y033oBCoTD48vf3l++XJAlvvPEGAgMD4ejoiKFDh+L06dMWbPG17dmzB+PHj0dgYCAUCgV+++03g/vrcz3l5eV4+umn4e3tDWdnZ9x1111ITk4241Vc2/WuccaMGbXe0/79+xuc05ivccmSJejTpw9cXV3h6+uLiRMn4ty5cwbnNOX3sT7X19Tfw5UrV6J79+7yom6RkZH4+++/5fub8vsHXP/6mvr7V9OSJUugUCgwf/58+Vhjew8Zbm7R+vXrMX/+fLzyyiuIiYnB4MGDMXr0aIP1eZqaLl26IC0tTf46efKkfN8HH3yApUuXYvny5Thy5Aj8/f1x5513orCw0IItrltxcTF69OiB5cuXG72/Ptczf/58bNy4EevWrcO+fftQVFSEcePGQa1Wm+syrul61wgAo0aNMnhPN2/ebHB/Y77G3bt3Y+7cufj3338RFRWFqqoqjBgxAsXFxfI5Tfl9rM/1AU37PWzVqhXee+89HD16FEePHsWwYcMwYcIE+cOvKb9/wPWvD2ja75++I0eO4IsvvkD37t0Njje691CiW9K3b19p9uzZBsdCQ0Oll156yUItujWLFi2SevToYfQ+jUYj+fv7S++99558rKysTHJ3d5c+++wzM7Xw5gGQNm7cKN+uz/Xk5eVJtra20rp16+RzUlJSJBsbG2nLli1ma3t91bxGSZKkhx9+WJowYUKdj2lq15iZmSkBkHbv3i1JkvW9jzWvT5Ks7z2UJEny9PSUvvrqK6t7/7S01ydJ1vP+FRYWSh06dJCioqKkIUOGSM8884wkSY3z/0FWbm5BRUUFoqOjMWLECIPjI0aMwIEDByzUqlt34cIFBAYGIiQkBPfddx8uX74MAIiPj0d6errB9drb22PIkCFN8nrrcz3R0dGorKw0OCcwMBBdu3ZtUte8a9cu+Pr6omPHjnjssceQmZkp39fUrjE/Px8A0KJFCwDW9z7WvD4ta3kP1Wo11q1bh+LiYkRGRlrd+1fz+rSs4f2bO3cuxo4dizvuuMPgeGN8Dy2+QnFTlp2dDbVaXWsXcz8/v1q7lzcV/fr1w3fffYeOHTsiIyMDb7/9NgYMGIDTp0/L12TsehMSEizR3FtSn+tJT0+HnZ0dPD09a53TVN7j0aNH495770VwcDDi4+Px2muvYdiwYYiOjoa9vX2TukZJkrBgwQIMGjQIXbt2BWBd76Ox6wOs4z08efIkIiMjUVZWBhcXF2zcuBGdO3eWP9ia+vtX1/UB1vH+rVu3DtHR0Th69Git+xrj/4MMNyagUCgMbkuSVOtYUzF69Gj5+27duiEyMhLt2rXDt99+Kw+As6brBW7ueprSNU+dOlX+vmvXroiIiEBwcDD++usvTJ48uc7HNcZrfOqpp3DixAns27ev1n3W8D7WdX3W8B526tQJsbGxyMvLw4YNG/Dwww9j9+7d8v1N/f2r6/o6d+7c5N+/pKQkPPPMM9i2bRscHBzqPK8xvYfslroF3t7eUCqVtVJnZmZmrQTbVDk7O6Nbt264cOGCPGvKWq63Ptfj7++PiooK5Obm1nlOUxMQEIDg4GBcuHABQNO5xqeffhqbNm3Czp070apVK/m4tbyPdV2fMU3xPbSzs0P79u0RERGBJUuWoEePHvjoo4+s5v2r6/qMaWrvX3R0NDIzMxEeHg6VSgWVSoXdu3fj448/hkqlktvYmN5DhptbYGdnh/DwcERFRRkcj4qKwoABAyzUKtMqLy9HXFwcAgICEBISAn9/f4PrraiowO7du5vk9dbnesLDw2Fra2twTlpaGk6dOtUkrxkAcnJykJSUhICAAACN/xolScJTTz2FX3/9FTt27EBISIjB/U39fbze9RnT1N5DYyRJQnl5eZN//+qivT5jmtr7N3z4cJw8eRKxsbHyV0REBB588EHExsaibdu2je89NPkQ5WZm3bp1kq2trbRq1SrpzJkz0vz58yVnZ2fpypUrlm7aTXnuueekXbt2SZcvX5b+/fdfady4cZKrq6t8Pe+9957k7u4u/frrr9LJkyel+++/XwoICJAKCgos3HLjCgsLpZiYGCkmJkYCIC1dulSKiYmREhISJEmq3/XMnj1batWqlbR9+3bp2LFj0rBhw6QePXpIVVVVlrosA9e6xsLCQum5556TDhw4IMXHx0s7d+6UIiMjpZYtWzaZa3zyyScld3d3adeuXVJaWpr8VVJSIp/TlN/H612fNbyHCxculPbs2SPFx8dLJ06ckF5++WXJxsZG2rZtmyRJTfv9k6RrX581vH/G6M+WkqTG9x4y3JjAp59+KgUHB0t2dnZS7969DaZwNjVTp06VAgICJFtbWykwMFCaPHmydPr0afl+jUYjLVq0SPL395fs7e2l2267TTp58qQFW3xtO3fulADU+nr44YclSarf9ZSWlkpPPfWU1KJFC8nR0VEaN26clJiYaIGrMe5a11hSUiKNGDFC8vHxkWxtbaXWrVtLDz/8cK32N+ZrNHZtAKSvv/5aPqcpv4/Xuz5reA8fffRR+d9IHx8fafjw4XKwkaSm/f5J0rWvzxreP2NqhpvG9h4qJEmSTF8PIiIiIrIMjrkhIiIiq8JwQ0RERFaF4YaIiIisCsMNERERWRWGGyIiIrIqDDdERERkVRhuiIiIyKow3BARQWz699tvv1m6GURkAgw3RGRxM2bMgEKhqPU1atQoSzeNiJoglaUbQEQEAKNGjcLXX39tcMze3t5CrSGipoyVGyJqFOzt7eHv72/w5enpCUB0Ga1cuRKjR4+Go6MjQkJC8PPPPxs8/uTJkxg2bBgcHR3h5eWFxx9/HEVFRQbnrF69Gl26dIG9vT0CAgLw1FNPGdyfnZ2NSZMmwcnJCR06dMCmTZsa9qKJqEEw3BBRk/Daa6/h7rvvxvHjx/HQQw/h/vvvR1xcHACgpKQEo0aNgqenJ44cOYKff/4Z27dvNwgvK1euxNy5c/H444/j5MmT2LRpE9q3b2/wGm+++SamTJmCEydOYMyYMXjwwQdx9epVs14nEZlAg2zHSUR0Ax5++GFJqVRKzs7OBl+LFy+WJEnsnD179myDx/Tr10968sknJUmSpC+++ELy9PSUioqK5Pv/+usvycbGRkpPT5ckSZICAwOlV155pc42AJBeffVV+XZRUZGkUCikv//+22TXSUTmwTE3RNQo3H777Vi5cqXBsRYtWsjfR0ZGGtwXGRmJ2NhYAEBcXBx69OgBZ2dn+f6BAwdCo9Hg3LlzUCgUSE1NxfDhw6/Zhu7du8vfOzs7w9XVFZmZmTd7SURkIQw3RNQoODs71+omuh6FQgEAkCRJ/t7YOY6OjvV6Pltb21qP1Wg0N9QmIrI8jrkhoibh33//rXU7NDQUANC5c2fExsaiuLhYvn///v2wsbFBx44d4erqijZt2uCff/4xa5uJyDJYuSGiRqG8vBzp6ekGx1QqFby9vQEAP//8MyIiIjBo0CCsWbMGhw8fxqpVqwAADz74IBYtWoSHH34Yb7zxBrKysvD0009j2rRp8PPzAwC88cYbmD17Nnx9fTF69GgUFhZi//79ePrpp817oUTU4BhuiKhR2LJlCwICAgyOderUCWfPngUgZjKtW7cOc+bMgb+/P9asWYPOnTsDAJycnLB161Y888wz6NOnD5ycnHD33Xdj6dKl8nM9/PDDKCsrw//+9z88//zz8Pb2xj333GO+CyQis1FIkiRZuhFERNeiUCiwceNGTJw40dJNIaImgGNuiIiIyKow3BAREZFV4ZgbImr02HtORDeClRsiIiKyKgw3REREZFUYboiIiMiqMNwQERGRVWG4ISIiIqvCcENERERWheGGiIiIrArDDREREVkVhhsiIiKyKv8Ph3bNj1U67lsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(train_eva_df,'Train','Epoch','Loss',['loss','accuracy'])\n",
    "plot_graph(test_eva_df,'Test','Epoch','Loss',['loss','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e98088e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
