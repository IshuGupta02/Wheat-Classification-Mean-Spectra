{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c4fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35daa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6aebd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'wheat_40_var_avg_new.csv'\n",
    "\n",
    "def dir(file_name):\n",
    "    return '../data/'+file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a19daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_X_Y(dataframe):\n",
    "    return (dataframe.drop('classes', axis =1), dataframe.loc[:,'classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554bc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class filter_method(Enum):\n",
    "    none = 0\n",
    "    snv = 1\n",
    "    msc = 2\n",
    "    savgol = 3\n",
    "\n",
    "FILT = 2\n",
    "FILTER = filter_method(FILT).name\n",
    "\n",
    "# to be set if filter chosen is savgol\n",
    "WINDOW = 7\n",
    "ORDER = 2\n",
    "DERIVATIVE = 1\n",
    "\n",
    "#will be used for test dataset in msc\n",
    "reference= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c20231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def create_test_train(X, y, test_size = 0.2, shuffle = True):\n",
    "    return train_test_split(X,y, test_size = test_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e9301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Normal Variate\n",
    "def snv(input_data):\n",
    "  \n",
    "    # Define a new array and populate it with the corrected data  \n",
    "    output_data = np.zeros_like(input_data)\n",
    "    for i in range(input_data.shape[0]):\n",
    " \n",
    "        # Apply correction\n",
    "        output_data[i,:] = (input_data[i,:] - np.mean(input_data[i,:])) / np.std(input_data[i,:])\n",
    " \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d925acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicative scatter correction\n",
    "def msc(input_data, reference=None):\n",
    "#     print(reference)\n",
    "    ''' Perform Multiplicative scatter correction'''\n",
    "\n",
    "    # Baseline correction\n",
    "    for i in range(input_data.shape[0]):\n",
    "        input_data[i,:] -= input_data[i,:].mean()\n",
    "\n",
    "    # Get the reference spectrum. If not given, estimate from the mean    \n",
    "    if reference is None:    \n",
    "        # Calculate mean\n",
    "        matm = np.mean(input_data, axis=0)\n",
    "    else:\n",
    "        matm = reference\n",
    "\n",
    "    # Define a new data matrix and populate it with the corrected data    \n",
    "    output_data = np.zeros_like(input_data)\n",
    "    for i in range(input_data.shape[0]):\n",
    "        # Run regression\n",
    "        fit = np.polyfit(matm, input_data[i,:], 1, full=True)\n",
    "        # Apply correction\n",
    "        output_data[i,:] = (input_data[i,:] - fit[0][1]) / fit[0][0] \n",
    "\n",
    "    return (output_data, matm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b5090be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter, general_gaussian\n",
    "def savgol(input_data):\n",
    "    w = WINDOW\n",
    "    p = ORDER\n",
    "    d = DERIVATIVE\n",
    "    \n",
    "    output_data = savgol_filter(np.array(input_data), w, polyorder = p, deriv=d)\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68affd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X,y, type=\"train\"):\n",
    "    if FILTER == \"snv\":\n",
    "        return {\"X\": snv(np.array(X)), \"y\": y}\n",
    "    elif FILTER == \"msc\":\n",
    "        msc_output = msc(np.array(X), reference = reference if type==\"test\" else None)\n",
    "        X = msc_output[0]\n",
    "        ref = msc_output[1]\n",
    "        return {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"ref\": ref\n",
    "        }\n",
    "    elif FILTER == \"savgol\":\n",
    "        return {\n",
    "            \"X\": savgol(X),\n",
    "            \"y\": y\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"X\":X,\n",
    "            \"y\":y\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8dec1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dir(file_name))\n",
    "X,y = seperate_X_Y(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58535347",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_set = set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8c8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_CLASSES = len(y_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd357e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = create_test_train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f79eb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_results = preprocess_data(X_train_raw,y_train_raw)\n",
    "X_train, y_train = preprocessed_results[\"X\"], preprocessed_results[\"y\"]\n",
    "\n",
    "if FILTER == \"msc\":\n",
    "    reference = preprocessed_results[\"ref\"]\n",
    "    \n",
    "preprocessed_results_test = preprocess_data(X_test_raw, y_test_raw, type=\"test\")\n",
    "X_test, y_test = preprocessed_results_test[\"X\"], preprocessed_results_test[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e1e570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64512, 147, 1)\n",
      "(16128, 147, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "print(X_train.shape)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec820add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Conv1D, Flatten, Dense, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba682707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(input_shape,num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=5, input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=5 ))\n",
    "    model.add(Conv1D(filters=32, kernel_size=5))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d95ace51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel(X_train.shape[1:],NUM_OF_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33a0377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 143, 32)           192       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 28, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 24, 32)            5152      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 4, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              129000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 4004      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,348\n",
      "Trainable params: 138,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20cd1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eva = []\n",
    "test_eva = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fd5eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dataframe = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "863f63f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  1\n",
      "252/252 - 7s - loss: 1.2123 - accuracy: 0.4410 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 6ms/step - loss: 1.1332 - accuracy: 0.5239\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 1.1348 - accuracy: 0.5182\n",
      "\n",
      "Epoch:  2\n",
      "252/252 - 6s - loss: 1.1042 - accuracy: 0.5303 - 6s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 1.0594 - accuracy: 0.5539\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 1.0602 - accuracy: 0.5510\n",
      "\n",
      "Epoch:  3\n",
      "252/252 - 5s - loss: 1.0053 - accuracy: 0.5687 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.9256 - accuracy: 0.6024\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.9262 - accuracy: 0.5996\n",
      "\n",
      "Epoch:  4\n",
      "252/252 - 5s - loss: 0.8987 - accuracy: 0.6113 - 5s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.8706 - accuracy: 0.6141\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.8707 - accuracy: 0.6124\n",
      "\n",
      "Epoch:  5\n",
      "252/252 - 6s - loss: 0.8421 - accuracy: 0.6291 - 6s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.7970 - accuracy: 0.6441\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.7970 - accuracy: 0.6434\n",
      "\n",
      "Epoch:  6\n",
      "252/252 - 5s - loss: 0.7733 - accuracy: 0.6573 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.7786 - accuracy: 0.6545\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.7816 - accuracy: 0.6531\n",
      "\n",
      "Epoch:  7\n",
      "252/252 - 5s - loss: 0.7162 - accuracy: 0.6887 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.6700 - accuracy: 0.7088\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.6723 - accuracy: 0.7085\n",
      "\n",
      "Epoch:  8\n",
      "252/252 - 6s - loss: 0.6600 - accuracy: 0.7155 - 6s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.6164 - accuracy: 0.7407\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.6190 - accuracy: 0.7374\n",
      "\n",
      "Epoch:  9\n",
      "252/252 - 5s - loss: 0.6182 - accuracy: 0.7354 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.5745 - accuracy: 0.7634\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.5782 - accuracy: 0.7638\n",
      "\n",
      "Epoch:  10\n",
      "252/252 - 5s - loss: 0.5799 - accuracy: 0.7516 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.6553 - accuracy: 0.6965\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.6564 - accuracy: 0.6962\n",
      "\n",
      "Epoch:  11\n",
      "252/252 - 5s - loss: 0.5532 - accuracy: 0.7646 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.5323 - accuracy: 0.7762\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.5363 - accuracy: 0.7742\n",
      "\n",
      "Epoch:  12\n",
      "252/252 - 5s - loss: 0.5258 - accuracy: 0.7777 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.5624 - accuracy: 0.7497\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.5637 - accuracy: 0.7509\n",
      "\n",
      "Epoch:  13\n",
      "252/252 - 5s - loss: 0.5180 - accuracy: 0.7794 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.5062 - accuracy: 0.7865\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.5084 - accuracy: 0.7845\n",
      "\n",
      "Epoch:  14\n",
      "252/252 - 5s - loss: 0.5196 - accuracy: 0.7780 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.5036 - accuracy: 0.7898\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.5071 - accuracy: 0.7913\n",
      "\n",
      "Epoch:  15\n",
      "252/252 - 5s - loss: 0.4927 - accuracy: 0.7912 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4794 - accuracy: 0.8029\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4829 - accuracy: 0.8007\n",
      "\n",
      "Epoch:  16\n",
      "252/252 - 5s - loss: 0.4835 - accuracy: 0.7963 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4798 - accuracy: 0.8014\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4839 - accuracy: 0.8009\n",
      "\n",
      "Epoch:  17\n",
      "252/252 - 5s - loss: 0.4801 - accuracy: 0.7974 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4713 - accuracy: 0.8054\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4746 - accuracy: 0.8032\n",
      "\n",
      "Epoch:  18\n",
      "252/252 - 5s - loss: 0.4723 - accuracy: 0.8007 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4459 - accuracy: 0.8154\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4480 - accuracy: 0.8150\n",
      "\n",
      "Epoch:  19\n",
      "252/252 - 5s - loss: 0.4749 - accuracy: 0.8001 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.4374 - accuracy: 0.8185\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4400 - accuracy: 0.8148\n",
      "\n",
      "Epoch:  20\n",
      "252/252 - 5s - loss: 0.4647 - accuracy: 0.8036 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4332 - accuracy: 0.8224\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4364 - accuracy: 0.8218\n",
      "\n",
      "Epoch:  21\n",
      "252/252 - 5s - loss: 0.4572 - accuracy: 0.8084 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4271 - accuracy: 0.8226\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4295 - accuracy: 0.8189\n",
      "\n",
      "Epoch:  22\n",
      "252/252 - 5s - loss: 0.4493 - accuracy: 0.8116 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4198 - accuracy: 0.8258\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4219 - accuracy: 0.8209\n",
      "\n",
      "Epoch:  23\n",
      "252/252 - 5s - loss: 0.4415 - accuracy: 0.8159 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4157 - accuracy: 0.8284\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4189 - accuracy: 0.8252\n",
      "\n",
      "Epoch:  24\n",
      "252/252 - 5s - loss: 0.4422 - accuracy: 0.8155 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4360 - accuracy: 0.8195\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4403 - accuracy: 0.8176\n",
      "\n",
      "Epoch:  25\n",
      "252/252 - 5s - loss: 0.4364 - accuracy: 0.8179 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4113 - accuracy: 0.8299\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4134 - accuracy: 0.8244\n",
      "\n",
      "Epoch:  26\n",
      "252/252 - 5s - loss: 0.4229 - accuracy: 0.8237 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.4082 - accuracy: 0.8328\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4105 - accuracy: 0.8312\n",
      "\n",
      "Epoch:  27\n",
      "252/252 - 5s - loss: 0.4267 - accuracy: 0.8224 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4028 - accuracy: 0.8367\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4036 - accuracy: 0.8376\n",
      "\n",
      "Epoch:  28\n",
      "252/252 - 6s - loss: 0.4246 - accuracy: 0.8213 - 6s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.4303 - accuracy: 0.8208\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.4329 - accuracy: 0.8224\n",
      "\n",
      "Epoch:  29\n",
      "252/252 - 5s - loss: 0.4201 - accuracy: 0.8248 - 5s/epoch - 22ms/step\n",
      "for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.4104 - accuracy: 0.8300\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4127 - accuracy: 0.8309\n",
      "\n",
      "Epoch:  30\n",
      "252/252 - 5s - loss: 0.4160 - accuracy: 0.8264 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.4080 - accuracy: 0.8289\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4099 - accuracy: 0.8263\n",
      "\n",
      "Epoch:  31\n",
      "252/252 - 5s - loss: 0.4174 - accuracy: 0.8254 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.4363 - accuracy: 0.8158\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4397 - accuracy: 0.8154\n",
      "\n",
      "Epoch:  32\n",
      "252/252 - 5s - loss: 0.4141 - accuracy: 0.8279 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.4148 - accuracy: 0.8265\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4183 - accuracy: 0.8228\n",
      "\n",
      "Epoch:  33\n",
      "252/252 - 5s - loss: 0.3992 - accuracy: 0.8339 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.4227 - accuracy: 0.8181\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4210 - accuracy: 0.8177\n",
      "\n",
      "Epoch:  34\n",
      "252/252 - 5s - loss: 0.4055 - accuracy: 0.8321 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.4855 - accuracy: 0.7907\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4919 - accuracy: 0.7917\n",
      "\n",
      "Epoch:  35\n",
      "252/252 - 5s - loss: 0.3903 - accuracy: 0.8370 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.4487 - accuracy: 0.8139\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.4530 - accuracy: 0.8120\n",
      "\n",
      "Epoch:  36\n",
      "252/252 - 5s - loss: 0.4013 - accuracy: 0.8339 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.4083 - accuracy: 0.8307\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.4096 - accuracy: 0.8312\n",
      "\n",
      "Epoch:  37\n",
      "252/252 - 5s - loss: 0.3861 - accuracy: 0.8402 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.3551 - accuracy: 0.8564\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3564 - accuracy: 0.8575\n",
      "\n",
      "Epoch:  38\n",
      "252/252 - 5s - loss: 0.3819 - accuracy: 0.8411 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.3694 - accuracy: 0.8482\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3712 - accuracy: 0.8464\n",
      "\n",
      "Epoch:  39\n",
      "252/252 - 5s - loss: 0.3952 - accuracy: 0.8355 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.3555 - accuracy: 0.8547\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3581 - accuracy: 0.8512\n",
      "\n",
      "Epoch:  40\n",
      "252/252 - 5s - loss: 0.3824 - accuracy: 0.8418 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.3724 - accuracy: 0.8459\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3758 - accuracy: 0.8457\n",
      "\n",
      "Epoch:  41\n",
      "252/252 - 6s - loss: 0.3701 - accuracy: 0.8475 - 6s/epoch - 24ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.3529 - accuracy: 0.8564\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3555 - accuracy: 0.8529\n",
      "\n",
      "Epoch:  42\n",
      "252/252 - 6s - loss: 0.3710 - accuracy: 0.8458 - 6s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 5ms/step - loss: 0.3562 - accuracy: 0.8531\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3587 - accuracy: 0.8508\n",
      "\n",
      "Epoch:  43\n",
      "252/252 - 5s - loss: 0.3727 - accuracy: 0.8457 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.3530 - accuracy: 0.8547\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3541 - accuracy: 0.8548\n",
      "\n",
      "Epoch:  44\n",
      "252/252 - 5s - loss: 0.3593 - accuracy: 0.8511 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.3334 - accuracy: 0.8664\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3341 - accuracy: 0.8651\n",
      "\n",
      "Epoch:  45\n",
      "252/252 - 5s - loss: 0.3592 - accuracy: 0.8512 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 17s 8ms/step - loss: 0.4093 - accuracy: 0.8245\n",
      "for testing\n",
      "504/504 [==============================] - 4s 7ms/step - loss: 0.4088 - accuracy: 0.8258\n",
      "\n",
      "Epoch:  46\n",
      "252/252 - 9s - loss: 0.3560 - accuracy: 0.8535 - 9s/epoch - 35ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.3327 - accuracy: 0.8660\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3337 - accuracy: 0.8641\n",
      "\n",
      "Epoch:  47\n",
      "252/252 - 5s - loss: 0.3554 - accuracy: 0.8523 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.3544 - accuracy: 0.8519\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3535 - accuracy: 0.8514\n",
      "\n",
      "Epoch:  48\n",
      "252/252 - 6s - loss: 0.3572 - accuracy: 0.8536 - 6s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 12s 6ms/step - loss: 0.3930 - accuracy: 0.8365\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.3950 - accuracy: 0.8344\n",
      "\n",
      "Epoch:  49\n",
      "252/252 - 6s - loss: 0.3442 - accuracy: 0.8587 - 6s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.3472 - accuracy: 0.8553\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3464 - accuracy: 0.8568\n",
      "\n",
      "Epoch:  50\n",
      "252/252 - 4s - loss: 0.3420 - accuracy: 0.8602 - 4s/epoch - 14ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3164 - accuracy: 0.8719\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3159 - accuracy: 0.8731\n",
      "\n",
      "Epoch:  51\n",
      "252/252 - 4s - loss: 0.3282 - accuracy: 0.8671 - 4s/epoch - 14ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.4271 - accuracy: 0.8160\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.4312 - accuracy: 0.8156\n",
      "\n",
      "Epoch:  52\n",
      "252/252 - 4s - loss: 0.3349 - accuracy: 0.8632 - 4s/epoch - 14ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3369 - accuracy: 0.8608\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3372 - accuracy: 0.8606\n",
      "\n",
      "Epoch:  53\n",
      "252/252 - 4s - loss: 0.3304 - accuracy: 0.8638 - 4s/epoch - 14ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3063 - accuracy: 0.8771\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3077 - accuracy: 0.8754\n",
      "\n",
      "Epoch:  54\n",
      "252/252 - 4s - loss: 0.3323 - accuracy: 0.8633 - 4s/epoch - 14ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3114 - accuracy: 0.8729\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3117 - accuracy: 0.8734\n",
      "\n",
      "Epoch:  55\n",
      "252/252 - 4s - loss: 0.3303 - accuracy: 0.8634 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3056 - accuracy: 0.8778\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3064 - accuracy: 0.8774\n",
      "\n",
      "Epoch:  56\n",
      "252/252 - 4s - loss: 0.3295 - accuracy: 0.8649 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3453 - accuracy: 0.8552\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3489 - accuracy: 0.8549\n",
      "\n",
      "Epoch:  57\n",
      "252/252 - 4s - loss: 0.3174 - accuracy: 0.8698 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3070 - accuracy: 0.8747\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3082 - accuracy: 0.8751\n",
      "\n",
      "Epoch:  58\n",
      "252/252 - 4s - loss: 0.3163 - accuracy: 0.8724 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2999 - accuracy: 0.8779\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3019 - accuracy: 0.8766\n",
      "\n",
      "Epoch:  59\n",
      "252/252 - 4s - loss: 0.3169 - accuracy: 0.8695 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3106 - accuracy: 0.8727\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3116 - accuracy: 0.8712\n",
      "\n",
      "Epoch:  60\n",
      "252/252 - 4s - loss: 0.3113 - accuracy: 0.8736 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2820 - accuracy: 0.8868\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2847 - accuracy: 0.8875\n",
      "\n",
      "Epoch:  61\n",
      "252/252 - 4s - loss: 0.3001 - accuracy: 0.8782 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3055 - accuracy: 0.8739\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3079 - accuracy: 0.8719\n",
      "\n",
      "Epoch:  62\n",
      "252/252 - 4s - loss: 0.3042 - accuracy: 0.8763 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3121 - accuracy: 0.8723\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3129 - accuracy: 0.8728\n",
      "\n",
      "Epoch:  63\n",
      "252/252 - 4s - loss: 0.2976 - accuracy: 0.8792 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3218 - accuracy: 0.8664\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3220 - accuracy: 0.8666\n",
      "\n",
      "Epoch:  64\n",
      "252/252 - 4s - loss: 0.3015 - accuracy: 0.8767 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2785 - accuracy: 0.8904\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2795 - accuracy: 0.8899\n",
      "\n",
      "Epoch:  65\n",
      "252/252 - 4s - loss: 0.2974 - accuracy: 0.8789 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2920 - accuracy: 0.8814\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2910 - accuracy: 0.8823\n",
      "\n",
      "Epoch:  66\n",
      "252/252 - 4s - loss: 0.2922 - accuracy: 0.8814 - 4s/epoch - 14ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3111 - accuracy: 0.8723\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.3124 - accuracy: 0.8748\n",
      "\n",
      "Epoch:  67\n",
      "252/252 - 4s - loss: 0.3004 - accuracy: 0.8770 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2878 - accuracy: 0.8823\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2910 - accuracy: 0.8798\n",
      "\n",
      "Epoch:  68\n",
      "252/252 - 4s - loss: 0.2884 - accuracy: 0.8811 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2644 - accuracy: 0.8949\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2658 - accuracy: 0.8947\n",
      "\n",
      "Epoch:  69\n",
      "252/252 - 4s - loss: 0.2971 - accuracy: 0.8783 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3094 - accuracy: 0.8692\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3091 - accuracy: 0.8702\n",
      "\n",
      "Epoch:  70\n",
      "252/252 - 4s - loss: 0.2860 - accuracy: 0.8835 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2684 - accuracy: 0.8938\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2701 - accuracy: 0.8943\n",
      "\n",
      "Epoch:  71\n",
      "252/252 - 4s - loss: 0.2826 - accuracy: 0.8846 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3432 - accuracy: 0.8522\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3478 - accuracy: 0.8524\n",
      "\n",
      "Epoch:  72\n",
      "252/252 - 4s - loss: 0.2735 - accuracy: 0.8904 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2693 - accuracy: 0.8911\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2714 - accuracy: 0.8919\n",
      "\n",
      "Epoch:  73\n",
      "252/252 - 4s - loss: 0.2811 - accuracy: 0.8865 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2606 - accuracy: 0.8968\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2607 - accuracy: 0.8971\n",
      "\n",
      "Epoch:  74\n",
      "252/252 - 4s - loss: 0.2717 - accuracy: 0.8900 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2425 - accuracy: 0.9055\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2442 - accuracy: 0.9036\n",
      "\n",
      "Epoch:  75\n",
      "252/252 - 4s - loss: 0.2738 - accuracy: 0.8902 - 4s/epoch - 15ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2561 - accuracy: 0.8980\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2585 - accuracy: 0.8967\n",
      "\n",
      "Epoch:  76\n",
      "252/252 - 4s - loss: 0.2681 - accuracy: 0.8919 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2600 - accuracy: 0.8961\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2610 - accuracy: 0.8965\n",
      "\n",
      "Epoch:  77\n",
      "252/252 - 4s - loss: 0.2664 - accuracy: 0.8920 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2418 - accuracy: 0.9060\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2428 - accuracy: 0.9065\n",
      "\n",
      "Epoch:  78\n",
      "252/252 - 4s - loss: 0.2786 - accuracy: 0.8859 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2635 - accuracy: 0.8955\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2651 - accuracy: 0.8952\n",
      "\n",
      "Epoch:  79\n",
      "252/252 - 4s - loss: 0.2661 - accuracy: 0.8928 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2367 - accuracy: 0.9067\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2400 - accuracy: 0.9056\n",
      "\n",
      "Epoch:  80\n",
      "252/252 - 4s - loss: 0.2564 - accuracy: 0.8970 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2554 - accuracy: 0.8975\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2588 - accuracy: 0.8954\n",
      "\n",
      "Epoch:  81\n",
      "252/252 - 4s - loss: 0.2553 - accuracy: 0.8964 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2448 - accuracy: 0.9015\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2479 - accuracy: 0.9005\n",
      "\n",
      "Epoch:  82\n",
      "252/252 - 4s - loss: 0.2581 - accuracy: 0.8952 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2658 - accuracy: 0.8886\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2699 - accuracy: 0.8875\n",
      "\n",
      "Epoch:  83\n",
      "252/252 - 4s - loss: 0.2609 - accuracy: 0.8953 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2688 - accuracy: 0.8898\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2723 - accuracy: 0.8906\n",
      "\n",
      "Epoch:  84\n",
      "252/252 - 4s - loss: 0.2589 - accuracy: 0.8949 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2400 - accuracy: 0.9047\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2430 - accuracy: 0.9022\n",
      "\n",
      "Epoch:  85\n",
      "252/252 - 4s - loss: 0.2542 - accuracy: 0.8979 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2551 - accuracy: 0.8945\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2570 - accuracy: 0.8948\n",
      "\n",
      "Epoch:  86\n",
      "252/252 - 4s - loss: 0.2552 - accuracy: 0.8981 - 4s/epoch - 16ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2452 - accuracy: 0.9007\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2487 - accuracy: 0.8981\n",
      "\n",
      "Epoch:  87\n",
      "252/252 - 4s - loss: 0.2512 - accuracy: 0.8990 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2561 - accuracy: 0.8945\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2589 - accuracy: 0.8943\n",
      "\n",
      "Epoch:  88\n",
      "252/252 - 4s - loss: 0.2445 - accuracy: 0.9019 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2958 - accuracy: 0.8786\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2992 - accuracy: 0.8782\n",
      "\n",
      "Epoch:  89\n",
      "252/252 - 4s - loss: 0.2441 - accuracy: 0.9024 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2163 - accuracy: 0.9150\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2185 - accuracy: 0.9153\n",
      "\n",
      "Epoch:  90\n",
      "252/252 - 4s - loss: 0.2494 - accuracy: 0.8993 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2638 - accuracy: 0.8938\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2656 - accuracy: 0.8929\n",
      "\n",
      "Epoch:  91\n",
      "252/252 - 4s - loss: 0.2384 - accuracy: 0.9051 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2782 - accuracy: 0.8850\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2810 - accuracy: 0.8842\n",
      "\n",
      "Epoch:  92\n",
      "252/252 - 4s - loss: 0.2398 - accuracy: 0.9041 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2284 - accuracy: 0.9103\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2305 - accuracy: 0.9091\n",
      "\n",
      "Epoch:  93\n",
      "252/252 - 4s - loss: 0.2397 - accuracy: 0.9040 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2193 - accuracy: 0.9138\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2242 - accuracy: 0.9100\n",
      "\n",
      "Epoch:  94\n",
      "252/252 - 4s - loss: 0.2404 - accuracy: 0.9037 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2184 - accuracy: 0.9135\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2213 - accuracy: 0.9139\n",
      "\n",
      "Epoch:  95\n",
      "252/252 - 4s - loss: 0.2328 - accuracy: 0.9063 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2071 - accuracy: 0.9189\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2103 - accuracy: 0.9170\n",
      "\n",
      "Epoch:  96\n",
      "252/252 - 4s - loss: 0.2396 - accuracy: 0.9034 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2369 - accuracy: 0.9054\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2419 - accuracy: 0.9039\n",
      "\n",
      "Epoch:  97\n",
      "252/252 - 4s - loss: 0.2380 - accuracy: 0.9049 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2113 - accuracy: 0.9175\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2133 - accuracy: 0.9178\n",
      "\n",
      "Epoch:  98\n",
      "252/252 - 4s - loss: 0.2268 - accuracy: 0.9108 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2055 - accuracy: 0.9195\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2077 - accuracy: 0.9183\n",
      "\n",
      "Epoch:  99\n",
      "252/252 - 4s - loss: 0.2336 - accuracy: 0.9059 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2119 - accuracy: 0.9167\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2157 - accuracy: 0.9149\n",
      "\n",
      "Epoch:  100\n",
      "252/252 - 4s - loss: 0.2294 - accuracy: 0.9082 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2143 - accuracy: 0.9141\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2177 - accuracy: 0.9118\n",
      "\n",
      "Epoch:  101\n",
      "252/252 - 4s - loss: 0.2247 - accuracy: 0.9115 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2147 - accuracy: 0.9153\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2179 - accuracy: 0.9122\n",
      "\n",
      "Epoch:  102\n",
      "252/252 - 4s - loss: 0.2249 - accuracy: 0.9099 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1939 - accuracy: 0.9264\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1988 - accuracy: 0.9242\n",
      "\n",
      "Epoch:  103\n",
      "252/252 - 4s - loss: 0.2283 - accuracy: 0.9092 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2154 - accuracy: 0.9158\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2192 - accuracy: 0.9133\n",
      "\n",
      "Epoch:  104\n",
      "252/252 - 4s - loss: 0.2242 - accuracy: 0.9104 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2494 - accuracy: 0.9005\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2522 - accuracy: 0.9017\n",
      "\n",
      "Epoch:  105\n",
      "252/252 - 4s - loss: 0.2202 - accuracy: 0.9121 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.3035 - accuracy: 0.8741\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.3099 - accuracy: 0.8708\n",
      "\n",
      "Epoch:  106\n",
      "252/252 - 4s - loss: 0.2214 - accuracy: 0.9117 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2473 - accuracy: 0.9027\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2482 - accuracy: 0.9023\n",
      "\n",
      "Epoch:  107\n",
      "252/252 - 4s - loss: 0.2208 - accuracy: 0.9129 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2092 - accuracy: 0.9187\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2128 - accuracy: 0.9180\n",
      "\n",
      "Epoch:  108\n",
      "252/252 - 4s - loss: 0.2208 - accuracy: 0.9120 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2018 - accuracy: 0.9207\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2033 - accuracy: 0.9209\n",
      "\n",
      "Epoch:  109\n",
      "252/252 - 4s - loss: 0.2147 - accuracy: 0.9149 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2236 - accuracy: 0.9113\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2239 - accuracy: 0.9142\n",
      "\n",
      "Epoch:  110\n",
      "252/252 - 4s - loss: 0.2163 - accuracy: 0.9138 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2303 - accuracy: 0.9058\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2349 - accuracy: 0.9046\n",
      "\n",
      "Epoch:  111\n",
      "252/252 - 4s - loss: 0.2146 - accuracy: 0.9152 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1888 - accuracy: 0.9260\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1920 - accuracy: 0.9261\n",
      "\n",
      "Epoch:  112\n",
      "252/252 - 4s - loss: 0.2177 - accuracy: 0.9126 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2102 - accuracy: 0.9147\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2152 - accuracy: 0.9130\n",
      "\n",
      "Epoch:  113\n",
      "252/252 - 4s - loss: 0.2195 - accuracy: 0.9124 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2522 - accuracy: 0.8995\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2556 - accuracy: 0.8994\n",
      "\n",
      "Epoch:  114\n",
      "252/252 - 4s - loss: 0.2139 - accuracy: 0.9147 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2148 - accuracy: 0.9129\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2190 - accuracy: 0.9114\n",
      "\n",
      "Epoch:  115\n",
      "252/252 - 4s - loss: 0.2079 - accuracy: 0.9176 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1833 - accuracy: 0.9302\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1890 - accuracy: 0.9278\n",
      "\n",
      "Epoch:  116\n",
      "252/252 - 4s - loss: 0.2091 - accuracy: 0.9167 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1861 - accuracy: 0.9289\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.9271\n",
      "\n",
      "Epoch:  117\n",
      "252/252 - 4s - loss: 0.2043 - accuracy: 0.9188 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1970 - accuracy: 0.9224\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2005 - accuracy: 0.9221\n",
      "\n",
      "Epoch:  118\n",
      "252/252 - 4s - loss: 0.2072 - accuracy: 0.9185 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1918 - accuracy: 0.9242\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1969 - accuracy: 0.9245\n",
      "\n",
      "Epoch:  119\n",
      "252/252 - 4s - loss: 0.2045 - accuracy: 0.9190 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2259 - accuracy: 0.9114\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2277 - accuracy: 0.9143\n",
      "\n",
      "Epoch:  120\n",
      "252/252 - 4s - loss: 0.2085 - accuracy: 0.9164 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2065 - accuracy: 0.9185\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2131 - accuracy: 0.9157\n",
      "\n",
      "Epoch:  121\n",
      "252/252 - 5s - loss: 0.2048 - accuracy: 0.9195 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2152 - accuracy: 0.9118\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2199 - accuracy: 0.9105\n",
      "\n",
      "Epoch:  122\n",
      "252/252 - 4s - loss: 0.2061 - accuracy: 0.9187 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.1818 - accuracy: 0.9293\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1872 - accuracy: 0.9271\n",
      "\n",
      "Epoch:  123\n",
      "252/252 - 4s - loss: 0.1982 - accuracy: 0.9221 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.2144 - accuracy: 0.9142\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2228 - accuracy: 0.9116\n",
      "\n",
      "Epoch:  124\n",
      "252/252 - 4s - loss: 0.2062 - accuracy: 0.9180 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2091 - accuracy: 0.9186\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2156 - accuracy: 0.9189\n",
      "\n",
      "Epoch:  125\n",
      "252/252 - 4s - loss: 0.1934 - accuracy: 0.9246 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1817 - accuracy: 0.9285\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9258\n",
      "\n",
      "Epoch:  126\n",
      "252/252 - 4s - loss: 0.2005 - accuracy: 0.9216 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2195 - accuracy: 0.9123\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2215 - accuracy: 0.9133\n",
      "\n",
      "Epoch:  127\n",
      "252/252 - 4s - loss: 0.1987 - accuracy: 0.9217 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2383 - accuracy: 0.9040\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2399 - accuracy: 0.9048\n",
      "\n",
      "Epoch:  128\n",
      "252/252 - 4s - loss: 0.1988 - accuracy: 0.9211 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2287 - accuracy: 0.9096\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2323 - accuracy: 0.9087\n",
      "\n",
      "Epoch:  129\n",
      "252/252 - 4s - loss: 0.1940 - accuracy: 0.9240 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1805 - accuracy: 0.9296\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1847 - accuracy: 0.9283\n",
      "\n",
      "Epoch:  130\n",
      "252/252 - 4s - loss: 0.1916 - accuracy: 0.9255 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1990 - accuracy: 0.9218\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2029 - accuracy: 0.9191\n",
      "\n",
      "Epoch:  131\n",
      "252/252 - 4s - loss: 0.1892 - accuracy: 0.9263 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1975 - accuracy: 0.9202\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2024 - accuracy: 0.9205\n",
      "\n",
      "Epoch:  132\n",
      "252/252 - 4s - loss: 0.1975 - accuracy: 0.9216 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2171 - accuracy: 0.9103\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2179 - accuracy: 0.9101\n",
      "\n",
      "Epoch:  133\n",
      "252/252 - 4s - loss: 0.1885 - accuracy: 0.9267 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1836 - accuracy: 0.9290\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1881 - accuracy: 0.9263\n",
      "\n",
      "Epoch:  134\n",
      "252/252 - 4s - loss: 0.2000 - accuracy: 0.9209 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1698 - accuracy: 0.9348\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1728 - accuracy: 0.9346\n",
      "\n",
      "Epoch:  135\n",
      "252/252 - 5s - loss: 0.1940 - accuracy: 0.9231 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2389 - accuracy: 0.8988\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2407 - accuracy: 0.8973\n",
      "\n",
      "Epoch:  136\n",
      "252/252 - 4s - loss: 0.2022 - accuracy: 0.9192 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2034 - accuracy: 0.9186\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2105 - accuracy: 0.9154\n",
      "\n",
      "Epoch:  137\n",
      "252/252 - 4s - loss: 0.1940 - accuracy: 0.9239 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1777 - accuracy: 0.9324\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1826 - accuracy: 0.9301\n",
      "\n",
      "Epoch:  138\n",
      "252/252 - 4s - loss: 0.1875 - accuracy: 0.9260 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1757 - accuracy: 0.9334\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1796 - accuracy: 0.9309\n",
      "\n",
      "Epoch:  139\n",
      "252/252 - 4s - loss: 0.1877 - accuracy: 0.9266 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1957 - accuracy: 0.9217\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2011 - accuracy: 0.9200\n",
      "\n",
      "Epoch:  140\n",
      "252/252 - 4s - loss: 0.1856 - accuracy: 0.9276 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1850 - accuracy: 0.9264\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1903 - accuracy: 0.9257\n",
      "\n",
      "Epoch:  141\n",
      "252/252 - 4s - loss: 0.1916 - accuracy: 0.9248 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1626 - accuracy: 0.9383\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1668 - accuracy: 0.9358\n",
      "\n",
      "Epoch:  142\n",
      "252/252 - 4s - loss: 0.1793 - accuracy: 0.9299 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1702 - accuracy: 0.9353\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1731 - accuracy: 0.9325\n",
      "\n",
      "Epoch:  143\n",
      "252/252 - 4s - loss: 0.1892 - accuracy: 0.9251 - 4s/epoch - 16ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1652 - accuracy: 0.9365\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1708 - accuracy: 0.9347\n",
      "\n",
      "Epoch:  144\n",
      "252/252 - 4s - loss: 0.1816 - accuracy: 0.9292 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1650 - accuracy: 0.9360\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1712 - accuracy: 0.9350\n",
      "\n",
      "Epoch:  145\n",
      "252/252 - 4s - loss: 0.1804 - accuracy: 0.9306 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1782 - accuracy: 0.9317\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9297\n",
      "\n",
      "Epoch:  146\n",
      "252/252 - 4s - loss: 0.1782 - accuracy: 0.9309 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1756 - accuracy: 0.9322\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9291\n",
      "\n",
      "Epoch:  147\n",
      "252/252 - 4s - loss: 0.1782 - accuracy: 0.9303 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.2082 - accuracy: 0.9169\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2114 - accuracy: 0.9150\n",
      "\n",
      "Epoch:  148\n",
      "252/252 - 4s - loss: 0.1800 - accuracy: 0.9294 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1769 - accuracy: 0.9327\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9299\n",
      "\n",
      "Epoch:  149\n",
      "252/252 - 4s - loss: 0.1756 - accuracy: 0.9310 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2044 - accuracy: 0.9165\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.2091 - accuracy: 0.9137\n",
      "\n",
      "Epoch:  150\n",
      "252/252 - 4s - loss: 0.1756 - accuracy: 0.9312 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1663 - accuracy: 0.9367\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1714 - accuracy: 0.9333\n",
      "\n",
      "Epoch:  151\n",
      "252/252 - 4s - loss: 0.1754 - accuracy: 0.9316 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1626 - accuracy: 0.9369\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9330\n",
      "\n",
      "Epoch:  152\n",
      "252/252 - 4s - loss: 0.1770 - accuracy: 0.9318 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1544 - accuracy: 0.9420\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1609 - accuracy: 0.9402\n",
      "\n",
      "Epoch:  153\n",
      "252/252 - 4s - loss: 0.1761 - accuracy: 0.9320 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1557 - accuracy: 0.9405\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1615 - accuracy: 0.9384\n",
      "\n",
      "Epoch:  154\n",
      "252/252 - 4s - loss: 0.1747 - accuracy: 0.9326 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2051 - accuracy: 0.9159\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2107 - accuracy: 0.9145\n",
      "\n",
      "Epoch:  155\n",
      "252/252 - 4s - loss: 0.1764 - accuracy: 0.9323 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2210 - accuracy: 0.9126\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2244 - accuracy: 0.9107\n",
      "\n",
      "Epoch:  156\n",
      "252/252 - 4s - loss: 0.1740 - accuracy: 0.9313 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1740 - accuracy: 0.9332\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1799 - accuracy: 0.9315\n",
      "\n",
      "Epoch:  157\n",
      "252/252 - 4s - loss: 0.1767 - accuracy: 0.9301 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1751 - accuracy: 0.9311\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1826 - accuracy: 0.9298\n",
      "\n",
      "Epoch:  158\n",
      "252/252 - 4s - loss: 0.1713 - accuracy: 0.9340 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1626 - accuracy: 0.9387\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1667 - accuracy: 0.9379\n",
      "\n",
      "Epoch:  159\n",
      "252/252 - 4s - loss: 0.1678 - accuracy: 0.9346 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1635 - accuracy: 0.9382\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1685 - accuracy: 0.9353\n",
      "\n",
      "Epoch:  160\n",
      "252/252 - 4s - loss: 0.1759 - accuracy: 0.9314 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1703 - accuracy: 0.9321\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1763 - accuracy: 0.9295\n",
      "\n",
      "Epoch:  161\n",
      "252/252 - 4s - loss: 0.1726 - accuracy: 0.9319 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1526 - accuracy: 0.9421\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1577 - accuracy: 0.9399\n",
      "\n",
      "Epoch:  162\n",
      "252/252 - 4s - loss: 0.1668 - accuracy: 0.9354 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1686 - accuracy: 0.9353\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1766 - accuracy: 0.9311\n",
      "\n",
      "Epoch:  163\n",
      "252/252 - 4s - loss: 0.1666 - accuracy: 0.9352 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1656 - accuracy: 0.9363\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1728 - accuracy: 0.9320\n",
      "\n",
      "Epoch:  164\n",
      "252/252 - 4s - loss: 0.1667 - accuracy: 0.9357 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1804 - accuracy: 0.9264\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1867 - accuracy: 0.9252\n",
      "\n",
      "Epoch:  165\n",
      "252/252 - 4s - loss: 0.1650 - accuracy: 0.9348 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1652 - accuracy: 0.9369\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1681 - accuracy: 0.9355\n",
      "\n",
      "Epoch:  166\n",
      "252/252 - 4s - loss: 0.1664 - accuracy: 0.9349 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1609 - accuracy: 0.9399\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1667 - accuracy: 0.9360\n",
      "\n",
      "Epoch:  167\n",
      "252/252 - 4s - loss: 0.1638 - accuracy: 0.9374 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1461 - accuracy: 0.9447\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1534 - accuracy: 0.9415\n",
      "\n",
      "Epoch:  168\n",
      "252/252 - 4s - loss: 0.1671 - accuracy: 0.9348 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1412 - accuracy: 0.9480\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1469 - accuracy: 0.9462\n",
      "\n",
      "Epoch:  169\n",
      "252/252 - 4s - loss: 0.1669 - accuracy: 0.9346 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1810 - accuracy: 0.9266\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1910 - accuracy: 0.9239\n",
      "\n",
      "Epoch:  170\n",
      "252/252 - 4s - loss: 0.1658 - accuracy: 0.9362 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1690 - accuracy: 0.9352\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1746 - accuracy: 0.9326\n",
      "\n",
      "Epoch:  171\n",
      "252/252 - 4s - loss: 0.1664 - accuracy: 0.9348 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1840 - accuracy: 0.9253\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1893 - accuracy: 0.9260\n",
      "\n",
      "Epoch:  172\n",
      "252/252 - 4s - loss: 0.1582 - accuracy: 0.9385 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1422 - accuracy: 0.9481\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1476 - accuracy: 0.9438\n",
      "\n",
      "Epoch:  173\n",
      "252/252 - 4s - loss: 0.1590 - accuracy: 0.9392 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1672 - accuracy: 0.9321\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1729 - accuracy: 0.9316\n",
      "\n",
      "Epoch:  174\n",
      "252/252 - 4s - loss: 0.1663 - accuracy: 0.9350 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.2280 - accuracy: 0.9111\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.2369 - accuracy: 0.9090\n",
      "\n",
      "Epoch:  175\n",
      "252/252 - 4s - loss: 0.1648 - accuracy: 0.9353 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1549 - accuracy: 0.9379\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1619 - accuracy: 0.9368\n",
      "\n",
      "Epoch:  176\n",
      "252/252 - 4s - loss: 0.1574 - accuracy: 0.9397 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1442 - accuracy: 0.9465\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1518 - accuracy: 0.9412\n",
      "\n",
      "Epoch:  177\n",
      "252/252 - 4s - loss: 0.1550 - accuracy: 0.9405 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1542 - accuracy: 0.9412\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1600 - accuracy: 0.9374\n",
      "\n",
      "Epoch:  178\n",
      "252/252 - 4s - loss: 0.1619 - accuracy: 0.9371 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 2ms/step - loss: 0.1491 - accuracy: 0.9418\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1563 - accuracy: 0.9415\n",
      "\n",
      "Epoch:  179\n",
      "252/252 - 4s - loss: 0.1640 - accuracy: 0.9361 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1611 - accuracy: 0.9382\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1701 - accuracy: 0.9326\n",
      "\n",
      "Epoch:  180\n",
      "252/252 - 4s - loss: 0.1556 - accuracy: 0.9403 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1463 - accuracy: 0.9422\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1520 - accuracy: 0.9423\n",
      "\n",
      "Epoch:  181\n",
      "252/252 - 4s - loss: 0.1564 - accuracy: 0.9391 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1583 - accuracy: 0.9371\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.9353\n",
      "\n",
      "Epoch:  182\n",
      "252/252 - 4s - loss: 0.1592 - accuracy: 0.9379 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1607 - accuracy: 0.9386\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1669 - accuracy: 0.9355\n",
      "\n",
      "Epoch:  183\n",
      "252/252 - 4s - loss: 0.1579 - accuracy: 0.9385 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1428 - accuracy: 0.9465\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1504 - accuracy: 0.9418\n",
      "\n",
      "Epoch:  184\n",
      "252/252 - 4s - loss: 0.1631 - accuracy: 0.9369 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1710 - accuracy: 0.9333\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1793 - accuracy: 0.9315\n",
      "\n",
      "Epoch:  185\n",
      "252/252 - 4s - loss: 0.1496 - accuracy: 0.9423 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1352 - accuracy: 0.9484\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1436 - accuracy: 0.9461\n",
      "\n",
      "Epoch:  186\n",
      "252/252 - 4s - loss: 0.1553 - accuracy: 0.9402 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1516 - accuracy: 0.9395\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1586 - accuracy: 0.9372\n",
      "\n",
      "Epoch:  187\n",
      "252/252 - 4s - loss: 0.1588 - accuracy: 0.9383 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1343 - accuracy: 0.9499\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1410 - accuracy: 0.9453\n",
      "\n",
      "Epoch:  188\n",
      "252/252 - 4s - loss: 0.1525 - accuracy: 0.9417 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1636 - accuracy: 0.9347\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1713 - accuracy: 0.9323\n",
      "\n",
      "Epoch:  189\n",
      "252/252 - 4s - loss: 0.1558 - accuracy: 0.9393 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1581 - accuracy: 0.9373\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1639 - accuracy: 0.9366\n",
      "\n",
      "Epoch:  190\n",
      "252/252 - 4s - loss: 0.1488 - accuracy: 0.9427 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1648 - accuracy: 0.9335\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1697 - accuracy: 0.9311\n",
      "\n",
      "Epoch:  191\n",
      "252/252 - 4s - loss: 0.1570 - accuracy: 0.9388 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1415 - accuracy: 0.9454\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1464 - accuracy: 0.9430\n",
      "\n",
      "Epoch:  192\n",
      "252/252 - 4s - loss: 0.1536 - accuracy: 0.9410 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1482 - accuracy: 0.9419\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1571 - accuracy: 0.9423\n",
      "\n",
      "Epoch:  193\n",
      "252/252 - 4s - loss: 0.1509 - accuracy: 0.9426 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1750 - accuracy: 0.9308\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1813 - accuracy: 0.9301\n",
      "\n",
      "Epoch:  194\n",
      "252/252 - 4s - loss: 0.1522 - accuracy: 0.9411 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1352 - accuracy: 0.9487\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1441 - accuracy: 0.9466\n",
      "\n",
      "Epoch:  195\n",
      "252/252 - 4s - loss: 0.1527 - accuracy: 0.9411 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1390 - accuracy: 0.9473\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1479 - accuracy: 0.9438\n",
      "\n",
      "Epoch:  196\n",
      "252/252 - 4s - loss: 0.1649 - accuracy: 0.9355 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1542 - accuracy: 0.9411\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1606 - accuracy: 0.9379\n",
      "\n",
      "Epoch:  197\n",
      "252/252 - 4s - loss: 0.1469 - accuracy: 0.9431 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1296 - accuracy: 0.9517\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1376 - accuracy: 0.9479\n",
      "\n",
      "Epoch:  198\n",
      "252/252 - 4s - loss: 0.1408 - accuracy: 0.9463 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1429 - accuracy: 0.9436\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1516 - accuracy: 0.9409\n",
      "\n",
      "Epoch:  199\n",
      "252/252 - 4s - loss: 0.1465 - accuracy: 0.9437 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1397 - accuracy: 0.9462\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1491 - accuracy: 0.9403\n",
      "\n",
      "Epoch:  200\n",
      "252/252 - 4s - loss: 0.1480 - accuracy: 0.9426 - 4s/epoch - 16ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1549 - accuracy: 0.9383\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1611 - accuracy: 0.9358\n",
      "\n",
      "Epoch:  201\n",
      "252/252 - 4s - loss: 0.1526 - accuracy: 0.9407 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1744 - accuracy: 0.9284\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1788 - accuracy: 0.9291\n",
      "\n",
      "Epoch:  202\n",
      "252/252 - 4s - loss: 0.1502 - accuracy: 0.9430 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1494 - accuracy: 0.9414\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1571 - accuracy: 0.9397\n",
      "\n",
      "Epoch:  203\n",
      "252/252 - 4s - loss: 0.1411 - accuracy: 0.9454 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1303 - accuracy: 0.9496\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1368 - accuracy: 0.9482\n",
      "\n",
      "Epoch:  204\n",
      "252/252 - 5s - loss: 0.1469 - accuracy: 0.9425 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1301 - accuracy: 0.9514\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1369 - accuracy: 0.9495\n",
      "\n",
      "Epoch:  205\n",
      "252/252 - 4s - loss: 0.1464 - accuracy: 0.9437 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.1308 - accuracy: 0.9512\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1398 - accuracy: 0.9472\n",
      "\n",
      "Epoch:  206\n",
      "252/252 - 4s - loss: 0.1478 - accuracy: 0.9436 - 4s/epoch - 16ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.1313 - accuracy: 0.9511\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1374 - accuracy: 0.9486\n",
      "\n",
      "Epoch:  207\n",
      "252/252 - 4s - loss: 0.1444 - accuracy: 0.9450 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1501 - accuracy: 0.9413\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1584 - accuracy: 0.9377\n",
      "\n",
      "Epoch:  208\n",
      "252/252 - 4s - loss: 0.1406 - accuracy: 0.9460 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1430 - accuracy: 0.9436\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1506 - accuracy: 0.9407\n",
      "\n",
      "Epoch:  209\n",
      "252/252 - 4s - loss: 0.1433 - accuracy: 0.9453 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1408 - accuracy: 0.9468\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1470 - accuracy: 0.9454\n",
      "\n",
      "Epoch:  210\n",
      "252/252 - 4s - loss: 0.1490 - accuracy: 0.9426 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1406 - accuracy: 0.9458\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1484 - accuracy: 0.9454\n",
      "\n",
      "Epoch:  211\n",
      "252/252 - 4s - loss: 0.1399 - accuracy: 0.9469 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1274 - accuracy: 0.9521\n",
      "for testing\n",
      "504/504 [==============================] - 1s 2ms/step - loss: 0.1333 - accuracy: 0.9525\n",
      "\n",
      "Epoch:  212\n",
      "252/252 - 4s - loss: 0.1414 - accuracy: 0.9460 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1457 - accuracy: 0.9428\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1550 - accuracy: 0.9397\n",
      "\n",
      "Epoch:  213\n",
      "252/252 - 4s - loss: 0.1362 - accuracy: 0.9482 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1501 - accuracy: 0.9402\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1596 - accuracy: 0.9384\n",
      "\n",
      "Epoch:  214\n",
      "252/252 - 4s - loss: 0.1368 - accuracy: 0.9476 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1371 - accuracy: 0.9477\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1450 - accuracy: 0.9461\n",
      "\n",
      "Epoch:  215\n",
      "252/252 - 4s - loss: 0.1481 - accuracy: 0.9427 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1244 - accuracy: 0.9541\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1320 - accuracy: 0.9508\n",
      "\n",
      "Epoch:  216\n",
      "252/252 - 4s - loss: 0.1372 - accuracy: 0.9467 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1435 - accuracy: 0.9456\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1531 - accuracy: 0.9430\n",
      "\n",
      "Epoch:  217\n",
      "252/252 - 4s - loss: 0.1375 - accuracy: 0.9480 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1802 - accuracy: 0.9274\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1868 - accuracy: 0.9273\n",
      "\n",
      "Epoch:  218\n",
      "252/252 - 4s - loss: 0.1435 - accuracy: 0.9444 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1287 - accuracy: 0.9515\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1366 - accuracy: 0.9480\n",
      "\n",
      "Epoch:  219\n",
      "252/252 - 4s - loss: 0.1391 - accuracy: 0.9468 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1221 - accuracy: 0.9535\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1307 - accuracy: 0.9508\n",
      "\n",
      "Epoch:  220\n",
      "252/252 - 4s - loss: 0.1375 - accuracy: 0.9478 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1234 - accuracy: 0.9535\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1337 - accuracy: 0.9481\n",
      "\n",
      "Epoch:  221\n",
      "252/252 - 4s - loss: 0.1370 - accuracy: 0.9474 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1311 - accuracy: 0.9509\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1404 - accuracy: 0.9465\n",
      "\n",
      "Epoch:  222\n",
      "252/252 - 4s - loss: 0.1384 - accuracy: 0.9473 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1189 - accuracy: 0.9563\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1260 - accuracy: 0.9537\n",
      "\n",
      "Epoch:  223\n",
      "252/252 - 4s - loss: 0.1352 - accuracy: 0.9483 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1343 - accuracy: 0.9500\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1416 - accuracy: 0.9474\n",
      "\n",
      "Epoch:  224\n",
      "252/252 - 4s - loss: 0.1357 - accuracy: 0.9474 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1154 - accuracy: 0.9577\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1239 - accuracy: 0.9546\n",
      "\n",
      "Epoch:  225\n",
      "252/252 - 4s - loss: 0.1350 - accuracy: 0.9481 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1254 - accuracy: 0.9525\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1335 - accuracy: 0.9500\n",
      "\n",
      "Epoch:  226\n",
      "252/252 - 4s - loss: 0.1321 - accuracy: 0.9499 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1260 - accuracy: 0.9524\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1344 - accuracy: 0.9476\n",
      "\n",
      "Epoch:  227\n",
      "252/252 - 4s - loss: 0.1348 - accuracy: 0.9484 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1418 - accuracy: 0.9458\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1523 - accuracy: 0.9404\n",
      "\n",
      "Epoch:  228\n",
      "252/252 - 4s - loss: 0.1331 - accuracy: 0.9491 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1303 - accuracy: 0.9495\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1383 - accuracy: 0.9469\n",
      "\n",
      "Epoch:  229\n",
      "252/252 - 4s - loss: 0.1330 - accuracy: 0.9489 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1324 - accuracy: 0.9497\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1388 - accuracy: 0.9469\n",
      "\n",
      "Epoch:  230\n",
      "252/252 - 4s - loss: 0.1376 - accuracy: 0.9467 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1230 - accuracy: 0.9538\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1307 - accuracy: 0.9497\n",
      "\n",
      "Epoch:  231\n",
      "252/252 - 4s - loss: 0.1275 - accuracy: 0.9523 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1590 - accuracy: 0.9345\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1623 - accuracy: 0.9369\n",
      "\n",
      "Epoch:  232\n",
      "252/252 - 4s - loss: 0.1323 - accuracy: 0.9495 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1323 - accuracy: 0.9481\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1381 - accuracy: 0.9468\n",
      "\n",
      "Epoch:  233\n",
      "252/252 - 4s - loss: 0.1325 - accuracy: 0.9491 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1253 - accuracy: 0.9537\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1318 - accuracy: 0.9497\n",
      "\n",
      "Epoch:  234\n",
      "252/252 - 4s - loss: 0.1315 - accuracy: 0.9495 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1346 - accuracy: 0.9483\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1472 - accuracy: 0.9426\n",
      "\n",
      "Epoch:  235\n",
      "252/252 - 4s - loss: 0.1393 - accuracy: 0.9468 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1510 - accuracy: 0.9387\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1591 - accuracy: 0.9359\n",
      "\n",
      "Epoch:  236\n",
      "252/252 - 4s - loss: 0.1285 - accuracy: 0.9506 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1389 - accuracy: 0.9470\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1477 - accuracy: 0.9443\n",
      "\n",
      "Epoch:  237\n",
      "252/252 - 4s - loss: 0.1314 - accuracy: 0.9502 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1114 - accuracy: 0.9595\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1199 - accuracy: 0.9560\n",
      "\n",
      "Epoch:  238\n",
      "252/252 - 4s - loss: 0.1301 - accuracy: 0.9506 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1278 - accuracy: 0.9521\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1338 - accuracy: 0.9519\n",
      "\n",
      "Epoch:  239\n",
      "252/252 - 4s - loss: 0.1318 - accuracy: 0.9497 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1349 - accuracy: 0.9494\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1429 - accuracy: 0.9457\n",
      "\n",
      "Epoch:  240\n",
      "252/252 - 4s - loss: 0.1294 - accuracy: 0.9507 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1249 - accuracy: 0.9532\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1324 - accuracy: 0.9507\n",
      "\n",
      "Epoch:  241\n",
      "252/252 - 4s - loss: 0.1271 - accuracy: 0.9522 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1355 - accuracy: 0.9484\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1453 - accuracy: 0.9454\n",
      "\n",
      "Epoch:  242\n",
      "252/252 - 4s - loss: 0.1288 - accuracy: 0.9505 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1504 - accuracy: 0.9403\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1566 - accuracy: 0.9390\n",
      "\n",
      "Epoch:  243\n",
      "252/252 - 4s - loss: 0.1291 - accuracy: 0.9510 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1182 - accuracy: 0.9553\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1263 - accuracy: 0.9519\n",
      "\n",
      "Epoch:  244\n",
      "252/252 - 4s - loss: 0.1270 - accuracy: 0.9518 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1179 - accuracy: 0.9547\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1256 - accuracy: 0.9532\n",
      "\n",
      "Epoch:  245\n",
      "252/252 - 4s - loss: 0.1270 - accuracy: 0.9508 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1502 - accuracy: 0.9423\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1576 - accuracy: 0.9405\n",
      "\n",
      "Epoch:  246\n",
      "252/252 - 4s - loss: 0.1300 - accuracy: 0.9503 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1259 - accuracy: 0.9523\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1336 - accuracy: 0.9505\n",
      "\n",
      "Epoch:  247\n",
      "252/252 - 4s - loss: 0.1252 - accuracy: 0.9520 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1280 - accuracy: 0.9500\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1354 - accuracy: 0.9475\n",
      "\n",
      "Epoch:  248\n",
      "252/252 - 4s - loss: 0.1317 - accuracy: 0.9498 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1316 - accuracy: 0.9497\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1405 - accuracy: 0.9466\n",
      "\n",
      "Epoch:  249\n",
      "252/252 - 4s - loss: 0.1253 - accuracy: 0.9530 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1170 - accuracy: 0.9559\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1293 - accuracy: 0.9520\n",
      "\n",
      "Epoch:  250\n",
      "252/252 - 4s - loss: 0.1254 - accuracy: 0.9524 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1227 - accuracy: 0.9523\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1305 - accuracy: 0.9515\n",
      "\n",
      "Epoch:  251\n",
      "252/252 - 4s - loss: 0.1248 - accuracy: 0.9520 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1088 - accuracy: 0.9599\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1186 - accuracy: 0.9555\n",
      "\n",
      "Epoch:  252\n",
      "252/252 - 4s - loss: 0.1242 - accuracy: 0.9521 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1228 - accuracy: 0.9530\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1343 - accuracy: 0.9484\n",
      "\n",
      "Epoch:  253\n",
      "252/252 - 4s - loss: 0.1252 - accuracy: 0.9525 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1459 - accuracy: 0.9438\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1528 - accuracy: 0.9428\n",
      "\n",
      "Epoch:  254\n",
      "252/252 - 4s - loss: 0.1262 - accuracy: 0.9517 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1233 - accuracy: 0.9539\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1299 - accuracy: 0.9498\n",
      "\n",
      "Epoch:  255\n",
      "252/252 - 4s - loss: 0.1237 - accuracy: 0.9531 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1138 - accuracy: 0.9576\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1213 - accuracy: 0.9532\n",
      "\n",
      "Epoch:  256\n",
      "252/252 - 4s - loss: 0.1240 - accuracy: 0.9530 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1348 - accuracy: 0.9484\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1414 - accuracy: 0.9457\n",
      "\n",
      "Epoch:  257\n",
      "252/252 - 4s - loss: 0.1245 - accuracy: 0.9524 - 4s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1373 - accuracy: 0.9460\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1479 - accuracy: 0.9433\n",
      "\n",
      "Epoch:  258\n",
      "252/252 - 4s - loss: 0.1191 - accuracy: 0.9544 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1169 - accuracy: 0.9552\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1255 - accuracy: 0.9523\n",
      "\n",
      "Epoch:  259\n",
      "252/252 - 4s - loss: 0.1272 - accuracy: 0.9511 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1136 - accuracy: 0.9574\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1250 - accuracy: 0.9535\n",
      "\n",
      "Epoch:  260\n",
      "252/252 - 4s - loss: 0.1282 - accuracy: 0.9507 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1300 - accuracy: 0.9496\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1377 - accuracy: 0.9473\n",
      "\n",
      "Epoch:  261\n",
      "252/252 - 4s - loss: 0.1152 - accuracy: 0.9567 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1120 - accuracy: 0.9584\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1207 - accuracy: 0.9549\n",
      "\n",
      "Epoch:  262\n",
      "252/252 - 4s - loss: 0.1159 - accuracy: 0.9563 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1128 - accuracy: 0.9580\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1210 - accuracy: 0.9549\n",
      "\n",
      "Epoch:  263\n",
      "252/252 - 4s - loss: 0.1176 - accuracy: 0.9547 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1030 - accuracy: 0.9624\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1114 - accuracy: 0.9591\n",
      "\n",
      "Epoch:  264\n",
      "252/252 - 4s - loss: 0.1188 - accuracy: 0.9559 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1117 - accuracy: 0.9590\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1220 - accuracy: 0.9545\n",
      "\n",
      "Epoch:  265\n",
      "252/252 - 4s - loss: 0.1253 - accuracy: 0.9516 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1196 - accuracy: 0.9544\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1267 - accuracy: 0.9531\n",
      "\n",
      "Epoch:  266\n",
      "252/252 - 4s - loss: 0.1209 - accuracy: 0.9537 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1173 - accuracy: 0.9557\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1265 - accuracy: 0.9534\n",
      "\n",
      "Epoch:  267\n",
      "252/252 - 5s - loss: 0.1148 - accuracy: 0.9571 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1323 - accuracy: 0.9506\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1411 - accuracy: 0.9495\n",
      "\n",
      "Epoch:  268\n",
      "252/252 - 4s - loss: 0.1196 - accuracy: 0.9547 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1120 - accuracy: 0.9568\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1223 - accuracy: 0.9526\n",
      "\n",
      "Epoch:  269\n",
      "252/252 - 4s - loss: 0.1224 - accuracy: 0.9538 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1064 - accuracy: 0.9607\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1164 - accuracy: 0.9573\n",
      "\n",
      "Epoch:  270\n",
      "252/252 - 4s - loss: 0.1169 - accuracy: 0.9546 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1040 - accuracy: 0.9617\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1126 - accuracy: 0.9591\n",
      "\n",
      "Epoch:  271\n",
      "252/252 - 4s - loss: 0.1165 - accuracy: 0.9557 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1097 - accuracy: 0.9582\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1194 - accuracy: 0.9537\n",
      "\n",
      "Epoch:  272\n",
      "252/252 - 4s - loss: 0.1194 - accuracy: 0.9538 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1091 - accuracy: 0.9594\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1177 - accuracy: 0.9570\n",
      "\n",
      "Epoch:  273\n",
      "252/252 - 4s - loss: 0.1137 - accuracy: 0.9572 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1176 - accuracy: 0.9559\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1285 - accuracy: 0.9531\n",
      "\n",
      "Epoch:  274\n",
      "252/252 - 4s - loss: 0.1126 - accuracy: 0.9575 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1116 - accuracy: 0.9565\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1194 - accuracy: 0.9526\n",
      "\n",
      "Epoch:  275\n",
      "252/252 - 4s - loss: 0.1191 - accuracy: 0.9546 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1241 - accuracy: 0.9526\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1371 - accuracy: 0.9488\n",
      "\n",
      "Epoch:  276\n",
      "252/252 - 4s - loss: 0.1186 - accuracy: 0.9552 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1042 - accuracy: 0.9610\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1121 - accuracy: 0.9586\n",
      "\n",
      "Epoch:  277\n",
      "252/252 - 4s - loss: 0.1157 - accuracy: 0.9562 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1107 - accuracy: 0.9583\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1219 - accuracy: 0.9546\n",
      "\n",
      "Epoch:  278\n",
      "252/252 - 4s - loss: 0.1174 - accuracy: 0.9553 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1245 - accuracy: 0.9509\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1336 - accuracy: 0.9479\n",
      "\n",
      "Epoch:  279\n",
      "252/252 - 4s - loss: 0.1125 - accuracy: 0.9581 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1024 - accuracy: 0.9628\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1109 - accuracy: 0.9603\n",
      "\n",
      "Epoch:  280\n",
      "252/252 - 4s - loss: 0.1184 - accuracy: 0.9551 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1326 - accuracy: 0.9505\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1432 - accuracy: 0.9466\n",
      "\n",
      "Epoch:  281\n",
      "252/252 - 4s - loss: 0.1205 - accuracy: 0.9531 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1369 - accuracy: 0.9458\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1479 - accuracy: 0.9423\n",
      "\n",
      "Epoch:  282\n",
      "252/252 - 4s - loss: 0.1165 - accuracy: 0.9557 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1392 - accuracy: 0.9464\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1469 - accuracy: 0.9444\n",
      "\n",
      "Epoch:  283\n",
      "252/252 - 5s - loss: 0.1120 - accuracy: 0.9569 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1141 - accuracy: 0.9575\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1246 - accuracy: 0.9521\n",
      "\n",
      "Epoch:  284\n",
      "252/252 - 5s - loss: 0.1144 - accuracy: 0.9569 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1130 - accuracy: 0.9573\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1193 - accuracy: 0.9563\n",
      "\n",
      "Epoch:  285\n",
      "252/252 - 5s - loss: 0.1137 - accuracy: 0.9565 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1104 - accuracy: 0.9580\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1170 - accuracy: 0.9559\n",
      "\n",
      "Epoch:  286\n",
      "252/252 - 7s - loss: 0.1120 - accuracy: 0.9578 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1188 - accuracy: 0.9546\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1261 - accuracy: 0.9534\n",
      "\n",
      "Epoch:  287\n",
      "252/252 - 4s - loss: 0.1088 - accuracy: 0.9588 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1156 - accuracy: 0.9566\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1290 - accuracy: 0.9512\n",
      "\n",
      "Epoch:  288\n",
      "252/252 - 4s - loss: 0.1099 - accuracy: 0.9584 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1134 - accuracy: 0.9564\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1214 - accuracy: 0.9565\n",
      "\n",
      "Epoch:  289\n",
      "252/252 - 4s - loss: 0.1100 - accuracy: 0.9583 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1028 - accuracy: 0.9622\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1148 - accuracy: 0.9593\n",
      "\n",
      "Epoch:  290\n",
      "252/252 - 4s - loss: 0.1125 - accuracy: 0.9575 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1197 - accuracy: 0.9552\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1270 - accuracy: 0.9523\n",
      "\n",
      "Epoch:  291\n",
      "252/252 - 4s - loss: 0.1108 - accuracy: 0.9581 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1189 - accuracy: 0.9538\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1287 - accuracy: 0.9506\n",
      "\n",
      "Epoch:  292\n",
      "252/252 - 4s - loss: 0.1094 - accuracy: 0.9590 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1152 - accuracy: 0.9559\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1239 - accuracy: 0.9522\n",
      "\n",
      "Epoch:  293\n",
      "252/252 - 4s - loss: 0.1108 - accuracy: 0.9585 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1104 - accuracy: 0.9580\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1224 - accuracy: 0.9550\n",
      "\n",
      "Epoch:  294\n",
      "252/252 - 4s - loss: 0.1107 - accuracy: 0.9579 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.0911 - accuracy: 0.9667\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1005 - accuracy: 0.9642\n",
      "\n",
      "Epoch:  295\n",
      "252/252 - 5s - loss: 0.1112 - accuracy: 0.9581 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1482 - accuracy: 0.9429\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1590 - accuracy: 0.9400\n",
      "\n",
      "Epoch:  296\n",
      "252/252 - 4s - loss: 0.1110 - accuracy: 0.9581 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1144 - accuracy: 0.9558\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1212 - accuracy: 0.9532\n",
      "\n",
      "Epoch:  297\n",
      "252/252 - 5s - loss: 0.1130 - accuracy: 0.9572 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1102 - accuracy: 0.9594\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1203 - accuracy: 0.9565\n",
      "\n",
      "Epoch:  298\n",
      "252/252 - 4s - loss: 0.1146 - accuracy: 0.9567 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1000 - accuracy: 0.9629\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1099 - accuracy: 0.9584\n",
      "\n",
      "Epoch:  299\n",
      "252/252 - 4s - loss: 0.1076 - accuracy: 0.9596 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1043 - accuracy: 0.9613\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1167 - accuracy: 0.9564\n",
      "\n",
      "Epoch:  300\n",
      "252/252 - 4s - loss: 0.1119 - accuracy: 0.9581 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1125 - accuracy: 0.9563\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1237 - accuracy: 0.9528\n",
      "\n",
      "Epoch:  301\n",
      "252/252 - 4s - loss: 0.1061 - accuracy: 0.9608 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1233 - accuracy: 0.9519\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1354 - accuracy: 0.9466\n",
      "\n",
      "Epoch:  302\n",
      "252/252 - 4s - loss: 0.1113 - accuracy: 0.9581 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1002 - accuracy: 0.9628\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1133 - accuracy: 0.9580\n",
      "\n",
      "Epoch:  303\n",
      "252/252 - 4s - loss: 0.1103 - accuracy: 0.9582 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1068 - accuracy: 0.9603\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1189 - accuracy: 0.9565\n",
      "\n",
      "Epoch:  304\n",
      "252/252 - 4s - loss: 0.1100 - accuracy: 0.9585 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1297 - accuracy: 0.9504\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1430 - accuracy: 0.9449\n",
      "\n",
      "Epoch:  305\n",
      "252/252 - 4s - loss: 0.1080 - accuracy: 0.9590 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.0936 - accuracy: 0.9659\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1033 - accuracy: 0.9624\n",
      "\n",
      "Epoch:  306\n",
      "252/252 - 4s - loss: 0.1064 - accuracy: 0.9607 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.0932 - accuracy: 0.9661\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1041 - accuracy: 0.9627\n",
      "\n",
      "Epoch:  307\n",
      "252/252 - 4s - loss: 0.1051 - accuracy: 0.9608 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.0966 - accuracy: 0.9653\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1088 - accuracy: 0.9598\n",
      "\n",
      "Epoch:  308\n",
      "252/252 - 4s - loss: 0.1088 - accuracy: 0.9596 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1213 - accuracy: 0.9529\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1309 - accuracy: 0.9513\n",
      "\n",
      "Epoch:  309\n",
      "252/252 - 4s - loss: 0.1111 - accuracy: 0.9574 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1143 - accuracy: 0.9570\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1267 - accuracy: 0.9534\n",
      "\n",
      "Epoch:  310\n",
      "252/252 - 5s - loss: 0.1034 - accuracy: 0.9612 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1023 - accuracy: 0.9608\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1158 - accuracy: 0.9579\n",
      "\n",
      "Epoch:  311\n",
      "252/252 - 4s - loss: 0.1058 - accuracy: 0.9605 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1016 - accuracy: 0.9616\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1108 - accuracy: 0.9596\n",
      "\n",
      "Epoch:  312\n",
      "252/252 - 4s - loss: 0.1092 - accuracy: 0.9581 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.0977 - accuracy: 0.9644\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1094 - accuracy: 0.9607\n",
      "\n",
      "Epoch:  313\n",
      "252/252 - 4s - loss: 0.1076 - accuracy: 0.9596 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0995 - accuracy: 0.9621\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1105 - accuracy: 0.9595\n",
      "\n",
      "Epoch:  314\n",
      "252/252 - 4s - loss: 0.1053 - accuracy: 0.9599 - 4s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1167 - accuracy: 0.9550\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1250 - accuracy: 0.9524\n",
      "\n",
      "Epoch:  315\n",
      "252/252 - 4s - loss: 0.1067 - accuracy: 0.9590 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1263 - accuracy: 0.9515\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1384 - accuracy: 0.9480\n",
      "\n",
      "Epoch:  316\n",
      "252/252 - 4s - loss: 0.1093 - accuracy: 0.9582 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0900 - accuracy: 0.9675\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1007 - accuracy: 0.9629\n",
      "\n",
      "Epoch:  317\n",
      "252/252 - 5s - loss: 0.0973 - accuracy: 0.9640 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1076 - accuracy: 0.9590\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1180 - accuracy: 0.9554\n",
      "\n",
      "Epoch:  318\n",
      "252/252 - 5s - loss: 0.1067 - accuracy: 0.9600 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1008 - accuracy: 0.9613\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.1133 - accuracy: 0.9582\n",
      "\n",
      "Epoch:  319\n",
      "252/252 - 5s - loss: 0.1045 - accuracy: 0.9601 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1037 - accuracy: 0.9610\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1165 - accuracy: 0.9567\n",
      "\n",
      "Epoch:  320\n",
      "252/252 - 4s - loss: 0.1042 - accuracy: 0.9612 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.1160 - accuracy: 0.9559\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1253 - accuracy: 0.9527\n",
      "\n",
      "Epoch:  321\n",
      "252/252 - 4s - loss: 0.1091 - accuracy: 0.9589 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1192 - accuracy: 0.9550\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1319 - accuracy: 0.9491\n",
      "\n",
      "Epoch:  322\n",
      "252/252 - 4s - loss: 0.0988 - accuracy: 0.9628 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1030 - accuracy: 0.9603\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1170 - accuracy: 0.9549\n",
      "\n",
      "Epoch:  323\n",
      "252/252 - 5s - loss: 0.1049 - accuracy: 0.9604 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0928 - accuracy: 0.9652\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1061 - accuracy: 0.9616\n",
      "\n",
      "Epoch:  324\n",
      "252/252 - 4s - loss: 0.1081 - accuracy: 0.9586 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.0980 - accuracy: 0.9640\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1109 - accuracy: 0.9591\n",
      "\n",
      "Epoch:  325\n",
      "252/252 - 5s - loss: 0.1018 - accuracy: 0.9620 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0961 - accuracy: 0.9637\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1057 - accuracy: 0.9606\n",
      "\n",
      "Epoch:  326\n",
      "252/252 - 4s - loss: 0.1027 - accuracy: 0.9608 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1064 - accuracy: 0.9601\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.1199 - accuracy: 0.9543\n",
      "\n",
      "Epoch:  327\n",
      "252/252 - 5s - loss: 0.1031 - accuracy: 0.9610 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1176 - accuracy: 0.9541\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1275 - accuracy: 0.9530\n",
      "\n",
      "Epoch:  328\n",
      "252/252 - 5s - loss: 0.0999 - accuracy: 0.9626 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0908 - accuracy: 0.9676\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1040 - accuracy: 0.9622\n",
      "\n",
      "Epoch:  329\n",
      "252/252 - 4s - loss: 0.0987 - accuracy: 0.9624 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0986 - accuracy: 0.9629\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1140 - accuracy: 0.9578\n",
      "\n",
      "Epoch:  330\n",
      "252/252 - 4s - loss: 0.1025 - accuracy: 0.9614 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1044 - accuracy: 0.9610\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1170 - accuracy: 0.9583\n",
      "\n",
      "Epoch:  331\n",
      "252/252 - 5s - loss: 0.1014 - accuracy: 0.9621 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0889 - accuracy: 0.9679\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1001 - accuracy: 0.9635\n",
      "\n",
      "Epoch:  332\n",
      "252/252 - 4s - loss: 0.0986 - accuracy: 0.9630 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0955 - accuracy: 0.9645\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1102 - accuracy: 0.9599\n",
      "\n",
      "Epoch:  333\n",
      "252/252 - 4s - loss: 0.1033 - accuracy: 0.9613 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0908 - accuracy: 0.9659\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1030 - accuracy: 0.9608\n",
      "\n",
      "Epoch:  334\n",
      "252/252 - 4s - loss: 0.0982 - accuracy: 0.9632 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1039 - accuracy: 0.9596\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1139 - accuracy: 0.9568\n",
      "\n",
      "Epoch:  335\n",
      "252/252 - 4s - loss: 0.0981 - accuracy: 0.9629 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1141 - accuracy: 0.9556\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1263 - accuracy: 0.9508\n",
      "\n",
      "Epoch:  336\n",
      "252/252 - 4s - loss: 0.1012 - accuracy: 0.9608 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0814 - accuracy: 0.9707\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0947 - accuracy: 0.9655\n",
      "\n",
      "Epoch:  337\n",
      "252/252 - 4s - loss: 0.1000 - accuracy: 0.9619 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1341 - accuracy: 0.9481\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1496 - accuracy: 0.9434\n",
      "\n",
      "Epoch:  338\n",
      "252/252 - 4s - loss: 0.1012 - accuracy: 0.9615 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0988 - accuracy: 0.9622\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1088 - accuracy: 0.9598\n",
      "\n",
      "Epoch:  339\n",
      "252/252 - 4s - loss: 0.1007 - accuracy: 0.9619 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1276 - accuracy: 0.9499\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1384 - accuracy: 0.9461\n",
      "\n",
      "Epoch:  340\n",
      "252/252 - 5s - loss: 0.0987 - accuracy: 0.9623 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0949 - accuracy: 0.9647\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1083 - accuracy: 0.9595\n",
      "\n",
      "Epoch:  341\n",
      "252/252 - 4s - loss: 0.0974 - accuracy: 0.9640 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0896 - accuracy: 0.9673\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1041 - accuracy: 0.9607\n",
      "\n",
      "Epoch:  342\n",
      "252/252 - 4s - loss: 0.1022 - accuracy: 0.9612 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0965 - accuracy: 0.9645\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1115 - accuracy: 0.9586\n",
      "\n",
      "Epoch:  343\n",
      "252/252 - 5s - loss: 0.0976 - accuracy: 0.9629 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0916 - accuracy: 0.9661\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1081 - accuracy: 0.9620\n",
      "\n",
      "Epoch:  344\n",
      "252/252 - 5s - loss: 0.0971 - accuracy: 0.9632 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0922 - accuracy: 0.9651\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1064 - accuracy: 0.9622\n",
      "\n",
      "Epoch:  345\n",
      "252/252 - 5s - loss: 0.0963 - accuracy: 0.9634 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1018 - accuracy: 0.9610\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1141 - accuracy: 0.9573\n",
      "\n",
      "Epoch:  346\n",
      "252/252 - 4s - loss: 0.0939 - accuracy: 0.9652 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0845 - accuracy: 0.9692\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0980 - accuracy: 0.9653\n",
      "\n",
      "Epoch:  347\n",
      "252/252 - 4s - loss: 0.0951 - accuracy: 0.9636 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0835 - accuracy: 0.9691\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0986 - accuracy: 0.9634\n",
      "\n",
      "Epoch:  348\n",
      "252/252 - 4s - loss: 0.0964 - accuracy: 0.9642 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0834 - accuracy: 0.9692\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0960 - accuracy: 0.9652\n",
      "\n",
      "Epoch:  349\n",
      "252/252 - 4s - loss: 0.0986 - accuracy: 0.9623 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1227 - accuracy: 0.9525\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1358 - accuracy: 0.9498\n",
      "\n",
      "Epoch:  350\n",
      "252/252 - 4s - loss: 0.0965 - accuracy: 0.9645 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0793 - accuracy: 0.9713\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0944 - accuracy: 0.9662\n",
      "\n",
      "Epoch:  351\n",
      "252/252 - 4s - loss: 0.0925 - accuracy: 0.9663 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0891 - accuracy: 0.9676\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1011 - accuracy: 0.9630\n",
      "\n",
      "Epoch:  352\n",
      "252/252 - 4s - loss: 0.0963 - accuracy: 0.9639 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0965 - accuracy: 0.9646\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1086 - accuracy: 0.9593\n",
      "\n",
      "Epoch:  353\n",
      "252/252 - 4s - loss: 0.0919 - accuracy: 0.9659 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0872 - accuracy: 0.9682\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1053 - accuracy: 0.9609\n",
      "\n",
      "Epoch:  354\n",
      "252/252 - 4s - loss: 0.0968 - accuracy: 0.9636 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0809 - accuracy: 0.9711\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0918 - accuracy: 0.9674\n",
      "\n",
      "Epoch:  355\n",
      "252/252 - 5s - loss: 0.0914 - accuracy: 0.9660 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0906 - accuracy: 0.9651\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1035 - accuracy: 0.9612\n",
      "\n",
      "Epoch:  356\n",
      "252/252 - 4s - loss: 0.0916 - accuracy: 0.9654 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0821 - accuracy: 0.9698\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0970 - accuracy: 0.9645\n",
      "\n",
      "Epoch:  357\n",
      "252/252 - 5s - loss: 0.0970 - accuracy: 0.9632 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1011 - accuracy: 0.9623\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1161 - accuracy: 0.9569\n",
      "\n",
      "Epoch:  358\n",
      "252/252 - 5s - loss: 0.0961 - accuracy: 0.9641 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0993 - accuracy: 0.9627\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1148 - accuracy: 0.9595\n",
      "\n",
      "Epoch:  359\n",
      "252/252 - 5s - loss: 0.0932 - accuracy: 0.9656 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0882 - accuracy: 0.9665\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0998 - accuracy: 0.9633\n",
      "\n",
      "Epoch:  360\n",
      "252/252 - 5s - loss: 0.0918 - accuracy: 0.9663 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0856 - accuracy: 0.9689\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0969 - accuracy: 0.9649\n",
      "\n",
      "Epoch:  361\n",
      "252/252 - 5s - loss: 0.0921 - accuracy: 0.9651 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0819 - accuracy: 0.9700\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0975 - accuracy: 0.9649\n",
      "\n",
      "Epoch:  362\n",
      "252/252 - 5s - loss: 0.0933 - accuracy: 0.9647 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0837 - accuracy: 0.9691\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0985 - accuracy: 0.9648\n",
      "\n",
      "Epoch:  363\n",
      "252/252 - 5s - loss: 0.0963 - accuracy: 0.9637 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0921 - accuracy: 0.9656\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1025 - accuracy: 0.9625\n",
      "\n",
      "Epoch:  364\n",
      "252/252 - 5s - loss: 0.0927 - accuracy: 0.9656 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0834 - accuracy: 0.9692\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0977 - accuracy: 0.9646\n",
      "\n",
      "Epoch:  365\n",
      "252/252 - 5s - loss: 0.0943 - accuracy: 0.9645 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0941 - accuracy: 0.9649\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1066 - accuracy: 0.9606\n",
      "\n",
      "Epoch:  366\n",
      "252/252 - 5s - loss: 0.0897 - accuracy: 0.9665 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1003 - accuracy: 0.9608\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1109 - accuracy: 0.9601\n",
      "\n",
      "Epoch:  367\n",
      "252/252 - 5s - loss: 0.0888 - accuracy: 0.9669 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0898 - accuracy: 0.9669\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1032 - accuracy: 0.9617\n",
      "\n",
      "Epoch:  368\n",
      "252/252 - 5s - loss: 0.0957 - accuracy: 0.9640 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0891 - accuracy: 0.9675\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1029 - accuracy: 0.9632\n",
      "\n",
      "Epoch:  369\n",
      "252/252 - 5s - loss: 0.0921 - accuracy: 0.9658 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0791 - accuracy: 0.9708\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0942 - accuracy: 0.9668\n",
      "\n",
      "Epoch:  370\n",
      "252/252 - 5s - loss: 0.0949 - accuracy: 0.9642 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0983 - accuracy: 0.9626\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1129 - accuracy: 0.9589\n",
      "\n",
      "Epoch:  371\n",
      "252/252 - 4s - loss: 0.0872 - accuracy: 0.9676 - 4s/epoch - 18ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 5s 3ms/step - loss: 0.0862 - accuracy: 0.9689\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1019 - accuracy: 0.9632\n",
      "\n",
      "Epoch:  372\n",
      "252/252 - 5s - loss: 0.0880 - accuracy: 0.9671 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0892 - accuracy: 0.9657\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1010 - accuracy: 0.9619\n",
      "\n",
      "Epoch:  373\n",
      "252/252 - 5s - loss: 0.0928 - accuracy: 0.9652 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0826 - accuracy: 0.9687\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0981 - accuracy: 0.9650\n",
      "\n",
      "Epoch:  374\n",
      "252/252 - 5s - loss: 0.0878 - accuracy: 0.9668 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1032 - accuracy: 0.9594\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1168 - accuracy: 0.9560\n",
      "\n",
      "Epoch:  375\n",
      "252/252 - 5s - loss: 0.0949 - accuracy: 0.9640 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0845 - accuracy: 0.9687\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1008 - accuracy: 0.9647\n",
      "\n",
      "Epoch:  376\n",
      "252/252 - 5s - loss: 0.0899 - accuracy: 0.9658 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1150 - accuracy: 0.9550\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1288 - accuracy: 0.9511\n",
      "\n",
      "Epoch:  377\n",
      "252/252 - 5s - loss: 0.0913 - accuracy: 0.9662 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0988 - accuracy: 0.9620\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1103 - accuracy: 0.9604\n",
      "\n",
      "Epoch:  378\n",
      "252/252 - 5s - loss: 0.0953 - accuracy: 0.9638 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0866 - accuracy: 0.9680\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1015 - accuracy: 0.9635\n",
      "\n",
      "Epoch:  379\n",
      "252/252 - 5s - loss: 0.0910 - accuracy: 0.9670 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1030 - accuracy: 0.9605\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1125 - accuracy: 0.9580\n",
      "\n",
      "Epoch:  380\n",
      "252/252 - 5s - loss: 0.0876 - accuracy: 0.9672 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0968 - accuracy: 0.9638\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.1123 - accuracy: 0.9588\n",
      "\n",
      "Epoch:  381\n",
      "252/252 - 5s - loss: 0.0882 - accuracy: 0.9670 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0820 - accuracy: 0.9696\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0972 - accuracy: 0.9650\n",
      "\n",
      "Epoch:  382\n",
      "252/252 - 5s - loss: 0.0856 - accuracy: 0.9686 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0712 - accuracy: 0.9742\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0858 - accuracy: 0.9695\n",
      "\n",
      "Epoch:  383\n",
      "252/252 - 5s - loss: 0.0900 - accuracy: 0.9660 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0752 - accuracy: 0.9727\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0935 - accuracy: 0.9668\n",
      "\n",
      "Epoch:  384\n",
      "252/252 - 5s - loss: 0.0847 - accuracy: 0.9685 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0834 - accuracy: 0.9696\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1016 - accuracy: 0.9639\n",
      "\n",
      "Epoch:  385\n",
      "252/252 - 5s - loss: 0.0832 - accuracy: 0.9698 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0779 - accuracy: 0.9713\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0940 - accuracy: 0.9660\n",
      "\n",
      "Epoch:  386\n",
      "252/252 - 5s - loss: 0.0913 - accuracy: 0.9658 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0851 - accuracy: 0.9692\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1030 - accuracy: 0.9614\n",
      "\n",
      "Epoch:  387\n",
      "252/252 - 5s - loss: 0.0888 - accuracy: 0.9661 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0931 - accuracy: 0.9644\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1096 - accuracy: 0.9606\n",
      "\n",
      "Epoch:  388\n",
      "252/252 - 5s - loss: 0.0880 - accuracy: 0.9674 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0706 - accuracy: 0.9743\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0864 - accuracy: 0.9699\n",
      "\n",
      "Epoch:  389\n",
      "252/252 - 5s - loss: 0.0859 - accuracy: 0.9684 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0732 - accuracy: 0.9738\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0923 - accuracy: 0.9670\n",
      "\n",
      "Epoch:  390\n",
      "252/252 - 5s - loss: 0.0856 - accuracy: 0.9680 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0783 - accuracy: 0.9719\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0971 - accuracy: 0.9650\n",
      "\n",
      "Epoch:  391\n",
      "252/252 - 5s - loss: 0.0918 - accuracy: 0.9655 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0845 - accuracy: 0.9685\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1019 - accuracy: 0.9649\n",
      "\n",
      "Epoch:  392\n",
      "252/252 - 5s - loss: 0.0872 - accuracy: 0.9676 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0741 - accuracy: 0.9732\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0900 - accuracy: 0.9670\n",
      "\n",
      "Epoch:  393\n",
      "252/252 - 5s - loss: 0.0849 - accuracy: 0.9677 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1070 - accuracy: 0.9599\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1224 - accuracy: 0.9545\n",
      "\n",
      "Epoch:  394\n",
      "252/252 - 5s - loss: 0.0898 - accuracy: 0.9671 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0769 - accuracy: 0.9712\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0942 - accuracy: 0.9678\n",
      "\n",
      "Epoch:  395\n",
      "252/252 - 5s - loss: 0.0819 - accuracy: 0.9697 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0747 - accuracy: 0.9731\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0906 - accuracy: 0.9685\n",
      "\n",
      "Epoch:  396\n",
      "252/252 - 5s - loss: 0.0843 - accuracy: 0.9687 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0830 - accuracy: 0.9698\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0973 - accuracy: 0.9638\n",
      "\n",
      "Epoch:  397\n",
      "252/252 - 5s - loss: 0.0839 - accuracy: 0.9690 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0759 - accuracy: 0.9723\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0906 - accuracy: 0.9673\n",
      "\n",
      "Epoch:  398\n",
      "252/252 - 5s - loss: 0.0823 - accuracy: 0.9689 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0914 - accuracy: 0.9649\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.1074 - accuracy: 0.9596\n",
      "\n",
      "Epoch:  399\n",
      "252/252 - 5s - loss: 0.0867 - accuracy: 0.9674 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1215 - accuracy: 0.9532\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1341 - accuracy: 0.9528\n",
      "\n",
      "Epoch:  400\n",
      "252/252 - 5s - loss: 0.0854 - accuracy: 0.9682 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0696 - accuracy: 0.9755\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0883 - accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for x in range(400):\n",
    "    print(\"\\nEpoch: \",x+1)\n",
    "    history = model.fit(X_train ,y_train ,batch_size=BATCH_SIZE ,epochs=1, verbose=2, shuffle=True)\n",
    "    if history_dataframe.size == 0:\n",
    "        history_dataframe = pd.DataFrame.from_dict(history.history)\n",
    "    else:\n",
    "        history_dataframe = pd.concat([history_dataframe,pd.DataFrame.from_dict(history.history)],axis=0,ignore_index=True)\n",
    "    \n",
    "    print(\"for training\")\n",
    "    train_eva.append(model.evaluate(X_train,y_train))\n",
    "    print(\"for testing\")\n",
    "    test_eva.append(model.evaluate(X_test, y_test))\n",
    "    \n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a52bb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  401\n",
      "252/252 - 8s - loss: 0.0835 - accuracy: 0.9684 - 8s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 6ms/step - loss: 0.0740 - accuracy: 0.9729\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0907 - accuracy: 0.9678\n",
      "\n",
      "Epoch:  402\n",
      "252/252 - 6s - loss: 0.0882 - accuracy: 0.9668 - 6s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0991 - accuracy: 0.9619\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1181 - accuracy: 0.9565\n",
      "\n",
      "Epoch:  403\n",
      "252/252 - 7s - loss: 0.0810 - accuracy: 0.9699 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 6ms/step - loss: 0.0867 - accuracy: 0.9673\n",
      "for testing\n",
      "504/504 [==============================] - 3s 7ms/step - loss: 0.1034 - accuracy: 0.9632\n",
      "\n",
      "Epoch:  404\n",
      "252/252 - 10s - loss: 0.0824 - accuracy: 0.9686 - 10s/epoch - 39ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0828 - accuracy: 0.9686\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0975 - accuracy: 0.9660\n",
      "\n",
      "Epoch:  405\n",
      "252/252 - 7s - loss: 0.0858 - accuracy: 0.9685 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0757 - accuracy: 0.9719\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0941 - accuracy: 0.9666\n",
      "\n",
      "Epoch:  406\n",
      "252/252 - 7s - loss: 0.0806 - accuracy: 0.9700 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0824 - accuracy: 0.9703\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0998 - accuracy: 0.9660\n",
      "\n",
      "Epoch:  407\n",
      "252/252 - 6s - loss: 0.0850 - accuracy: 0.9685 - 6s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0859 - accuracy: 0.9674\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1009 - accuracy: 0.9645\n",
      "\n",
      "Epoch:  408\n",
      "252/252 - 7s - loss: 0.0825 - accuracy: 0.9693 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0810 - accuracy: 0.9711\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0984 - accuracy: 0.9647\n",
      "\n",
      "Epoch:  409\n",
      "252/252 - 6s - loss: 0.0805 - accuracy: 0.9707 - 6s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0749 - accuracy: 0.9724\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0923 - accuracy: 0.9691\n",
      "\n",
      "Epoch:  410\n",
      "252/252 - 6s - loss: 0.0801 - accuracy: 0.9704 - 6s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0768 - accuracy: 0.9715\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0952 - accuracy: 0.9665\n",
      "\n",
      "Epoch:  411\n",
      "252/252 - 7s - loss: 0.0836 - accuracy: 0.9689 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0804 - accuracy: 0.9710\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0946 - accuracy: 0.9673\n",
      "\n",
      "Epoch:  412\n",
      "252/252 - 7s - loss: 0.0801 - accuracy: 0.9705 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0662 - accuracy: 0.9768\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0829 - accuracy: 0.9718\n",
      "\n",
      "Epoch:  413\n",
      "252/252 - 7s - loss: 0.0792 - accuracy: 0.9708 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 12s 6ms/step - loss: 0.0776 - accuracy: 0.9710\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.0947 - accuracy: 0.9661\n",
      "\n",
      "Epoch:  414\n",
      "252/252 - 10s - loss: 0.0829 - accuracy: 0.9684 - 10s/epoch - 42ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 6ms/step - loss: 0.0903 - accuracy: 0.9663\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.1095 - accuracy: 0.9625\n",
      "\n",
      "Epoch:  415\n",
      "252/252 - 8s - loss: 0.0812 - accuracy: 0.9701 - 8s/epoch - 31ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 6ms/step - loss: 0.0865 - accuracy: 0.9665\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.1044 - accuracy: 0.9619\n",
      "\n",
      "Epoch:  416\n",
      "252/252 - 7s - loss: 0.0835 - accuracy: 0.9692 - 7s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 12s 6ms/step - loss: 0.0928 - accuracy: 0.9644\n",
      "for testing\n",
      "504/504 [==============================] - 3s 7ms/step - loss: 0.1098 - accuracy: 0.9592\n",
      "\n",
      "Epoch:  417\n",
      "252/252 - 8s - loss: 0.0832 - accuracy: 0.9690 - 8s/epoch - 34ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 6ms/step - loss: 0.0841 - accuracy: 0.9691\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.1040 - accuracy: 0.9635\n",
      "\n",
      "Epoch:  418\n",
      "252/252 - 7s - loss: 0.0861 - accuracy: 0.9673 - 7s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.0831 - accuracy: 0.9685\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.1021 - accuracy: 0.9639\n",
      "\n",
      "Epoch:  419\n",
      "252/252 - 10s - loss: 0.0818 - accuracy: 0.9692 - 10s/epoch - 39ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.0951 - accuracy: 0.9644\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.1164 - accuracy: 0.9581\n",
      "\n",
      "Epoch:  420\n",
      "252/252 - 9s - loss: 0.0840 - accuracy: 0.9685 - 9s/epoch - 34ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0799 - accuracy: 0.9701\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.0963 - accuracy: 0.9664\n",
      "\n",
      "Epoch:  421\n",
      "252/252 - 7s - loss: 0.0771 - accuracy: 0.9713 - 7s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 6ms/step - loss: 0.0787 - accuracy: 0.9704\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0988 - accuracy: 0.9645\n",
      "\n",
      "Epoch:  422\n",
      "252/252 - 7s - loss: 0.0788 - accuracy: 0.9713 - 7s/epoch - 28ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 12s 6ms/step - loss: 0.0775 - accuracy: 0.9717\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0929 - accuracy: 0.9666\n",
      "\n",
      "Epoch:  423\n",
      "252/252 - 7s - loss: 0.0760 - accuracy: 0.9716 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0729 - accuracy: 0.9732\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0907 - accuracy: 0.9681\n",
      "\n",
      "Epoch:  424\n",
      "252/252 - 6s - loss: 0.0791 - accuracy: 0.9708 - 6s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0968 - accuracy: 0.9630\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.1138 - accuracy: 0.9597\n",
      "\n",
      "Epoch:  425\n",
      "252/252 - 7s - loss: 0.0817 - accuracy: 0.9696 - 7s/epoch - 28ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0805 - accuracy: 0.9696\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1005 - accuracy: 0.9636\n",
      "\n",
      "Epoch:  426\n",
      "252/252 - 7s - loss: 0.0781 - accuracy: 0.9711 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0758 - accuracy: 0.9719\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0947 - accuracy: 0.9673\n",
      "\n",
      "Epoch:  427\n",
      "252/252 - 6s - loss: 0.0766 - accuracy: 0.9714 - 6s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0683 - accuracy: 0.9751\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0869 - accuracy: 0.9692\n",
      "\n",
      "Epoch:  428\n",
      "252/252 - 7s - loss: 0.0747 - accuracy: 0.9728 - 7s/epoch - 29ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 6ms/step - loss: 0.0705 - accuracy: 0.9747\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0908 - accuracy: 0.9686\n",
      "\n",
      "Epoch:  429\n",
      "252/252 - 7s - loss: 0.0812 - accuracy: 0.9697 - 7s/epoch - 28ms/step\n",
      "for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016/2016 [==============================] - 11s 6ms/step - loss: 0.0736 - accuracy: 0.9726\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0918 - accuracy: 0.9686\n",
      "\n",
      "Epoch:  430\n",
      "252/252 - 7s - loss: 0.0815 - accuracy: 0.9698 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0727 - accuracy: 0.9737\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0910 - accuracy: 0.9687\n",
      "\n",
      "Epoch:  431\n",
      "252/252 - 7s - loss: 0.0851 - accuracy: 0.9683 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0879 - accuracy: 0.9655\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1020 - accuracy: 0.9616\n",
      "\n",
      "Epoch:  432\n",
      "252/252 - 7s - loss: 0.0798 - accuracy: 0.9701 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0725 - accuracy: 0.9745\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0912 - accuracy: 0.9683\n",
      "\n",
      "Epoch:  433\n",
      "252/252 - 9s - loss: 0.0748 - accuracy: 0.9729 - 9s/epoch - 34ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 13s 6ms/step - loss: 0.0718 - accuracy: 0.9738\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0880 - accuracy: 0.9693\n",
      "\n",
      "Epoch:  434\n",
      "252/252 - 6s - loss: 0.0753 - accuracy: 0.9725 - 6s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0695 - accuracy: 0.9744\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0873 - accuracy: 0.9694\n",
      "\n",
      "Epoch:  435\n",
      "252/252 - 7s - loss: 0.0772 - accuracy: 0.9712 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0683 - accuracy: 0.9751\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0873 - accuracy: 0.9699\n",
      "\n",
      "Epoch:  436\n",
      "252/252 - 6s - loss: 0.0752 - accuracy: 0.9724 - 6s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0895 - accuracy: 0.9656\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1044 - accuracy: 0.9638\n",
      "\n",
      "Epoch:  437\n",
      "252/252 - 7s - loss: 0.0749 - accuracy: 0.9724 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0679 - accuracy: 0.9754\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0887 - accuracy: 0.9692\n",
      "\n",
      "Epoch:  438\n",
      "252/252 - 7s - loss: 0.0781 - accuracy: 0.9710 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0703 - accuracy: 0.9745\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0878 - accuracy: 0.9690\n",
      "\n",
      "Epoch:  439\n",
      "252/252 - 7s - loss: 0.0753 - accuracy: 0.9719 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0843 - accuracy: 0.9678\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0998 - accuracy: 0.9639\n",
      "\n",
      "Epoch:  440\n",
      "252/252 - 6s - loss: 0.0769 - accuracy: 0.9719 - 6s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0693 - accuracy: 0.9747\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0868 - accuracy: 0.9697\n",
      "\n",
      "Epoch:  441\n",
      "252/252 - 7s - loss: 0.0742 - accuracy: 0.9723 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0644 - accuracy: 0.9763\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0859 - accuracy: 0.9706\n",
      "\n",
      "Epoch:  442\n",
      "252/252 - 7s - loss: 0.0747 - accuracy: 0.9719 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0621 - accuracy: 0.9781\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.0820 - accuracy: 0.9721\n",
      "\n",
      "Epoch:  443\n",
      "252/252 - 7s - loss: 0.0751 - accuracy: 0.9724 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0804 - accuracy: 0.9703\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1000 - accuracy: 0.9640\n",
      "\n",
      "Epoch:  444\n",
      "252/252 - 7s - loss: 0.0751 - accuracy: 0.9718 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0845 - accuracy: 0.9680\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1017 - accuracy: 0.9617\n",
      "\n",
      "Epoch:  445\n",
      "252/252 - 7s - loss: 0.0791 - accuracy: 0.9704 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0996 - accuracy: 0.9616\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.1179 - accuracy: 0.9560\n",
      "\n",
      "Epoch:  446\n",
      "252/252 - 6s - loss: 0.0721 - accuracy: 0.9738 - 6s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0804 - accuracy: 0.9696\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0977 - accuracy: 0.9649\n",
      "\n",
      "Epoch:  447\n",
      "252/252 - 6s - loss: 0.0768 - accuracy: 0.9713 - 6s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0799 - accuracy: 0.9696\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1030 - accuracy: 0.9630\n",
      "\n",
      "Epoch:  448\n",
      "252/252 - 6s - loss: 0.0733 - accuracy: 0.9730 - 6s/epoch - 25ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.1074 - accuracy: 0.9584\n",
      "for testing\n",
      "504/504 [==============================] - 2s 5ms/step - loss: 0.1288 - accuracy: 0.9537\n",
      "\n",
      "Epoch:  449\n",
      "252/252 - 7s - loss: 0.0754 - accuracy: 0.9727 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0640 - accuracy: 0.9773\n",
      "for testing\n",
      "504/504 [==============================] - 3s 6ms/step - loss: 0.0836 - accuracy: 0.9705\n",
      "\n",
      "Epoch:  450\n",
      "252/252 - 8s - loss: 0.0709 - accuracy: 0.9741 - 8s/epoch - 30ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 12s 6ms/step - loss: 0.0653 - accuracy: 0.9758\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0875 - accuracy: 0.9696\n",
      "\n",
      "Epoch:  451\n",
      "252/252 - 7s - loss: 0.0786 - accuracy: 0.9705 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0765 - accuracy: 0.9708\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0938 - accuracy: 0.9652\n",
      "\n",
      "Epoch:  452\n",
      "252/252 - 7s - loss: 0.0733 - accuracy: 0.9735 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0725 - accuracy: 0.9734\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0930 - accuracy: 0.9680\n",
      "\n",
      "Epoch:  453\n",
      "252/252 - 7s - loss: 0.0719 - accuracy: 0.9727 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0638 - accuracy: 0.9776\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0842 - accuracy: 0.9711\n",
      "\n",
      "Epoch:  454\n",
      "252/252 - 7s - loss: 0.0704 - accuracy: 0.9741 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 11s 5ms/step - loss: 0.0581 - accuracy: 0.9799\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0771 - accuracy: 0.9733\n",
      "\n",
      "Epoch:  455\n",
      "252/252 - 7s - loss: 0.0695 - accuracy: 0.9745 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0812 - accuracy: 0.9685\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0970 - accuracy: 0.9664\n",
      "\n",
      "Epoch:  456\n",
      "252/252 - 7s - loss: 0.0758 - accuracy: 0.9715 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0610 - accuracy: 0.9784\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0803 - accuracy: 0.9726\n",
      "\n",
      "Epoch:  457\n",
      "252/252 - 7s - loss: 0.0689 - accuracy: 0.9752 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0781 - accuracy: 0.9701\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0996 - accuracy: 0.9643\n",
      "\n",
      "Epoch:  458\n",
      "252/252 - 7s - loss: 0.0761 - accuracy: 0.9722 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0935 - accuracy: 0.9636\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.1154 - accuracy: 0.9566\n",
      "\n",
      "Epoch:  459\n",
      "252/252 - 7s - loss: 0.0760 - accuracy: 0.9715 - 7s/epoch - 26ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0759 - accuracy: 0.9715\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0981 - accuracy: 0.9643\n",
      "\n",
      "Epoch:  460\n",
      "252/252 - 7s - loss: 0.0693 - accuracy: 0.9745 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0626 - accuracy: 0.9776\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0797 - accuracy: 0.9728\n",
      "\n",
      "Epoch:  461\n",
      "252/252 - 7s - loss: 0.0712 - accuracy: 0.9733 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0681 - accuracy: 0.9753\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0851 - accuracy: 0.9700\n",
      "\n",
      "Epoch:  462\n",
      "252/252 - 7s - loss: 0.0728 - accuracy: 0.9731 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0692 - accuracy: 0.9742\n",
      "for testing\n",
      "504/504 [==============================] - 3s 5ms/step - loss: 0.0918 - accuracy: 0.9666\n",
      "\n",
      "Epoch:  463\n",
      "252/252 - 7s - loss: 0.0700 - accuracy: 0.9749 - 7s/epoch - 27ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 10s 5ms/step - loss: 0.0726 - accuracy: 0.9726\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0937 - accuracy: 0.9655\n",
      "\n",
      "Epoch:  464\n",
      "252/252 - 4s - loss: 0.0721 - accuracy: 0.9737 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0802 - accuracy: 0.9694\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1006 - accuracy: 0.9642\n",
      "\n",
      "Epoch:  465\n",
      "252/252 - 4s - loss: 0.0725 - accuracy: 0.9733 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0694 - accuracy: 0.9746\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0870 - accuracy: 0.9684\n",
      "\n",
      "Epoch:  466\n",
      "252/252 - 4s - loss: 0.0733 - accuracy: 0.9725 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0705 - accuracy: 0.9734\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0878 - accuracy: 0.9688\n",
      "\n",
      "Epoch:  467\n",
      "252/252 - 4s - loss: 0.0705 - accuracy: 0.9738 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0624 - accuracy: 0.9771\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0830 - accuracy: 0.9714\n",
      "\n",
      "Epoch:  468\n",
      "252/252 - 4s - loss: 0.0691 - accuracy: 0.9744 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0657 - accuracy: 0.9759\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0885 - accuracy: 0.9689\n",
      "\n",
      "Epoch:  469\n",
      "252/252 - 4s - loss: 0.0703 - accuracy: 0.9735 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0605 - accuracy: 0.9779\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0792 - accuracy: 0.9722\n",
      "\n",
      "Epoch:  470\n",
      "252/252 - 4s - loss: 0.0713 - accuracy: 0.9731 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0728 - accuracy: 0.9726\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0946 - accuracy: 0.9656\n",
      "\n",
      "Epoch:  471\n",
      "252/252 - 4s - loss: 0.0697 - accuracy: 0.9744 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0623 - accuracy: 0.9780\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0859 - accuracy: 0.9697\n",
      "\n",
      "Epoch:  472\n",
      "252/252 - 4s - loss: 0.0706 - accuracy: 0.9743 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0616 - accuracy: 0.9777\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0808 - accuracy: 0.9723\n",
      "\n",
      "Epoch:  473\n",
      "252/252 - 4s - loss: 0.0712 - accuracy: 0.9738 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0595 - accuracy: 0.9788\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0809 - accuracy: 0.9734\n",
      "\n",
      "Epoch:  474\n",
      "252/252 - 4s - loss: 0.0682 - accuracy: 0.9748 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0585 - accuracy: 0.9788\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0808 - accuracy: 0.9724\n",
      "\n",
      "Epoch:  475\n",
      "252/252 - 4s - loss: 0.0697 - accuracy: 0.9745 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0612 - accuracy: 0.9778\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0829 - accuracy: 0.9719\n",
      "\n",
      "Epoch:  476\n",
      "252/252 - 4s - loss: 0.0708 - accuracy: 0.9740 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0765 - accuracy: 0.9713\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0943 - accuracy: 0.9653\n",
      "\n",
      "Epoch:  477\n",
      "252/252 - 4s - loss: 0.0674 - accuracy: 0.9754 - 4s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0685 - accuracy: 0.9740\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0897 - accuracy: 0.9668\n",
      "\n",
      "Epoch:  478\n",
      "252/252 - 4s - loss: 0.0698 - accuracy: 0.9740 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0838 - accuracy: 0.9685\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1043 - accuracy: 0.9621\n",
      "\n",
      "Epoch:  479\n",
      "252/252 - 5s - loss: 0.0695 - accuracy: 0.9740 - 5s/epoch - 18ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0804 - accuracy: 0.9691\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0978 - accuracy: 0.9640\n",
      "\n",
      "Epoch:  480\n",
      "252/252 - 4s - loss: 0.0693 - accuracy: 0.9737 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0777 - accuracy: 0.9705\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0995 - accuracy: 0.9639\n",
      "\n",
      "Epoch:  481\n",
      "252/252 - 4s - loss: 0.0678 - accuracy: 0.9748 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0803 - accuracy: 0.9691\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1012 - accuracy: 0.9634\n",
      "\n",
      "Epoch:  482\n",
      "252/252 - 4s - loss: 0.0681 - accuracy: 0.9755 - 4s/epoch - 17ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.1056 - accuracy: 0.9591\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1270 - accuracy: 0.9543\n",
      "\n",
      "Epoch:  483\n",
      "252/252 - 5s - loss: 0.0697 - accuracy: 0.9742 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0598 - accuracy: 0.9787\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0811 - accuracy: 0.9720\n",
      "\n",
      "Epoch:  484\n",
      "252/252 - 5s - loss: 0.0648 - accuracy: 0.9764 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0623 - accuracy: 0.9773\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0822 - accuracy: 0.9712\n",
      "\n",
      "Epoch:  485\n",
      "252/252 - 5s - loss: 0.0689 - accuracy: 0.9743 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0575 - accuracy: 0.9796\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0795 - accuracy: 0.9736\n",
      "\n",
      "Epoch:  486\n",
      "252/252 - 5s - loss: 0.0687 - accuracy: 0.9748 - 5s/epoch - 21ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0643 - accuracy: 0.9761\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0867 - accuracy: 0.9686\n",
      "\n",
      "Epoch:  487\n",
      "252/252 - 5s - loss: 0.0689 - accuracy: 0.9740 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0704 - accuracy: 0.9730\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0921 - accuracy: 0.9685\n",
      "\n",
      "Epoch:  488\n",
      "252/252 - 6s - loss: 0.0683 - accuracy: 0.9752 - 6s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0596 - accuracy: 0.9791\n",
      "for testing\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 0.0807 - accuracy: 0.9722\n",
      "\n",
      "Epoch:  489\n",
      "252/252 - 5s - loss: 0.0650 - accuracy: 0.9761 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0583 - accuracy: 0.9790\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0775 - accuracy: 0.9720\n",
      "\n",
      "Epoch:  490\n",
      "252/252 - 5s - loss: 0.0717 - accuracy: 0.9730 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0643 - accuracy: 0.9763\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0844 - accuracy: 0.9703\n",
      "\n",
      "Epoch:  491\n",
      "252/252 - 5s - loss: 0.0703 - accuracy: 0.9738 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0580 - accuracy: 0.9792\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0769 - accuracy: 0.9721\n",
      "\n",
      "Epoch:  492\n",
      "252/252 - 5s - loss: 0.0658 - accuracy: 0.9759 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0611 - accuracy: 0.9781\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0821 - accuracy: 0.9725\n",
      "\n",
      "Epoch:  493\n",
      "252/252 - 5s - loss: 0.0670 - accuracy: 0.9747 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0821 - accuracy: 0.9677\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0985 - accuracy: 0.9636\n",
      "\n",
      "Epoch:  494\n",
      "252/252 - 5s - loss: 0.0668 - accuracy: 0.9753 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0749 - accuracy: 0.9725\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0949 - accuracy: 0.9658\n",
      "\n",
      "Epoch:  495\n",
      "252/252 - 5s - loss: 0.0668 - accuracy: 0.9749 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0601 - accuracy: 0.9785\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0832 - accuracy: 0.9702\n",
      "\n",
      "Epoch:  496\n",
      "252/252 - 5s - loss: 0.0650 - accuracy: 0.9762 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0796 - accuracy: 0.9708\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.1003 - accuracy: 0.9646\n",
      "\n",
      "Epoch:  497\n",
      "252/252 - 5s - loss: 0.0668 - accuracy: 0.9753 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0623 - accuracy: 0.9786\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0799 - accuracy: 0.9734\n",
      "\n",
      "Epoch:  498\n",
      "252/252 - 5s - loss: 0.0681 - accuracy: 0.9743 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0542 - accuracy: 0.9808\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0740 - accuracy: 0.9749\n",
      "\n",
      "Epoch:  499\n",
      "252/252 - 5s - loss: 0.0646 - accuracy: 0.9761 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0917 - accuracy: 0.9642\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.1158 - accuracy: 0.9575\n",
      "\n",
      "Epoch:  500\n",
      "252/252 - 5s - loss: 0.0658 - accuracy: 0.9757 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0586 - accuracy: 0.9787\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0834 - accuracy: 0.9719\n",
      "\n",
      "Epoch:  501\n",
      "252/252 - 5s - loss: 0.0651 - accuracy: 0.9758 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0746 - accuracy: 0.9715\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0944 - accuracy: 0.9673\n",
      "\n",
      "Epoch:  502\n",
      "252/252 - 5s - loss: 0.0641 - accuracy: 0.9763 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0629 - accuracy: 0.9764\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0851 - accuracy: 0.9722\n",
      "\n",
      "Epoch:  503\n",
      "252/252 - 5s - loss: 0.0623 - accuracy: 0.9772 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0606 - accuracy: 0.9774\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0790 - accuracy: 0.9725\n",
      "\n",
      "Epoch:  504\n",
      "252/252 - 5s - loss: 0.0646 - accuracy: 0.9762 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0662 - accuracy: 0.9756\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0870 - accuracy: 0.9680\n",
      "\n",
      "Epoch:  505\n",
      "252/252 - 5s - loss: 0.0637 - accuracy: 0.9763 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0642 - accuracy: 0.9759\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0841 - accuracy: 0.9694\n",
      "\n",
      "Epoch:  506\n",
      "252/252 - 5s - loss: 0.0647 - accuracy: 0.9763 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0650 - accuracy: 0.9762\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0866 - accuracy: 0.9689\n",
      "\n",
      "Epoch:  507\n",
      "252/252 - 5s - loss: 0.0654 - accuracy: 0.9759 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0578 - accuracy: 0.9787\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0773 - accuracy: 0.9725\n",
      "\n",
      "Epoch:  508\n",
      "252/252 - 5s - loss: 0.0621 - accuracy: 0.9772 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0589 - accuracy: 0.9783\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0759 - accuracy: 0.9729\n",
      "\n",
      "Epoch:  509\n",
      "252/252 - 5s - loss: 0.0617 - accuracy: 0.9778 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0687 - accuracy: 0.9741\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0880 - accuracy: 0.9679\n",
      "\n",
      "Epoch:  510\n",
      "252/252 - 5s - loss: 0.0682 - accuracy: 0.9749 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0610 - accuracy: 0.9784\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0813 - accuracy: 0.9719\n",
      "\n",
      "Epoch:  511\n",
      "252/252 - 5s - loss: 0.0681 - accuracy: 0.9745 - 5s/epoch - 19ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0552 - accuracy: 0.9801\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0738 - accuracy: 0.9745\n",
      "\n",
      "Epoch:  512\n",
      "252/252 - 5s - loss: 0.0598 - accuracy: 0.9790 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0523 - accuracy: 0.9815\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9733\n",
      "\n",
      "Epoch:  513\n",
      "252/252 - 5s - loss: 0.0625 - accuracy: 0.9770 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0562 - accuracy: 0.9797\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9737\n",
      "\n",
      "Epoch:  514\n",
      "252/252 - 5s - loss: 0.0652 - accuracy: 0.9761 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0599 - accuracy: 0.9782\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0805 - accuracy: 0.9712\n",
      "\n",
      "Epoch:  515\n",
      "252/252 - 5s - loss: 0.0617 - accuracy: 0.9769 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0519 - accuracy: 0.9816\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9733\n",
      "\n",
      "Epoch:  516\n",
      "252/252 - 5s - loss: 0.0627 - accuracy: 0.9774 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0635 - accuracy: 0.9766\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0859 - accuracy: 0.9677\n",
      "\n",
      "Epoch:  517\n",
      "252/252 - 5s - loss: 0.0595 - accuracy: 0.9786 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0498 - accuracy: 0.9829\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0726 - accuracy: 0.9751\n",
      "\n",
      "Epoch:  518\n",
      "252/252 - 5s - loss: 0.0660 - accuracy: 0.9759 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0623 - accuracy: 0.9762\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0842 - accuracy: 0.9702\n",
      "\n",
      "Epoch:  519\n",
      "252/252 - 5s - loss: 0.0625 - accuracy: 0.9772 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0663 - accuracy: 0.9746\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0881 - accuracy: 0.9687\n",
      "\n",
      "Epoch:  520\n",
      "252/252 - 5s - loss: 0.0613 - accuracy: 0.9770 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0683 - accuracy: 0.9743\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0881 - accuracy: 0.9688\n",
      "\n",
      "Epoch:  521\n",
      "252/252 - 5s - loss: 0.0635 - accuracy: 0.9761 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0495 - accuracy: 0.9825\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0717 - accuracy: 0.9751\n",
      "\n",
      "Epoch:  522\n",
      "252/252 - 5s - loss: 0.0633 - accuracy: 0.9768 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0524 - accuracy: 0.9812\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0729 - accuracy: 0.9753\n",
      "\n",
      "Epoch:  523\n",
      "252/252 - 5s - loss: 0.0605 - accuracy: 0.9778 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0591 - accuracy: 0.9782\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0824 - accuracy: 0.9713\n",
      "\n",
      "Epoch:  524\n",
      "252/252 - 5s - loss: 0.0691 - accuracy: 0.9741 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0546 - accuracy: 0.9806\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9735\n",
      "\n",
      "Epoch:  525\n",
      "252/252 - 5s - loss: 0.0625 - accuracy: 0.9769 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0600 - accuracy: 0.9772\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9720\n",
      "\n",
      "Epoch:  526\n",
      "252/252 - 5s - loss: 0.0662 - accuracy: 0.9757 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0604 - accuracy: 0.9780\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9715\n",
      "\n",
      "Epoch:  527\n",
      "252/252 - 5s - loss: 0.0603 - accuracy: 0.9774 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0484 - accuracy: 0.9830\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0695 - accuracy: 0.9751\n",
      "\n",
      "Epoch:  528\n",
      "252/252 - 5s - loss: 0.0581 - accuracy: 0.9785 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0569 - accuracy: 0.9792\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9725\n",
      "\n",
      "Epoch:  529\n",
      "252/252 - 5s - loss: 0.0587 - accuracy: 0.9785 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0520 - accuracy: 0.9816\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0741 - accuracy: 0.9740\n",
      "\n",
      "Epoch:  530\n",
      "252/252 - 5s - loss: 0.0596 - accuracy: 0.9782 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0647 - accuracy: 0.9754\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0835 - accuracy: 0.9706\n",
      "\n",
      "Epoch:  531\n",
      "252/252 - 5s - loss: 0.0638 - accuracy: 0.9763 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0666 - accuracy: 0.9751\n",
      "for testing\n",
      "504/504 [==============================] - 1s 3ms/step - loss: 0.0889 - accuracy: 0.9673\n",
      "\n",
      "Epoch:  532\n",
      "252/252 - 5s - loss: 0.0606 - accuracy: 0.9780 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0512 - accuracy: 0.9819\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0747 - accuracy: 0.9748\n",
      "\n",
      "Epoch:  533\n",
      "252/252 - 5s - loss: 0.0581 - accuracy: 0.9786 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0620 - accuracy: 0.9768\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0793 - accuracy: 0.9712\n",
      "\n",
      "Epoch:  534\n",
      "252/252 - 5s - loss: 0.0605 - accuracy: 0.9776 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0562 - accuracy: 0.9792\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9712\n",
      "\n",
      "Epoch:  535\n",
      "252/252 - 5s - loss: 0.0605 - accuracy: 0.9773 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0623 - accuracy: 0.9773\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0830 - accuracy: 0.9706\n",
      "\n",
      "Epoch:  536\n",
      "252/252 - 5s - loss: 0.0615 - accuracy: 0.9772 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0545 - accuracy: 0.9800\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0768 - accuracy: 0.9725\n",
      "\n",
      "Epoch:  537\n",
      "252/252 - 5s - loss: 0.0599 - accuracy: 0.9780 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0490 - accuracy: 0.9830\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0722 - accuracy: 0.9741\n",
      "\n",
      "Epoch:  538\n",
      "252/252 - 5s - loss: 0.0616 - accuracy: 0.9768 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0545 - accuracy: 0.9803\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9722\n",
      "\n",
      "Epoch:  539\n",
      "252/252 - 5s - loss: 0.0622 - accuracy: 0.9773 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0535 - accuracy: 0.9800\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0773 - accuracy: 0.9729\n",
      "\n",
      "Epoch:  540\n",
      "252/252 - 5s - loss: 0.0575 - accuracy: 0.9792 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0542 - accuracy: 0.9800\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0748 - accuracy: 0.9744\n",
      "\n",
      "Epoch:  541\n",
      "252/252 - 5s - loss: 0.0597 - accuracy: 0.9783 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0730 - accuracy: 0.9721\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0935 - accuracy: 0.9656\n",
      "\n",
      "Epoch:  542\n",
      "252/252 - 5s - loss: 0.0619 - accuracy: 0.9772 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0513 - accuracy: 0.9812\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0730 - accuracy: 0.9733\n",
      "\n",
      "Epoch:  543\n",
      "252/252 - 5s - loss: 0.0597 - accuracy: 0.9788 - 5s/epoch - 21ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0532 - accuracy: 0.9804\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0728 - accuracy: 0.9750\n",
      "\n",
      "Epoch:  544\n",
      "252/252 - 5s - loss: 0.0601 - accuracy: 0.9778 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0522 - accuracy: 0.9812\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9745\n",
      "\n",
      "Epoch:  545\n",
      "252/252 - 5s - loss: 0.0572 - accuracy: 0.9788 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0625 - accuracy: 0.9768\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0851 - accuracy: 0.9703\n",
      "\n",
      "Epoch:  546\n",
      "252/252 - 5s - loss: 0.0563 - accuracy: 0.9798 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0515 - accuracy: 0.9813\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0712 - accuracy: 0.9742\n",
      "\n",
      "Epoch:  547\n",
      "252/252 - 5s - loss: 0.0614 - accuracy: 0.9767 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0578 - accuracy: 0.9790\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0803 - accuracy: 0.9722\n",
      "\n",
      "Epoch:  548\n",
      "252/252 - 5s - loss: 0.0550 - accuracy: 0.9798 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0533 - accuracy: 0.9810\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0762 - accuracy: 0.9730\n",
      "\n",
      "Epoch:  549\n",
      "252/252 - 5s - loss: 0.0596 - accuracy: 0.9785 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0550 - accuracy: 0.9801\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9728\n",
      "\n",
      "Epoch:  550\n",
      "252/252 - 5s - loss: 0.0595 - accuracy: 0.9775 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0622 - accuracy: 0.9764\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0858 - accuracy: 0.9706\n",
      "\n",
      "Epoch:  551\n",
      "252/252 - 5s - loss: 0.0621 - accuracy: 0.9770 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0594 - accuracy: 0.9781\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0803 - accuracy: 0.9715\n",
      "\n",
      "Epoch:  552\n",
      "252/252 - 5s - loss: 0.0612 - accuracy: 0.9774 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0673 - accuracy: 0.9753\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0905 - accuracy: 0.9679\n",
      "\n",
      "Epoch:  553\n",
      "252/252 - 5s - loss: 0.0574 - accuracy: 0.9796 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0481 - accuracy: 0.9829\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9762\n",
      "\n",
      "Epoch:  554\n",
      "252/252 - 5s - loss: 0.0579 - accuracy: 0.9789 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0517 - accuracy: 0.9811\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9737\n",
      "\n",
      "Epoch:  555\n",
      "252/252 - 5s - loss: 0.0585 - accuracy: 0.9784 - 5s/epoch - 20ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0548 - accuracy: 0.9800\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0755 - accuracy: 0.9723\n",
      "\n",
      "Epoch:  556\n",
      "252/252 - 5s - loss: 0.0575 - accuracy: 0.9783 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0501 - accuracy: 0.9815\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9741\n",
      "\n",
      "Epoch:  557\n",
      "252/252 - 5s - loss: 0.0556 - accuracy: 0.9793 - 5s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 8s 4ms/step - loss: 0.0583 - accuracy: 0.9780\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0788 - accuracy: 0.9723\n",
      "\n",
      "Epoch:  558\n",
      "252/252 - 6s - loss: 0.0566 - accuracy: 0.9794 - 6s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 9s 4ms/step - loss: 0.0555 - accuracy: 0.9797\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9729\n",
      "\n",
      "Epoch:  559\n",
      "252/252 - 5s - loss: 0.0577 - accuracy: 0.9787 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0483 - accuracy: 0.9831\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0704 - accuracy: 0.9762\n",
      "\n",
      "Epoch:  560\n",
      "252/252 - 5s - loss: 0.0559 - accuracy: 0.9796 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0582 - accuracy: 0.9781\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.9709\n",
      "\n",
      "Epoch:  561\n",
      "252/252 - 5s - loss: 0.0538 - accuracy: 0.9803 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0504 - accuracy: 0.9821\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0722 - accuracy: 0.9732\n",
      "\n",
      "Epoch:  562\n",
      "252/252 - 5s - loss: 0.0538 - accuracy: 0.9801 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0468 - accuracy: 0.9830\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0670 - accuracy: 0.9758\n",
      "\n",
      "Epoch:  563\n",
      "252/252 - 5s - loss: 0.0553 - accuracy: 0.9795 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0603 - accuracy: 0.9775\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0797 - accuracy: 0.9714\n",
      "\n",
      "Epoch:  564\n",
      "252/252 - 5s - loss: 0.0577 - accuracy: 0.9789 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0436 - accuracy: 0.9849\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0656 - accuracy: 0.9766\n",
      "\n",
      "Epoch:  565\n",
      "252/252 - 5s - loss: 0.0562 - accuracy: 0.9792 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0563 - accuracy: 0.9791\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0777 - accuracy: 0.9713\n",
      "\n",
      "Epoch:  566\n",
      "252/252 - 5s - loss: 0.0553 - accuracy: 0.9797 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0611 - accuracy: 0.9768\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0827 - accuracy: 0.9714\n",
      "\n",
      "Epoch:  567\n",
      "252/252 - 5s - loss: 0.0561 - accuracy: 0.9792 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0522 - accuracy: 0.9805\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0700 - accuracy: 0.9745\n",
      "\n",
      "Epoch:  568\n",
      "252/252 - 5s - loss: 0.0599 - accuracy: 0.9776 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0647 - accuracy: 0.9759\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0863 - accuracy: 0.9690\n",
      "\n",
      "Epoch:  569\n",
      "252/252 - 5s - loss: 0.0565 - accuracy: 0.9791 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0624 - accuracy: 0.9764\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0825 - accuracy: 0.9705\n",
      "\n",
      "Epoch:  570\n",
      "252/252 - 5s - loss: 0.0546 - accuracy: 0.9804 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0471 - accuracy: 0.9837\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9764\n",
      "\n",
      "Epoch:  571\n",
      "252/252 - 5s - loss: 0.0565 - accuracy: 0.9790 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0769 - accuracy: 0.9714\n",
      "for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 2s 3ms/step - loss: 0.1005 - accuracy: 0.9638\n",
      "\n",
      "Epoch:  572\n",
      "252/252 - 5s - loss: 0.0524 - accuracy: 0.9806 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0513 - accuracy: 0.9816\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0703 - accuracy: 0.9731\n",
      "\n",
      "Epoch:  573\n",
      "252/252 - 5s - loss: 0.0559 - accuracy: 0.9792 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0443 - accuracy: 0.9844\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0640 - accuracy: 0.9769\n",
      "\n",
      "Epoch:  574\n",
      "252/252 - 5s - loss: 0.0534 - accuracy: 0.9801 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0495 - accuracy: 0.9816\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0672 - accuracy: 0.9756\n",
      "\n",
      "Epoch:  575\n",
      "252/252 - 5s - loss: 0.0569 - accuracy: 0.9792 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0496 - accuracy: 0.9818\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9740\n",
      "\n",
      "Epoch:  576\n",
      "252/252 - 5s - loss: 0.0571 - accuracy: 0.9786 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0531 - accuracy: 0.9798\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0762 - accuracy: 0.9717\n",
      "\n",
      "Epoch:  577\n",
      "252/252 - 5s - loss: 0.0529 - accuracy: 0.9805 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0529 - accuracy: 0.9811\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0746 - accuracy: 0.9741\n",
      "\n",
      "Epoch:  578\n",
      "252/252 - 5s - loss: 0.0562 - accuracy: 0.9794 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0471 - accuracy: 0.9833\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0678 - accuracy: 0.9746\n",
      "\n",
      "Epoch:  579\n",
      "252/252 - 5s - loss: 0.0530 - accuracy: 0.9808 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0517 - accuracy: 0.9809\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9742\n",
      "\n",
      "Epoch:  580\n",
      "252/252 - 5s - loss: 0.0536 - accuracy: 0.9806 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0517 - accuracy: 0.9811\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9746\n",
      "\n",
      "Epoch:  581\n",
      "252/252 - 5s - loss: 0.0547 - accuracy: 0.9796 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0659 - accuracy: 0.9750\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0877 - accuracy: 0.9684\n",
      "\n",
      "Epoch:  582\n",
      "252/252 - 5s - loss: 0.0542 - accuracy: 0.9798 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0544 - accuracy: 0.9798\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9723\n",
      "\n",
      "Epoch:  583\n",
      "252/252 - 5s - loss: 0.0554 - accuracy: 0.9796 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0527 - accuracy: 0.9809\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0739 - accuracy: 0.9730\n",
      "\n",
      "Epoch:  584\n",
      "252/252 - 5s - loss: 0.0548 - accuracy: 0.9794 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0526 - accuracy: 0.9808\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0702 - accuracy: 0.9758\n",
      "\n",
      "Epoch:  585\n",
      "252/252 - 5s - loss: 0.0571 - accuracy: 0.9792 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0408 - accuracy: 0.9861\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9780\n",
      "\n",
      "Epoch:  586\n",
      "252/252 - 6s - loss: 0.0540 - accuracy: 0.9798 - 6s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0647 - accuracy: 0.9754\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0872 - accuracy: 0.9671\n",
      "\n",
      "Epoch:  587\n",
      "252/252 - 5s - loss: 0.0512 - accuracy: 0.9815 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0416 - accuracy: 0.9858\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0644 - accuracy: 0.9756\n",
      "\n",
      "Epoch:  588\n",
      "252/252 - 5s - loss: 0.0521 - accuracy: 0.9809 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0493 - accuracy: 0.9816\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9736\n",
      "\n",
      "Epoch:  589\n",
      "252/252 - 5s - loss: 0.0515 - accuracy: 0.9807 - 5s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0621 - accuracy: 0.9761\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0879 - accuracy: 0.9682\n",
      "\n",
      "Epoch:  590\n",
      "252/252 - 5s - loss: 0.0542 - accuracy: 0.9795 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0534 - accuracy: 0.9805\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0776 - accuracy: 0.9722\n",
      "\n",
      "Epoch:  591\n",
      "252/252 - 5s - loss: 0.0496 - accuracy: 0.9817 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0525 - accuracy: 0.9800\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0756 - accuracy: 0.9732\n",
      "\n",
      "Epoch:  592\n",
      "252/252 - 5s - loss: 0.0535 - accuracy: 0.9801 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0593 - accuracy: 0.9776\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9695\n",
      "\n",
      "Epoch:  593\n",
      "252/252 - 6s - loss: 0.0543 - accuracy: 0.9799 - 6s/epoch - 23ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0599 - accuracy: 0.9775\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0789 - accuracy: 0.9728\n",
      "\n",
      "Epoch:  594\n",
      "252/252 - 5s - loss: 0.0543 - accuracy: 0.9797 - 5s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0675 - accuracy: 0.9739\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0940 - accuracy: 0.9665\n",
      "\n",
      "Epoch:  595\n",
      "252/252 - 5s - loss: 0.0504 - accuracy: 0.9814 - 5s/epoch - 21ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0519 - accuracy: 0.9813\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9728\n",
      "\n",
      "Epoch:  596\n",
      "252/252 - 5s - loss: 0.0530 - accuracy: 0.9807 - 5s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0457 - accuracy: 0.9834\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0702 - accuracy: 0.9744\n",
      "\n",
      "Epoch:  597\n",
      "252/252 - 5s - loss: 0.0500 - accuracy: 0.9816 - 5s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0437 - accuracy: 0.9836\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0664 - accuracy: 0.9761\n",
      "\n",
      "Epoch:  598\n",
      "252/252 - 6s - loss: 0.0518 - accuracy: 0.9812 - 6s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0569 - accuracy: 0.9786\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0791 - accuracy: 0.9699\n",
      "\n",
      "Epoch:  599\n",
      "252/252 - 5s - loss: 0.0535 - accuracy: 0.9797 - 5s/epoch - 22ms/step\n",
      "for training\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.0655 - accuracy: 0.9752\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0840 - accuracy: 0.9696\n",
      "\n",
      "Epoch:  600\n",
      "252/252 - 5s - loss: 0.0539 - accuracy: 0.9800 - 5s/epoch - 21ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.0457 - accuracy: 0.9835\n",
      "for testing\n",
      "504/504 [==============================] - 2s 3ms/step - loss: 0.0684 - accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "for x in range(200):\n",
    "    print(\"\\nEpoch: \",400+x+1)\n",
    "    history = model.fit(X_train ,y_train ,batch_size=BATCH_SIZE ,epochs=1, verbose=2, shuffle=True)\n",
    "    if history_dataframe.size == 0:\n",
    "        history_dataframe = pd.DataFrame.from_dict(history.history)\n",
    "    else:\n",
    "        history_dataframe = pd.concat([history_dataframe,pd.DataFrame.from_dict(history.history)],axis=0,ignore_index=True)\n",
    "    \n",
    "    print(\"for training\")\n",
    "    train_eva.append(model.evaluate(X_train,y_train))\n",
    "    print(\"for testing\")\n",
    "    test_eva.append(model.evaluate(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec37c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.212283</td>\n",
       "      <td>0.441019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.104193</td>\n",
       "      <td>0.530320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.005266</td>\n",
       "      <td>0.568716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.898711</td>\n",
       "      <td>0.611282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.842148</td>\n",
       "      <td>0.629139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.084250</td>\n",
       "      <td>0.968688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.083923</td>\n",
       "      <td>0.968967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.082259</td>\n",
       "      <td>0.968921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.086699</td>\n",
       "      <td>0.967386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.085447</td>\n",
       "      <td>0.968161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy\n",
       "0    1.212283  0.441019\n",
       "1    1.104193  0.530320\n",
       "2    1.005266  0.568716\n",
       "3    0.898711  0.611282\n",
       "4    0.842148  0.629139\n",
       "..        ...       ...\n",
       "395  0.084250  0.968688\n",
       "396  0.083923  0.968967\n",
       "397  0.082259  0.968921\n",
       "398  0.086699  0.967386\n",
       "399  0.085447  0.968161\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c8463e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eva_df = pd.DataFrame(train_eva,columns=['loss','accuracy'])\n",
    "test_eva_df = pd.DataFrame(test_eva,columns=['loss','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cc95506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(df,title,xlabel,ylabel,values=['loss'],legends=[]):\n",
    "    \n",
    "    for value in values:\n",
    "        epoch_count = range(1, len(df.index) + 1)\n",
    "        plt.plot(epoch_count, df[value].tolist())\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    if legends==[]:\n",
    "        legends = values\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35c781fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAACJzElEQVR4nO3dd3iTVRsG8DtJ03TvTUspUPYuAmULMl2AG1FQUREUEScu+Fw4Ef0QFBXHJwouFBXZU5C9oezRlrZ075E2eb8/TvImadKdNm25f9fVK8m7ct4GzdPnPOcchSRJEoiIiIiaCaWjG0BERERkTwxuiIiIqFlhcENERETNCoMbIiIialYY3BAREVGzwuCGiIiImhUGN0RERNSsMLghIiKiZoXBDRERETUrDG6IyKEUCkW1frZu3Vqn95k3bx4UCoV9Gk1EjZqCyy8QkSPt3r3b4vXrr7+OLVu2YPPmzRbbO3XqBC8vr1q/T2JiIhITE9GvX79aX4OImgYnRzeAiK5t5YONwMBAKJXKKoOQwsJCuLm5Vft9wsPDER4eXqs2ElHTwm4pImr0hg4dii5dumD79u3o378/3Nzc8OCDDwIAVq5ciZEjRyI0NBSurq7o2LEjXnjhBRQUFFhcw1a3VKtWrXDTTTdh7dq16NWrF1xdXdGhQwcsW7aswe6NiOyPmRsiahKSk5MxadIkPPfcc3jrrbegVIq/zc6ePYuxY8di1qxZcHd3x6lTp/DOO+9g7969Vl1bthw5cgRPP/00XnjhBQQHB+OLL77AQw89hLZt22Lw4MH1fVtEVA8Y3BBRk5CZmYmffvoJw4YNs9j+8ssvy88lScKAAQPQsWNHDBkyBEePHkW3bt0qvW56ejp27tyJli1bAgAGDx6MTZs24fvvv2dwQ9REsVuKiJoEX19fq8AGAC5cuICJEyciJCQEKpUKarUaQ4YMAQDExcVVed0ePXrIgQ0AuLi4oF27drh8+bL9Gk9EDYqZGyJqEkJDQ6225efnY9CgQXBxccEbb7yBdu3awc3NDQkJCZgwYQKKioqqvK6/v7/VNo1GU61ziahxYnBDRE2CrTlqNm/ejKSkJGzdulXO1gBAdnZ2A7aMiBobdksRUZNlDHg0Go3F9s8++8wRzSGiRoKZGyJqsvr37w9fX19MmzYNc+fOhVqtxvLly3HkyBFHN42IHIiZGyJqsvz9/fHXX3/Bzc0NkyZNwoMPPggPDw+sXLnS0U0jIgfi8gtERETUrDBzQ0RERM0KgxsiIiJqVhjcEBERUbPC4IaIiIiaFQY3RERE1KwwuCEiIqJm5ZqbxE+v1yMpKQmenp42p3MnIiKixkeSJOTl5SEsLAxKZeW5mWsuuElKSkJERISjm0FERES1kJCQgPDw8EqPueaCG09PTwDil+Pl5eXg1hAREVF15ObmIiIiQv4er8w1F9wYu6K8vLwY3BARETUx1SkpYUExERERNSsMboiIiKhZYXBDREREzco1V3NTXTqdDqWlpY5uBtWAWq2GSqVydDOIiMjBGNyUI0kSUlJSkJ2d7eimUC34+PggJCSEcxgREV3DGNyUYwxsgoKC4Obmxi/JJkKSJBQWFiI1NRUAEBoa6uAWERGRozC4MaPT6eTAxt/f39HNoRpydXUFAKSmpiIoKIhdVERE1ygWFJsx1ti4ubk5uCVUW8bPjvVSRETXLgY3NrArquniZ0dERAxuiIiIqFlhcNNMDB06FLNmzXJ0M4iIiByOwQ0RERE1Kwxu7ESSJJTq9Cgp1Tm6KURERNc0Bjd2UqrTIy45F2dS8x3dFGRlZeH++++Hr68v3NzcMGbMGJw9e1bef/nyZdx8883w9fWFu7s7OnfujDVr1sjn3nvvvQgMDISrqyuio6Px1VdfOepWiIiIaozz3FRBkiQUVSMbU6bTo9hwXH5JKZR2GLXjqlbVavTPlClTcPbsWaxevRpeXl54/vnnMXbsWJw8eRJqtRozZsyAVqvF9u3b4e7ujpMnT8LDwwMA8Morr+DkyZP4+++/ERAQgHPnzqGoqKjO90JERNRQGNxUoahUh06vrnPIe598bRTcnGv2ERmDmp07d6J///4AgOXLlyMiIgK//fYb7rjjDsTHx+O2225D165dAQCtW7eWz4+Pj0fPnj3Ru3dvAECrVq3sczNEREQNhN1SzUxcXBycnJzQt29feZu/vz/at2+PuLg4AMDMmTPxxhtvYMCAAZg7dy6OHj0qH/vYY49hxYoV6NGjB5577jns2rWrwe+BiIioLpi5qYKrWoWTr42q1rFxybnQ6SW0DfKAi7ruU/+71uIakiRVuN3YxTV16lSMGjUKf/31F9avX4/58+fjgw8+wBNPPIExY8bg8uXL+Ouvv7Bx40YMHz4cM2bMwPvvv1+neyEiImoozNxUQaFQwM3ZqVo/7s5OcFGr4KJWVfucyn5qU2/TqVMnlJWVYc+ePfK2jIwMnDlzBh07dpS3RUREYNq0afj111/x9NNP4/PPP5f3BQYGYsqUKfjuu++wcOFCLF26tG6/RCIiogbEzI0dKZUKQAfo9bazJw0hOjoat956Kx5++GF89tln8PT0xAsvvIAWLVrg1ltvBQDMmjULY8aMQbt27ZCVlYXNmzfLgc+rr76KmJgYdO7cGSUlJfjzzz8tgiIiIqLGjpkbOzKOkNI5LrYBAHz11VeIiYnBTTfdhNjYWEiShDVr1kCtVov26XSYMWMGOnbsiNGjR6N9+/ZYvHgxAMDZ2Rlz5sxBt27dMHjwYKhUKqxYscKRt0NERFQjCqmiIo1mKjc3F97e3sjJyYGXl5fFvuLiYly8eBFRUVFwcXGp8bUvpOUjv6QMEX5u8HVztleTqQbq+hkSEVHjVNn3d3nM3NiRMXPjyG4pIiKiax2DGztSKQ3BzbWVDCMiImpUGNzYkSG2cXjNDRER0bWMwY0dKZXsliIiInI0Bjd2JNfcsFuKiIjIYRjc2JGpoNjBDSEiIrqGMbixI5Xht8nMDRERkeMwuLEj0yR+DG6IiIgchcGNHbHmhoiIyPEY3NiRabSUgxtCRER0DWNwY0cqwzw3zNwQERE5DoMbO+LyC5ZKS0sd3QQiIroGMbixI6XZ8guOWI907dq1GDhwIHx8fODv74+bbroJ58+fl/cnJibi7rvvhp+fH9zd3dG7d2/s2bNH3r969Wr07t0bLi4uCAgIwIQJE+R9CoUCv/32m8X7+fj44OuvvwYAXLp0CQqFAj/++COGDh0KFxcXfPfdd8jIyMA999yD8PBwuLm5oWvXrvjhhx8srqPX6/HOO++gbdu20Gg0aNmyJd58800AwLBhw/D4449bHJ+RkQGNRoPNmzfb49dGRETNDIObqkgSoC2o1o+ytBCK0kKgtBBSSfXOqfSnhgFSQUEBZs+ejX379mHTpk1QKpUYP3489Ho98vPzMWTIECQlJWH16tU4cuQInnvuOegNBUJ//fUXJkyYgBtvvBGHDh3Cpk2b0Lt37xr/up5//nnMnDkTcXFxGDVqFIqLixETE4M///wTx48fxyOPPIL77rvPIqiaM2cO3nnnHbzyyis4efIkvv/+ewQHBwMApk6diu+//x4lJSXy8cuXL0dYWBiuv/76GrePiIiaP4XkiBSDA1W2ZHpxcTEuXryIqKgouLi4iI3aAuCtMAe0FMCLSYCze61PT0tLQ1BQEI4dO4Zdu3bhmWeewaVLl+Dn52d1bP/+/dG6dWt89913Nq+lUCiwatUqjBs3Tt7m4+ODhQsXYsqUKbh06RKioqKwcOFCPPnkk5W268Ybb0THjh3x/vvvIy8vD4GBgVi0aBGmTp1qdWxJSQnCwsKwZMkS3HnnnQCAnj17Yty4cZg7d67V8TY/QyIiavIq+/4uj5mbZuT8+fOYOHEiWrduDS8vL0RFRQEA4uPjcfjwYfTs2dNmYAMAhw8fxvDhw+vchvLZHp1OhzfffBPdunWDv78/PDw8sH79esTHxwMA4uLiUFJSUuF7azQaTJo0CcuWLZPbeeTIEUyZMqXObSUioubJydENaPTUbiKDUk0nk3Oh00uIDvKAi1pV9/eugZtvvhkRERH4/PPPERYWBr1ejy5dukCr1cLV1bXSc6var1AorOqIbBUMu7tbZpo++OADfPjhh1i4cCG6du0Kd3d3zJo1C1qttlrvC4iuqR49eiAxMRHLli3D8OHDERkZWeV5RER0bWLmpioKhegaqu6P2h2S2g2SugbnVPRjGH1VHRkZGYiLi8PLL7+M4cOHo2PHjsjKypL3d+vWDYcPH0ZmZqbN87t164ZNmzZVeP3AwEAkJyfLr8+ePYvCwsIq27Vjxw7ceuutmDRpErp3747WrVvj7Nmz8v7o6Gi4urpW+t5du3ZF79698fnnn+P777/Hgw8+WOX7EhHRtYvBjZ0ZBkxBQsOWMvn6+sLf3x9Lly7FuXPnsHnzZsyePVvef8899yAkJATjxo3Dzp07ceHCBfzyyy/4999/AQBz587FDz/8gLlz5yIuLg7Hjh3Du+++K58/bNgwLFq0CAcPHsT+/fsxbdo0qNXqKtvVtm1bbNiwAbt27UJcXBweffRRpKSkyPtdXFzw/PPP47nnnsO3336L8+fPY/fu3fjyyy8trjN16lS8/fbb0Ol0GD9+fF1/XURE1IwxuLEzhSHb0tBl2kqlEitWrMCBAwfQpUsXPPXUU3jvvffk/c7Ozli/fj2CgoIwduxYdO3aFW+//TZUKtF1NnToUPz0009YvXo1evTogWHDhlmMaPrggw8QERGBwYMHY+LEiXjmmWfg5lZ1t9krr7yCXr16YdSoURg6dKgcYJU/5umnn8arr76Kjh074q677kJqaqrFMffccw+cnJwwceJEFgoTEVGlOFrKjD1G2py5mofiUh1aB7jDw6XqzAZVT0JCAlq1aoV9+/ahV69eFR7H0VJERM1TTUZLsaDYzoxVMpyk2D5KS0uRnJyMF154Af369as0sCEiIgLYLWV3creUg9vRXOzcuRORkZE4cOAAPv30U0c3h4iImgBmbuzMOMDpGuvtqzdDhw7l75KIiGqEmRs7kxfP5PcxERGRQzC4saEumQJjzU1DDwUngVkeIiJicGPGOG9LdSanq4ipW8oeLaKaMn521ZmDh4iImifW3JhRqVTw8fGR51hxc3OTC4SrS1+qhVRWCm2JEsVOjHAaiiRJKCwsRGpqKnx8fOT5e4iI6NrD4KackJAQALCaRK66sgq1KCjRocTVCTmc56bB+fj4yJ8hERFdmxjclKNQKBAaGoqgoCCbC0NW5feNZ/DHkVRM6heJBwZE1UMLqSJqtZoZGyIicmxws337drz33ns4cOAAkpOTsWrVKqup+cvbtm0bZs+ejRMnTiAsLAzPPfccpk2bZve2qVSqWn1RaiUnXMnTIUer4Ay5REREDuDQguKCggJ0794dixYtqtbxFy9exNixYzFo0CAcOnQIL774ImbOnIlffvmlnltafc5O4ldaqtM7uCVERETXJodmbsaMGYMxY8ZU+/hPP/0ULVu2xMKFCwEAHTt2xP79+/H+++/jtttuq6dW1owxuNGWMbghIiJyhCY1FPzff//FyJEjLbaNGjUK+/fvr7A+pqSkBLm5uRY/9UnD4IaIiMihmlRwk5KSguDgYIttwcHBKCsrQ3p6us1z5s+fD29vb/knIiKiXtvorDIEN+yWIiIicogmFdwAsJp3xjgjbUXz0cyZMwc5OTnyT0JCQr22T60S7WDmhoiIyDGa1FDwkJAQpKSkWGxLTU2Fk5MT/P39bZ6j0Wig0WgaonkAAGcnMcKqhMENERGRQzSpzE1sbCw2bNhgsW39+vXo3bt3o5lun6OliIiIHMuhwU1+fj4OHz6Mw4cPAxBDvQ8fPoz4+HgAokvp/vvvl4+fNm0aLl++jNmzZyMuLg7Lli3Dl19+iWeeecYRzbeJo6WIiIgcy6HdUvv378f1118vv549ezYAYPLkyfj666+RnJwsBzoAEBUVhTVr1uCpp57CJ598grCwMHz88ceNZhg4wIJiIiIiR3NocDN06FC5INiWr7/+2mrbkCFDcPDgwXpsVd1wKDgREV2zyrTAsZ+AwPZAWC9A6ZgOoiZVc9MUqFUMboiIqIk4uxH4pC/w1zNAbpLlvn8/AT7oCHw5Cjj2swhcdi0CLv0DSBJwag2wewlQkg+kHBPbMi8Av08Hvr0VqGAUc0NoUqOlmgK55obdUkREzU/GeWD9K8D1LwIhXao+PvEAkJMAKJRAQDvg8k7gn4XATR8C0TcAqaeAv2YDwZ2BzuOBnCtAtzsszz+3EXD1ATrcCDi5AHkplu+dmySCk8j+QMxkQK8DXLyBwgzAPQAozgU0ntbBRsZ54OcHgJJcIO0UcPJ3QO0CuAcBI18Htr0LFGcDeUlAwm7Tee6BQKuBwIlV4vWm14HSAmD4q4B/W7EtoB2Dm+aEBcVERFW4uEN8oXa4sXrH68pEUFCYDvhHA8FdKu/ukCRgwyuAmz8w8Cnbx+h1IuAw/wK+chDY9BowdI4INn6ZCngGAwNmiWN9I4H/jQeyLwMpR4GnjpuutXuJCBD6zwRyE0UbcxKAL4YDMJRfqDSAUgWUFgLLbxPBwI4PAW2euL+9n4tj3XxFl84fTwJxq03t2/wGoHQCijKBcZ8C7UYBW98Gjv8ifjen/wK2vAXoy4CgDkDyEaDfdNG2YS8D/Z8AfrhHXOPu5cCqaeJzAICA9kD6afE8Ox74ymxppKFzgG3vAJLhe60gTQQ2ChWgchaBDQD885EIrgDRLeVADG7sjAXFRNQolRaJL31nt7pdpyAD0OaLL/rqKCsBnMzmGstPA765STyfHQd4hYngIP5fEVC4+hreJx3QFoj3+XcRsHGu6RrdJwLjl5heH/4BOLoC6POICJiSjwC7/iv2tRsjvujNpZ8Flo0GVGqRgQjvA1w3FdjyJnBhi/iJeQA487c4/sDX4kt86kYR2AAicNEb/j//y0OmLMah/xneRAE5qPFpCRTlACU5gM6sHZteK/fLMhz/2wwRVCUfEYFIh5tEluXqMdOhq58Q2ZukQ5aXMAYayUfE4+7F4nHz6yKwOr9JvP7mFiBxL+DsAczYIzI9/3wIeIcDcX8A5zeL4/o/AQx9AQi/DljzjOh2Mmo9BLhhHnB6LbD3M5Ep2vWx2BfQDo7Emhs7Y+aGiKqUdkbUL9RGabGoddDZXk/PppwrwKLrgA87AQn7LPeVlQAbXgU+6iHqJLIuib/cjUrygb+fBw4tF1/mX40BFvcDchKrft9t7wJvhgBxf5q2HfrW9DztlLiPnx8Evr4RWNAZ2P+VCHaWjQY+6gasewk4azm/GU78CmgLgSMrgC9uAH6bBlzYCqyYCOz+FEg0u8fFfYGlQ4FfHxWBla4U+PURkenISxbFr38/C/wxE7i003Tega8s31OnFeeZe6cV8O0thiyGEvCNEts9giEHKgBw13KgywTTaydXoJdhmpM2w4BnLwCBHQGNl9iWnyKCE7cA4OHNwJ3fAA+tA7rdJQKdDjcB+lJTYDPyDZHJMfJvC3iFW38eG+eZnsfvEo/DXhYBjcZTZJJ6Pwjc9qUIyJROQI9J4ri2w4GZh4CRb5qu0eEmILQ7MPR54Ib/WL4XMzfNC0dLETVz6ecA7xYim+DkCvR/vGbnn14L/HCX+KKasLT650mS+GJe+4L44u03HRg9X2z/9xNA7Qpc95A49uoJ8dd3x5uB3x4z/RUPAP8bBzy0XmRJ9DrR9WLs+si6CHzUXXxRD3oaaDVIZFT2GL44931h6ro4/TfQ52FRz7HtHdHVknUJCOkK9Josgoat88WxG14B2o0WWY99y0xtST0lghnj+5cWiPqTgjQg46zY9u8i0/HT9wDLbxdZkz+eBI79aNihAFoPFRmXrfOBlv0sf3dJh8RPYTrQIgZIOgio3URQkHtFZCzkjAuAwA4i8AruCkz6RbT7m1vENnMlOcClHeJ5aHfggbUisxHUUWSBtr8n2hXaTbyvMWAK6wnc8l9g8HOAVwvRxTb9X/FZbn8P2P+lCFDGvCN+nwDg7G7696ItBL4eK+6pz6Miu6LXiQBP4wGMfV90ty0bLT4/cyHdgMyLoius5ySR7SrPzQ94ZBtQmAkEtLXc18Y0fYtFt2KHG0U2yRjUBTg2uFFIlY3FboZyc3Ph7e2NnJwceHl52f36KTnF6Dd/E1RKBc6/Ndbu1yeieqTXAwe/AcJ6iC8gAMhNFqNDnN2BoiwxEsS/LZBxTuwf/xnQ9U5TDYheJwozw3oCflHiCzz/qkjhA+JL8uI28Xz6HsAjSGRPvEKBU38B614EPEPFF294b3GcJAE/3m9Zf6FyFn9J71oE7DF00czYByQfBn592HCMBtCViOdu/qKrwPhlp3IG2t4AnF4jnvd7DDjwjSggrY62I4BJPwPb3gO2vFH18a0GAfG7RcahPJUzcOf/gJO/AUd+sH2+iw/w3EUR/JhnVa57WASY3hEio5R+xuwcb5FN6XgTsPUdy/e+7Uug6+3i+c6PRPYKACIHAJP/AC7vAoI6Ae6GpX3+XQysm1Px/cU+Dowyy2pIEpCwR2QwXH2B1DjRPgDoO00ELnVRlC0KjTveAjg52z4mYa8pW3PZkJW6f7UIhLMuAV3vqF3R786Pxe/WWF9j9KGhzggAXskAVPbNn9Tk+5uZGzszdkvp9BJ0egkqpeOqxYmapOJcUR/iGVz1sYkHxF+6I18HAqIt95VpgSPfA+c2ifS6xgPo+xgQ3Ml0zI4PgOOrgIkrRGp+3xeii8I9EHjqpOhu+ONJoKzI8trGwAYAVj0qRs88tE58Aa97UXxBRw4Abl8GLBsJFOcA96wAokeKIbNGfzwpsiySDnhoA7DnM/Glk3VJZGgkvWiLb5RlYAOIbpKfHhB1E0YHv7E8zhjY9H4IGPysqH1ZOkR0O+m0IrABxBdt7wdF8W1JHnDwfyJ7UJgh9qs0QPQI4JRZ99KlHSKDcPJ38dq3lbjnoytFQWtIV9E9E36dyKYYMxxthotakZ0fma5162Kg/WggahCQdBhIixPbb/iPqdbGv60IIKNHmoKb7hOBse+ZvqCvfxH4aYrpurOOAy6GL0GvcPH71pWIrJkxsAFEFmz7ByIT03m8qE2JGmT5++73mOgu2vOZ+PekzbfcH9nf8rVCYZlBCmgHOHuKjIkxG1MXrj6W92BLRB/ggTWiUPqL4SKYNQbZEX1q/94DZtre3maY+DcI2D2wqSlmbuwsv6QMXeauAwDEvTYars4qu78HUaNQkC66QzrdKjIdAHB+C3DlADBwtimTUZwj/qde0eiWoiyxf8Mr4kv8yA9A2mng8X2i+6e8Ta+LkSo9JwFr54huBf9ocbz5X6ErJ4muGXORA8T/7POuAlf2ixoNQPwlff2LwMKuor2A+Mt+3Ysi6xLcVYyAKcoClGpTBsA7wvSXKgCLIlJAdAsZ2+ARAoxbDHxnVnthzq+1CGqkKrq0lU7i97v9XdM245emkcYbaNFTdFMYAzXjX/c5iWK00ra3xfuFdgce3iK+0Ms7+Tuw5lng+peAFr2ATweK7e6Bouuo532iO0ehAp49J7ozrp4EyorF8UaXd4kC1fA+IkiK/9c0GkftBjx/yVR0nHZG1NE4uwEzDwNvGoJcYzdcaTHww92iEPnmj0RRsLkt88W9hV8nCoDNaQtEMXFwF+sv3+wEEezFPFBxJgQQmbnlt5sKbo2euyjuvzLrXgJOrhbtqk7wbk85iaKGR+1Sf+9RlAWsninqizqPt/vla/L9zeDGzrRlerR7WVTYH5k7Et6ujWNBTyK7yk0C/ttb1EhEDQEmG7IF87zF47hPgR73AIn7Rb9/+zHAnd9ap8BPrwVW3COCDuNf9kY3LhA1JHkpoi+/8wQx9PXdqIrbNehpURR59QSwpL/40h06R/wPff0rACRR67B2juVf3u5BQMwUy4AhqBOQelI8fyFBHH/sZ1E7svl10WXUf6YoXv1yhOk8Vz8xVFemEHUVuYmGxysiIOz7GPD7DPElnX5GBFGAyP6o3cTcIkbeEeJn0s+m4bcf9zCN3LlnpRjJYgy0rnsY6P0A8ONkUY9RvvsAEJ/hrkUiY1O+rqIicX+KYCIvWWRBjMz/DVRHfirwviHT1noocP/v5fanibofd3+RyTn0HTD8FdEVUh2JBwCfCNHlVx8OfSc+u4D2IojyChWFuVSv2C3lQGqV6X/eLCqmRinrspgLo0Uvkf1wdrd9nLZA/NXeZphl+ltXKr40jUNOjfUjJWbBwpX9IrjZsUBkOeJWA/+NEV+0kl70/495VxTWAtaBDSCKPvEQsH8ZcHa9+Ol0a+X3tuMDMaR3t6EGpePNwJBnxfMz68T7rH7CdLzaXdxHQaopsBn5JrD+JVNg4xEsujZcvEzp+LvMik8j+oh6i71Lxbl9HwE+Hy5+B4CovekxUQQfuVfEti63AZGxwBMHxOvdS0z1HC37id+xMbipKCPQc5IoWnULEN0Nk34V95ZxFuj7qOime2J/xb8rrzBg9FuV/z7L62gYwi1JQPwe0e3n7AHEzqjZddwDTc8DO1jv9zDbH9bDlBmsrvCYmh1fUz3uFUFoy35ikjxqdBjc2JlCoYCzkxLaMj3nuqGGcXmXyFTETLFO0ZdXkCFS+qknxbwgl3eJYaZGCfuAfZ+L+glJAg4vFz+tBok0eplWfIEm7hXdI/oyw3XTLYcGp58VmYEza03bMs8Df84yvT7+S+VtvbhDXOf036ZtxvqO6FHAWdH9i56TRNGvcf6OnR+brt3vMdO53e8xBVHd7wFGvy26YvZ8KiZHA8R993tMBBu5hvvxr0ZWY9SbouvGOIdMSBdTcNN6qAhm1s4RgZ5XONDeMMrEmMnqOckU3AR1FHUp5zeJ2pCKujr6PCw+x443iy6WwHai7kevr//1fBQK4NZPgGEvieJnW11aVZ3fa7IIOAfMqpcm1iuFwhToUaPE4KYeOKsMwQ0zN2QP2QliyGfHm627dfQ6YOV9YojrtnfFbKPeEcCot4B2Iy2PtTWq5coBs2vpxbBh4xBccx+0E3NauPiIoEihBO74WtQQZF8Wk5GVmhXdXjkAHP9VFMpG9BPDRC/9YwpIKuPiI0bsZF8GFvW2fUxkf+D6OaKbZPAzYvTHqTWii8s4csgjBIjoazqn+92ia8m/jch0GA16RrzngW+Aka+JL+qwHmbBTZuq2wxYTo4XbDY1fushIkDpdIsIuvo+al3v4eIF3LJIBJJ9HxOB5MzDInCoiKuv+AzKa6iFCpVKUYRdW7d8bL+2EJXD4KYeODspgRJ2S5EdaAuAhYYvygfWii/aP58S3To9J4mAozBd7C9IFY8ZZ4Hv7wA6jRN1F0mHRAGtsfsIEH91/z5DZFtKi01zp1TGfKTMnf8Tf7ke+k4EIQe/sTxWm28atRM1SHTnDJgpCo7zUkTQs3uJCNj0OtEl5OwJPHNa1JR8MRy4etx0vcCOosbFOBeJcai2cbg2IOpxPELEiBZAdKeZB4NKlQgsylMoRBakz8OmbaHdTfdbncxNeeazsxoDrBsXAF1uFzU7tvS6T/wY+VVSW0RElWJwUw+cuTL4tSk/TYwAUbsCQ54TX9zmSvJFLUfXO8SU70mHRaYlarDYn5MI/Dlb1KUYh6Fufdt0fupJEdQYv3T3fm6a5Ta0h+iWatlPZFPObxJzhlzcXq64FcCsYyK7s/ZFMfQ1Lc4ysBn0jBgOvPczMZdJ7wcNa9AYRhH5tzVN3hXY3rLryVzCHvHo28q0zXwCsJ6GmU8zzotAp9sdpvqfMe+KkSvuAWLtnUGzRZGtMbgJ7W79fkqVqA0yTvrWdrjtdlWH+fVrE9xEDRbFzQHtxL8HwLDwIee+ImoIDG7qAVcGb+J2fiQCDWNNRkUu7RRBxJAXxNDRM2tNXTo/TRETtPm2MnVBHPlBrFFz5SDwwN9ivhFABBs+LUW30tl14ufVTAAKUUxrVJhhWXhbViwCEEAERMai3zu+Aja/KfaVD2w0XiKwUSgA/9Yiq2Ne09LlNjEhmtpdzMXR9gYxEuTen8R8LYAYtWTMiPjayC74R1t2bZkHN7b4twFeuCy6uoxaDRA/gGHRQoUI5DreIrpCjOsPldf9blNw03po5e9bGfPgpqr226JQiFFbROQQDG7qgXEJhpJSXRVHUqOTlwJsmAtAEsOX2wyr+NivDX+FK9Wi/iPpoGmfvgxYFCNGsjy2E/AMEZkaQHS3mM/CGr9HBDfGYb2AmDwutLvlcOWsy+JYwFRwCoglAMyzFC7ewNh3ReBiPsEbIIY3GwMTvzbiGOM8LMZJ54zMu0gi+oi5XtLPAN3uNG1vMwxwchE1JsYC2s7jxMR6RtUJDioLIo3tdXK2HKVkS0hXMfeJ2r1uo1g8Q8SaOsXZtkfzEFGjxoUz64Gni4gZc4vLHNwSsin9HHDWMLlX8hExisVYWHvqT8iTsB3/1fK8IyvFcObfplsuemisLbliCG6GvGDKQhSmAyd+E/NuGIMfSS+GBRsZA5DMi6Ztvz5smmDO/H10JWJosvkMoWPftZ3JKL++DmA5O6+xu8U45Dmok/XxRgqFmMdkxh7LmYB9I8UEcVP+BG7/CrjpQ7EKs5HKufKi2PoQM0V0cdXVuE+Au5fXfCQQETkcMzf1wDhxX25RDVbtJfuSJDEUufwMt/lpwJJYMfV8/5nArv8CkET9yojXgDNmXTRxf4giUCdnMWR6lWGBuYxzlpmS9LNiKPTVE+J1j3tE8evfz4lF9NY+X3lbE/eJJQeMmZuIfkDCbtMaOR7BYoK3klzxutUg8dPnEbGv5322r9syFthVbkSKR4jpeflRQMGVBDeAGPFja1iyce0d46rH5vPdOHswOCCiBsfMTT3wMgQ3OQxuHGfLW8CHnUTWBBCLzP36KPB+WxHYAIYvfgnwiRTzj6ybIwpwATFypzgbiN8lXp/bZHn9tS+anutLgZ0LxaOrn7he9AiRyahMpGEq+5RjpqyOZ6iYqyT8OtNx0SMszwu/TgQMY98Tw6ArWvguMlYMcfaJFI+AZVAWWG7V3soyNzWh8TA9LyuxzzWJiGqAwU09kDM3xQxuHMY42+zqJ0QX0v/Gi/lZbHlwrRido1SL+pV+00W9DQBcNqygbOw6ijYU1RqHGxvtNwQyLXqZgo2QbtbvpXYT2YzwPmIqfY8QUZ+zZ6nYH9xZPHY1q2uJHmV5jZAuqBZXX1HvM3Uj8Ngu4MF1plWmje2T30chJo+zNyWTw0TU8Bjc1ANvZm7qRpIsZ7tNPipmoL2wFTj6Y82uVZIr5ndJOihGCo16S6xxZBTaXUxD3/dR4JkzwHMXxOJ8LQ1zkyTsBnRlomYGEOvHeLc0nW8sODYW/rYaaNqnVALXvyyyMTd/LIZr3/U/8T4PrhVDhDvdIo49/Zd4NE7+ZuzicfawHvVjDICqwztcrK/j3cK6BkehAMZ/Box+RyzoWN11e6rj5o9EYDN+if2uSURUTfyzqh54uTC4qZPt74k1c27+SEzStnSoWDE5NwkoLRSjbyL6iGPz08T6RJ3Hi+HQtmScE4+th4o1cCRJBCg58ZYTqpnXk7SMFY8J+8QK1KUFIjgK7grcvNC0snPvhyxXB25tNo8LINY1Mq5tZGvxwj6PiDWJjIzHuAcAM/aKwmSXcgvEVTQMujaUSqDfNPtdzyhmCtDt7vpdgZiIqAIMbuoBC4rroLQY2L1YPN/6jph1VtKbAhRArMDs1xq4Z4UY0XTlgPjRFgJOGutF/Iwjofxai0eFAhj8tJhzptf9ttsR2BHQeIuJ6w58Lba1iBHBQNvhIgOUeQFoP1aMOso4J+ptbHVFVSYgWgRYZ9aKgmZjGwHrmhhAzN7bVDCwISIHYXBTD7xcxa+VmZtaOPkbUJQlnuclVbwkQOYFUediXDkZALYaVjg2L2gFxJpDgGXgEDNF/FREqQQirgPObTQVJZvXpJgHUC1jRXDTekjt1vUZ/5nIDrUaVPWx1V3niIjoGsaam3rA0VLVkHcVWH6HmB037g8x22+ZFtixQOw3X3gQZqOBjKN+AFFLYyz4NffX05avSwvEo3lwUx3+hvlcjMsO+LS0fdzAp8RSC0OqGPJdEVcfMV1/RaOeAOCW/4ph3xOWVnwMEREBYOamXphGS3ESP1nGeaAgzVTU+vt0kRU5u950zLCXgfTTgHsgMPkPUcty8FsxI+6+L8W6Q7d8LGbp/e0x0+RzRh4hYph3+SUHjGqa9Si/cGFFwY1/G+Cu72p27ZrqdX/FXWhERGSBwU09YEExxNwt/tGmuosf7hGT0j28SdSunNtofc7uT8XjiNdEcW/X203rJRkXWQREBmbf56ZamsgBYii3qy+w7kXRtQWYJr8zMp/ArjrKr5tUUXBDRESNCrul6oG3mwhutGV6FF+L60ud/B34dCCw/mXxuiRfZGQgiYUg89Nsn1eYLh4j+1f9Ht3uEo9uAUDs42LuF+8WplFUgCj2NVfTepjyayJ5R9TsfCIicggGN/XAw9kJSkP5xDU5Ysq4EOPJ3wC9Hsg8b9p3/FdTZqUi7kFVv0efR4AnjwJPnwY6mAUx4WbBjUcwMGq+eN5jEmrMNxJyvY+Lt6iNISKiRo/dUvVAqVTA00WNnKJS5BSVIsirGQ6JzbwIeLUAkg8DKrWYjyY7XhQGG5cqKEgTo4DMg5vSQmDjvIqv6+wJOLtV/f4KhSH4KCfUbCh2aQEw9AWxrSYT3xk5acQ95iayS4qIqAlhcFNPvF1FcNMsl2BI2Ad8eYOoqck4Kya3u/dn4Ie7rYt5z28C9IauObWbCG6Ms/kGdwGuHrc83qMaWZvKOGlEbU1+ipi0T6GwnDW4pnxbGYIbG4EUERE1SuyWqifNbq6b1DgxTLu0SEw4B4jABhBLHNgKbABgx4ditmEAuO4hQOUsnod0s5wd2KiuwQ0APLoNmPQr0GZ41cdWxa+VeGS9DRFRk8HMTT1pdutLLTYM4XZyMY1SMmcMbNqPBU6vAYa+CJz6U3RLGYX3AQqzgMPfiWHNahvdT/YIbjxDxI89xDwI5FwBet5rn+sREVG9Y+amnnhoRNyYX9KERkulnQH+NwFI2Av89YxpZl6d2Xw9aadMwc2UNZarV2u8gbuWA9N3A4OfAR7ZBnS9w7Tfvy0w9j3g/tXAdVMt62OMqlNM3JDCY4D7fwNCujq6JUREVE3M3NQTd0NwU1DShCby+2wwUFYk6mQA4MQqoNOtQFqc6ZjibNENpXYDIvoCCXtM+4I7i+HW5ssUjHxTTManVIvJ7pw0YpkCQAQMD6wFEvcBG14R2zyC6/UWiYio+WNwU0/cncWvtrCpBDe6UhHYmCtMB9LPiuDD6PwW8RjWC1A5WS7uGNIFVjyDgRmG85001vsjY4GyYtNrj8DatZ+IiMiAwU09cW+M3VLFOcCm14Fe94kuIqUaiFsNJB2qeETR5Z1AolmNTUmueDQGNQFmwU2wjeAGANz9K2+Xu1lAw8wNERHVEYObeuLurAIAFGobUebm38Vi2YJ9nwOufmLulvQzYnj2v4tsn3N5l1hKoTz3APHo20qMgNJpbWduqsMiuGlkNTdERNTkMLipJ3LNjbYRZW4yL5ieF2VWvMCkuQtbTcsimHMzZGNUTsDwuWJYeGjP2rXLzR9QqABJB3iG1u4aREREBgxu6om7RmRuGlVBcWGG7e1d7wSiRwKtBgALOlruK0i1fY6bWVdT/8fr1i6VEzDmHaAwE/AKq9u1iIjomsfgpp40ytFS5pmb6JHA2fXi+YAnRZeSJAEqDaArMRykACCJp66+QFGW6Xy3KupoaqrPw/a9HhERXbMY3NQT42ipgsZSc6MrBXISxPPZcSJDsnsJUFZiWndJoRD1L7mJ4nXLWCB+l3jeapAoPjayd3BDRERkJ5zEr54YMzeF9T1aKucKcGSlaf2mCo9LAPRlYoZhD8Psvf0eAwbOEkGNkbFQGAA6jzM9jxpseT3z44iIiBoRBjf1xM0wWqpeMzfZ8cCi3sCqR4Djv1R+bOZF8egbJSbaq4h50NLhRlHoC1gHN8zcEBFRI8VuqXriIdfc1FPmJvkosGKiGMYNABe2iTlifFsBvjZWsDbW2/i1rvy6xmHZKg3g1QIY/6motTGfrA+wPSEfERFRI8Dgpp64aUyZG0mSoDDv+qmtqyfFCtstY4FdHwP5V037Dn8nfgBg4FNA/5mGhviJx3TDCt5+UZW/hzFz4x4ouqu63Vn58URERI0Mg5t6YszcSBJQVKqDm3Mdf9UnVgE/TRHPT/1peJNgsXjlohjLY//9BDj0nZhcb8ZeQONhWuwyrIq5aIyZG9bUEBFRE8Xgpp64OKmgUIjgpqCkDsHNnqUiW1Ocbb2v8wQgoK31dp0WKEgTz4+uBHreB6QcFa9b9Kr8/XwMXVq2uraIiIiaABYU1xOlUgE3dR0n8tv/FfD3s4bARgH0nQbctNC0v+vt4rHVIPEYfh3Q/kbLa+z5TCyfoNOKJRd8q+iW6nAjcOtisZo3ERFRE8TMTT1y1zihQKur3Yipfz8B1r0onsc+Dgx+FnD1AbQFwL4vxGilFobuqFs/AfYuBQbMEpma03+ZrpN+GjjyvXjeIsZy2LctKjXQ896at5eIiKiRYOamHslz3dR0faniXGDTa+L5wKeAkW+IwAYAnN2Bx3YCk1ebAhXfSGDUm4BHoJibxtkTiOgLhHQT+0+sEo8tytXm1NSYd8Xj7cvqdh0iIqJ65PDgZvHixYiKioKLiwtiYmKwY8eOSo9fvnw5unfvDjc3N4SGhuKBBx5ARkYFayY5mHF9qfyadkvF/QGUFQP+0WJRypqMtPIOB548DNy3ylQ3Y1xTqrardhv1fRR4IR7oclvdrkNERFSPHBrcrFy5ErNmzcJLL72EQ4cOYdCgQRgzZgzi4+NtHv/PP//g/vvvx0MPPYQTJ07gp59+wr59+zB16tQGbnn1GIuIazxL8dEV4rHbXTULbIzcA0SGx6dcUXBAu5pfqzwX77pfg4iIqB45NLhZsGABHnroIUydOhUdO3bEwoULERERgSVLltg8fvfu3WjVqhVmzpyJqKgoDBw4EI8++ij279/fwC2vHo+aLJ5ZWgT8NgP4oANwcTsABdDtjro1wLeV6blCVXUxMRERUTPgsOBGq9XiwIEDGDlypMX2kSNHYteuXTbP6d+/PxITE7FmzRpIkoSrV6/i559/xo033mjzeAAoKSlBbm6uxU9DqdESDMd/FZPw5SWL10NfsAxOasP8fN9WgJNz3a5HRETUBDgsuElPT4dOp0NwcLDF9uDgYKSkpNg8p3///li+fDnuuusuODs7IyQkBD4+Pvjvf/9b4fvMnz8f3t7e8k9ERIRd76MyHjUpKD6/2fT8+peAIc/XvQHm3VIB0XW/HhERURPg8ILi8ssSVLZUwcmTJzFz5ky8+uqrOHDgANauXYuLFy9i2rRpFV5/zpw5yMnJkX8SEhLs2v7K+HuITEliVmHlB+r1wIUt4vkDa4Ehz9Wu1qY8n5am5wxuiIjoGuGweW4CAgKgUqmssjSpqalW2Ryj+fPnY8CAAXj22WcBAN26dYO7uzsGDRqEN954A6GhoVbnaDQaaDSOWeSxc5govj2RVEVXWNIhMaLJ2RMI722/BqhdAM9Q0dXlz+CGiIiuDQ7L3Dg7OyMmJgYbNmyw2L5hwwb079/f5jmFhYVQKi2brFKJuhZJkuqnoXXQxRDcnErOQ6lOb/ugnCumNaPaXC8m0bOnVgMBpRMQOcC+1yUiImqkHNotNXv2bHzxxRdYtmwZ4uLi8NRTTyE+Pl7uZpozZw7uv/9++fibb74Zv/76K5YsWYILFy5g586dmDlzJvr06YOwsDBH3UaFIvxc4eniBK1Oj7NX820ftO8LICce8GstJuKzt/FLgWfP2V6DioiIqBly6PILd911FzIyMvDaa68hOTkZXbp0wZo1axAZKQphk5OTLea8mTJlCvLy8rBo0SI8/fTT8PHxwbBhw/DOO+846hYqpVAo0DnMC7svZOJ4Ug46hXlZH5RjqAGKecCyRsZelErA1df+1yUiImqkFFJj7M+pR7m5ufD29kZOTg68vGwEG3b2xp8n8cU/FzE5NhL/udVshuCCDCD9jFjx+9IOYMLnQLc76709RERETVFNvr+5cGY9a+HrCgBIL9Ba7vjyBiDzgum1h+0iaiIiIqoZhw8Fb+5c1KLguaS0XEGxeWADAJ4hDdQiIiKi5o3BTT3TOIlfcUlZFRP5MXNDRERkF+yWqmcap3KZm8yLgKJcTKnScEFKIiIiO2FwU89c1CKQKS7TASX5wMc9rA/yDLbPjMRERETEbqn6ZpG5ST9t+yAP1tsQERHZC4ObembM3JSU6USXlC1KJtCIiIjshcFNPTNmbopL9WJeG1vKihqwRURERM0bg5t6ZpG5qSi40VcxkoqIiIiqjcFNPbPI3KTZCG6UamD02w3cKiIiouaLxR71TGPI3JSWlULKOAeLMVF+bYDHdgFqF4e0jYiIqDli5qaeuRgyN6FIg0JXYrnT2Z2BDRERkZ0xuKlnxsxNuCLdeqezewO3hoiIqPljcFPPjMsvhCHDeieDGyIiIrtjcFPPFAoFnJ2UCFXYCG7Ubg3fICIiomaOwU0DcHFSIlSRKV6YT9jn7OGYBhERETVjDG4agEatMmVuAjuYdjgzc0NERGRvDG4agIvarFsqsL1pB2tuiIiI7I7BTQPQOKlM3VKBHU071AxuiIiI7I3BTQPwUmnhoygQLwLbmXYwc0NERGR3DG4aQLhSZG1KnTwArxamHay5ISIisjsGNw0gxDDHTZFrCKDxNO3gaCkiIiK7Y3DTAPwVuQCAImd/y4CG89wQERHZHYObBuANUW9T7ORpmblR8NdPRERkb/x2bQBehmLiIqWnZeZG0juoRURERM0Xg5sG4CnlAwAKVR6A0uxXrnJ2UIuIiIiaL6eqD6G6cteL4CZfaeiS6j8TSD4CtLnega0iIiJqnhjcNAB3Q+amQGGY12bk6w5sDRERUfPGbqkG4KYTwU0uOPSbiIiovjG4aQCuujwAQB44IzEREVF9Y3DTADQ6Mc9NtsTghoiIqL4xuGkAmlKRucmROGkfERFRfWNwU990ZXDWiXluspi5ISIiqncMbupbcY78NFPn6sCGEBERXRsY3NS34mwAQJ7kivxSxzaFiIjoWsDgpr4ZgpscuCO3iNENERFRfWNwU9+KsgEAuZI7chjcEBER1TsGN/XNkLnJhRtyi8sgSRLKdHoUassc2y4iIqJmisFNfTNkbnIkd+j0EvJLynDzop3o9Oo6ZnKIiIjqAYOb+pawFwCQpAgCAGQXliIuWUzqt/tChsOaRURE1FwxuKlPpcXAqb8AAP+oBwCARbZGrVI4pFlERETNGYOb+nRuA6DNA7zCEe/WGQBwNbdY3q1UMLghIiKyNwY39en8FvHY6RZ4uWkAAEk5DG6IiIjqE4Ob+pQdLx6DOsLbVQ0ASMkpkneX6fWOaBUREVGzVqvgJiEhAYmJifLrvXv3YtasWVi6dKndGtYs5CSIR+9wObhJNsvclJQyuCEiIrK3WgU3EydOxJYtosslJSUFI0aMwN69e/Hiiy/itddes2sDmyxJAnIMAaB3S7PMjSm40eoY3BAREdlbrYKb48ePo0+fPgCAH3/8EV26dMGuXbvw/fff4+uvv7Zn+5quoixAmy+ee7dg5oaIiKiB1Cq4KS0thUYjCmQ3btyIW265BQDQoUMHJCcn2691TZkxa+MeCKhdzYIbU81NCTM3REREdler4KZz58749NNPsWPHDmzYsAGjR48GACQlJcHf39+uDWyy5HqbCPFgCG6KzbI12jIGN0RERPZWq+DmnXfewWeffYahQ4finnvuQffu3QEAq1evlrurrnlyvU24eDAEN+ZKynQN2SIiIqJrglNtTho6dCjS09ORm5sLX19fefsjjzwCNzc3uzWuSTMOA/dpCQDwdrMObpi5ISIisr9aZW6KiopQUlIiBzaXL1/GwoULcfr0aQQFBdm1gU1W9mXxaMjc+NoIbkoY3BAREdldrYKbW2+9Fd9++y0AIDs7G3379sUHH3yAcePGYcmSJTW61uLFixEVFQUXFxfExMRgx44dlR5fUlKCl156CZGRkdBoNGjTpg2WLVtWm9uoP7oy4NI/4nlINwCAn7vG6jBmboiIiOyvVsHNwYMHMWjQIADAzz//jODgYFy+fBnffvstPv7442pfZ+XKlZg1axZeeuklHDp0CIMGDcKYMWMQHx9f4Tl33nknNm3ahC+//BKnT5/GDz/8gA4dOtTmNupPwh4xFNzVF4joCwDwcVVDWW61BdbcEBER2V+tam4KCwvh6ekJAFi/fj0mTJgApVKJfv364fLly9W+zoIFC/DQQw9h6tSpAICFCxdi3bp1WLJkCebPn291/Nq1a7Ft2zZcuHABfn5+AIBWrVrV5hbq1+k14jF6FKASv2KlUgE/dw3S80vkw5i5ISIisr9aZW7atm2L3377DQkJCVi3bh1GjhwJAEhNTYWXl1e1rqHVanHgwAH5XKORI0di165dNs9ZvXo1evfujXfffRctWrRAu3bt8Mwzz6CoqMjm8YDoxsrNzbX4qXcXt4nHdqMsNgd4OFu2jcENERGR3dUquHn11VfxzDPPoFWrVujTpw9iY2MBiCxOz549q3WN9PR06HQ6BAcHW2wPDg5GSkqKzXMuXLiAf/75B8ePH8eqVauwcOFC/Pzzz5gxY0aF7zN//nx4e3vLPxEREdW8yzrIuyoe/dtabPYvF9wwc0NERGR/tQpubr/9dsTHx2P//v1Yt26dvH348OH48MMPa3QthcKyEEWSJKttRnq9HgqFAsuXL0efPn0wduxYLFiwAF9//XWF2Zs5c+YgJydH/klISKhR+2pMkoCiTPHczc9iV/miYgY3RERE9lermhsACAkJQUhICBITE6FQKNCiRYsaTeAXEBAAlUpllaVJTU21yuYYhYaGokWLFvD29pa3dezYEZIkITExEdHR0VbnaDQaeamIBlGSB+jLxHNXy+DG353dUkRERPWtVpkbvV6P1157Dd7e3oiMjETLli3h4+OD119/HXp99b6wnZ2dERMTgw0bNlhs37BhA/r372/znAEDBiApKQn5+fnytjNnzkCpVCI8PLw2t2J/xqyNkwvgbDmhYfmaG2ZuiIiI7K9Wwc1LL72ERYsW4e2338ahQ4dw8OBBvPXWW/jvf/+LV155pdrXmT17Nr744gssW7YMcXFxeOqppxAfH49p06YBEF1K999/v3z8xIkT4e/vjwceeAAnT57E9u3b8eyzz+LBBx+Eq6trbW7F/oqyxGO5rA0A+HtYZpA4FJyIiMj+atUt9c033+CLL76QVwMHgO7du6NFixaYPn063nzzzWpd56677kJGRgZee+01JCcno0uXLlizZg0iIyMBAMnJyRZz3nh4eGDDhg144okn0Lt3b/j7++POO+/EG2+8UZvbqB+FhsyNq6/VLnZLERER1b9aBTeZmZk2J87r0KEDMjMza3St6dOnY/r06Tb3ff311zbfo3xXVqNizNy4VZ25YbcUERGR/dWqW6p79+5YtGiR1fZFixahW7dudW5Uk1ZJ5qb8yuDM3BAREdlfrTI37777Lm688UZs3LgRsbGxUCgU2LVrFxISErBmzRp7t7FpqWAYOAAEeZWvuWFwQ0REZG+1ytwMGTIEZ86cwfjx45GdnY3MzExMmDABJ06cwFdffWXvNjYtlRQUe7mosfKRfnjntq4AAC0LiomIiOyu1vPchIWFWRUOHzlyBN98803jW6W7IRVWnLkBgL6t/RHmI0Z2aXXM3BAREdlbrTI3VImiimtujDRq8WsvLtVj6jf7kJRd8dpYREREVDMMbuxNLii2nbkBAI1KJT/fGJeK1/88Wd+tIiIiumYwuLG3SgqKjYyZG6OU3OL6bBEREdE1pUY1NxMmTKh0f3Z2dl3a0jwUVlxQbOSssgxuPDS1Ln0iIiKicmr0rWq+YGVF+82XS7jmaAuAkhzx3COowsOUSstVzxncEBER2U+NvlWv+WHeVclJFI8aL8DVp9qnleqk+mkPERHRNYg1N/aUkyAevSNqdFpuUWk9NIaIiOjaxODGnrKNwU14jU7LLWZwQ0REZC8MbuzJmLnxYeaGiIjIURjc2JOx5qaGmZscBjdERER2w+DGnrJrV3NToNWhjEsxEBER2QWDG3uqQUHx4nt7YWzXEPl1XnFZfbWKiIjomsLgxl50ZUBuknhejZqbsV1DsfjeGLg7i6UY2DVFRERkHwxu7CUvGZB0gFINeIRUfbyBl6saAEdMERER2QunxrUXjQdw04dAcS6grH7M6OWiRnJOMXKL2C1FRERkDwxu7MXVF+j9YI1P8zZkbtgtRUREZB/slnIwL1cRX7JbioiIyD4Y3DiYl4uh5oaZGyIiIrtgcONgxoLiPRczUVyqc3BriIiImj4GNw52fYcgAMDmU6l4f91pB7eGiIio6WNw42BD2gXi9XFdAAB7L2U6uDVERERNH4ObRqBvlB8A4GJaASRJcnBriIiImjYGN41ASz83KBRAXkkZ0vO1jm4OERFRk8bgphFwUavQwscVAHAxvcDBrSEiImraGNw0ElEB7gCAS3YObs5czcPKffHs7iIiomsGg5tGwhjcXLBzcPPyquN4/pdj2HORxcpERHRtYHDTSBiDm0+3ncfyPZftdt2recUAgITMQrtdk4iIqDFjcNNIdAjxkp+/8ttxJGUX2eW6+cViQc60/BK7XI+IiKixY3DTSPRr7Yd3b+8GANBLwPd74u1y3TxDcJOex1FYRER0bWBw00goFArc2TsCS+7tBQD4YW88ynT6Ol2zuFQHreEazNwQEdG1gsFNIzOiUzBc1SpkFGgRX8c6mfySMvl5mqH2hoiIqLljcNPIOKmUiPR3AwBczqhbcGPskgKAtDxmboiI6NrA4KYRauVvmPMmo27DwvPNghvOfExERNcKBjeNUGSAvTI3pfLznKJSlJTp6nQ9IiKipoDBTSNkr8xNrlnmBmD2hoiIrg0Mbhohe9XcmBcUA6y7ISKiawODm0bIOFtxQmZhnYaDm3dLAUA6gxsiIroGMLhphII9XaBxUqJMLyEhq/YzFeeV65biXDdERHQtYHDTCCmVCnQL9wYA/HU0qdbXYbcUERFdixjcNFIT+7YEACzfU/uZio3dUk5KBQAgnZkbIiK6BjC4aaTGdg2Fn7szknOK8fmOi/J2nV7CtP8dwCu/Hbd5XkFJGcZ8tAOv/n5c7paK8BMFyszcEBHRtYDBTSOlcVLhqRHtAADvrjuFradTAQDHruRg7YkU/G/3ZRSYdTsZszubTqUiLjkX3/57GTlFInNjLFBmcENERNcCBjeN2KS+LTGxb0tIEvDq7ydQXKrDxfR8eb9xHpxfDiSi09x12HjyqkXAczIpFwDQ2hjc2KFbSq+X6nwNIiKi+sTgphFTKBR4cWxHBHlqEJ9ZiK93XcLpFLPgJl3Mg/P0T0egLdNj6rf7kZxtGl2VUSAm7YsKFMFNXYeC/3boCrrOW4cdZ9PqdB0iIqL6xOCmkfPQOOG50R0AAEu3X8Ch+Cx5n60ZjG0NHW8d4AEAKNDqLDI7NTVr5WEUaHWY8tW+Wl+DiIiovjG4aQLG9QhDpL8bMgu02HMxU95+Md06uLlsI+CJ9HeDq1oFwD4jpnTsmiIiokaMwU0T4KRS4vHr21ptv5ReYDUL8cH4bIvX/dv4I8zHFYGeGgAsKiYiouaPwU0TcVuvcAxpF2ix7VJGARIyK5/B+OFBrQFADm441w0RETV3Dg9uFi9ejKioKLi4uCAmJgY7duyo1nk7d+6Ek5MTevToUb8NbCSUSgWW3h+Dp0e0w6eTegEQq3z/YWMGY42TErf1CsdtvcIxtL0IiAI9qpe50eklrD2ejNS8YjvfARERUcNwaHCzcuVKzJo1Cy+99BIOHTqEQYMGYcyYMYiPj6/0vJycHNx///0YPnx4A7W0cdA4qfDE8GiM7hKK2Nb+AIAlW89bHefv7owP7uyOD+7sDoVCzE5c3W6pzadSMe27g3jjzzg7t56IiKhhODS4WbBgAR566CFMnToVHTt2xMKFCxEREYElS5ZUet6jjz6KiRMnIjY2toFa2vgsvT9GDnAAYOrAKLxyUyd4apwwvlcLq+N93Z0BAFmFpVb7zKXkFBkembkhIqKmyWHBjVarxYEDBzBy5EiL7SNHjsSuXbsqPO+rr77C+fPnMXfu3PpuYqPm6aLGR/f0kF+3CnDHQwOjcGTuSDw7qoPV8d6uagBAdlHlwU2BVmd4rP2QcSIiIkdyctQbp6enQ6fTITg42GJ7cHAwUlJSbJ5z9uxZvPDCC9ixYwecnKrX9JKSEpSUmLpicnNza9/oRibI0wU/T4vF6iNJGN9TZGuUhkUyyzMGNzlVBDeFhnlwCg1BDhERUVPj8IJiY02IkSRJVtsAQKfTYeLEifjPf/6Ddu3aVfv68+fPh7e3t/wTERFR5zY3Jr1b+eG1W7vAXVN5sFfd4EbO3BiCnPT8ElzJrnxEFhERUWPisOAmICAAKpXKKkuTmppqlc0BgLy8POzfvx+PP/44nJyc4OTkhNdeew1HjhyBk5MTNm/ebPN95syZg5ycHPknISGhXu6nsfNxE8FNblWZG0N3VJFWB0mSMH7xToxcsK1OMxsTERE1JId1Szk7OyMmJgYbNmzA+PHj5e0bNmzArbfeanW8l5cXjh07ZrFt8eLF2Lx5M37++WdERUXZfB+NRgONRmPfxjdBcs1NobbS4wpKTDU3+SVl8jw6SczeEBFRE+Gw4AYAZs+ejfvuuw+9e/dGbGwsli5divj4eEybNg2AyLpcuXIF3377LZRKJbp06WJxflBQEFxcXKy2kzVjcJNbXFZh1x9gytzoJSAxy3oRTqPKrkFERORIDg1u7rrrLmRkZOC1115DcnIyunTpgjVr1iAyMhIAkJycXOWcN1Q9xuBGp5eQX1IGTxe1zePyzbqfzNepupprOTRcq9ND46Sqh5YSERHVjUODGwCYPn06pk+fbnPf119/Xem58+bNw7x58+zfqGbIRa2Cs5MS2jI9copKKwxuzEdJXcoolJ8nl5v3pljL4IaIiBonh4+WooZTnRFTBRVkbspP6ldYygJjIiJqnBjcXEN8jMGNYZbiT7acw4INZyyOMc/cXLbI3FgWFBdxHhwiImqkHN4tRQ3HmLm5b9lePDuqPd5bdxoAcEdMOCL83ACUz9xU3C1VVMrghoiIGidmbq4h5kXFb/99St4elyxmbZYkySJzYz55n1XNDYMbIiJqpBjcXEOMwU15ccl5AMQIqDK9ZPOY8quJF2n19m0cERGRnTC4uYZ4VRjciMxNYUn1szHsliIiosaKwc01JLfY9iiptSdSMH9NnMWkfVUp5KrhRETUSLGg+Bri7lzxx/3Z9gv4bPuFal+LNTdERNRYMbi5hsy4vi1S84pxX79WuJhRgBY+Lpi3+iTiMwuhUiqgs1FvE+ipQWaB1mofh4ITEVFjxeDmGhLi7YLP7usNABgYHQAACPV2xaX0AlzJLsIbf8VZndM+2BOnUnKRnm+5tlRRqamgOL+kDIXaMgR5utRj64mIiKqHNTfXuI6hXhjTNRQdQ71s7m8b5AF/d+tV1c0Liu/49F8MeXerPDlgbWw7k4Y/jybV+nwiIiIjBjcEAOgQ4mlze3SwB8J8rDMyxpqbMp0ep1JyUVSqw0Wz5RpqQpIkTP/uAGb+cAjZhdqqTyAiIqoEgxsCAPh7WGdnACA6yBOhPq5W242jpTILtJAM5Tjl58KpruJSPQq0OuglIKsO2R8iIiKAwQ1VITrIAy1sBDf5xSK4Scs3BTS1DW4KzIaVmy//QEREVBsMbkjWs6WP1TZfd2eLbqlQb/HcuByDeaFxbYMb88kDGdwQEVFdcbQUyT67LwZf/nMRt/UKx5t/xWFIu0AAQJi3KXMTFeCO5JxiJBlWCU83C2jS8i3Xn6ou88xNIYeYExFRHTG4IVmQpwvmjOkIAPjmwT7y9jAfy+Bm1/kMpOQUQ6eXkG6PbimzbE0+MzdERFRHDG6oSsFepm4pd40TVEoFSnUisKkquJEkCR9uOAMoFJg9op3N6xeYZWu4rAMREdUVa26oSs5Opn8m2YVahBiCnaTsIsuam3zr4OZEUi4+3nwOH286i6wC28O8C0vMC4rZLUVERHXD4IZqpFQnyQXGSdnFVpkbSbJcpuGXg4ny84wC291W+SUcLUVERPbD4IaqZd7NndDCxxUzh0fLNTj/3XwWO86my8cUl+qRV1KGS+kF0OsllOr0WH3YNOtwRr4WWQVa/Hk0CSVl5l1RZqOlWFBMRER1xJobqpYpA6IwZUAUAFOB8amUPKvj3vjzJH7cn4i3J3RFmV5ChllXVEaBFjO+P4hd5zPw9Ih2eHBgFB74eh/OXjVdh5kbIiKqKwY3VGNBntazGXtonJBfUoYf94tuqFd+Pw5vV2eLYzIKtNh1PgMAsHxPPLqGe2PvxUyLYwoaYUGxJElQKBSObgYREVUTu6WoxmLb+EOhME365+OmxohOwRbHGEdTtfBxxe0x4QCA5Owieb+LWoksG+tINbbMzboTKYh5YyO2nUlzdFOIiKiamLmhGusQ4oVDr4yAl4sacSm5UKuUcFIqsOrQFatjr2vli2AvkenZfSFD3l6o1SGzwHodqcY2id/W06nILNBix5k0eVJDIiJq3Ji5oVrxcXOGUqlA5zBvtAv2ROtAD9zXL9LquNaBHvBzF8HNwfhseXtqXgkSMgutjrfHJH7Hr+Rg6fbzKNXp63ytnKJSi0ciImr8mLkhu/nPLZ3x+LC2GPzuFpSUicCidaA7ynSSzeP3X8602lZoh3lubvrvPwDEhIP39rUOuGrCGNTkFjO4ISJqKpi5IbtRKhUI9nKxWEW8dYAH/D2cbR5/MinXaps9C4rP2BjNVVNycFPUuGqBiIioYgxuyO7cNCr5eVSAO/zcLYObQdEBAAC9jYROXQuKzScRdNPUPTHJbikioqaHwQ3ZXWmZKcBwdVbB3900dDzc17XSwty6TuJnXrPjplZVcmT15BSyW4qIqKlhcEN2F+rjYvHa110tP+8W7o3Wge4Vnqst09epENh8rStjZqj8khDVpddLyDMES7nM3BARNRkMbsju5t3cGTGRvlh6XwwAQONkyqB0C/dB6wCPSs+vS1Gx+VpXhaVlOJmUi56vb8Cyfy7W+Fp5xWUwxkV5JWXQ2+pHIyKiRofBDdldqwB3/PJYf4zsHCJvmzowCjGRvpjULxLhvq5Qqyqe8Te/DkXFaXlmwU2JDjO+P4jswlK89ufJGl/LvM5GkiBncYiIqHHjUHBqEC/f1MnidSt/d5xNzbfY5uykhLZMj8IaBBFlOj2cVKYY3SJzo9XhYnpBLVtsXWeTW1QKb1d1BUcTEVFjwcwNOYStuptAD1F4bL7YZkUkScLn2y+gy7x1Fl1O6WaZm9S8Yvm5shZLQ5UfIcWiYiKipoHBDTlE60BRd9M+2BMeGid0CvVCjwgfAMAfR5Ksjj+RlGOxevhPBxLx5po4FJfqsdrs+DSzguIdZ9Pl5661GDlVPrjhcHAioqaBwQ05RJcwbwBA+xBP7HvpBvz++ADc268lAOC3Q1fkId1nruYhOacIN378D0Z8uB1lhpFUW0+nytfKNluA07xbylyBVgdtWc1GYVllbjiRHxFRk8CaG3KI0V1C8MnEXrguyheuziKrEtvaH20C3XE+rQAbT17FlewivLfuNFr6ucnnJecUI8LPDafNZh9Oyi6GXi9BqVRUGNwAQHaRFkGeLhXuL4/dUkRETRODG3IIlVKBG7uFWmxTKBQYFB2I82kF+HjzWVxIE8XA8WYLbH648Qzyi8twPs1UKKzV6fHCr0fRN8rfYrRUeTmFpXULbtgtRUTUJDC4oUalSwvRXXUhzfYop18PXpGfe7k4wdNFjSvZRfhxfyJ+3J9Y6bWzCmsWnFQ3uDmZlIv5f8fhmZHt0d1QN0RERI7DmhtqVLoagpvqaB/iiXBfV6vtod62szPmtTnVYQxu3AzdZrnFtmtufj2YiB1n0/Hj/oQaXZ+IiOoHgxtqVNqUGyJ+U7muK3NeLmq0sBHc3B4TbvE6wDDEPLuGmZsMQ/1OK3/RpopGSxmHrlfWJUZERA2HwQ01KuYT8gFATKRvhcf2bOmDMG/L4EatUmBczxYW21oHiOAku6hmmZvUXBGsdArzAlDx/DvGIuZUBjdERI0CgxtqdKYOjAIAPDe6PXq2tA5u7uodgadHtMPUQa2hN1sU8/uH++K7h/oi0mx0FQC0ChCva1JzI0kSUnLFJIDGrrKKMjMZ+XXL3KTnl2DDyauVLvB5LjUP7687zRFbRETVwIJianSeGdUeN3QKRp9WflAqFVhwZ3eEeLtg4ud7AACD2gXgpm5hAIB7+rTEVzsv4faYcPRvE2DzemE+Irtj3i217J+L+GrXRREM+VvPlpxXUoZCrVjAs4sc3BRbHQcAGQUlhv0lkCQJCkXNpkN+4Zdj2Bh3FS+O7YBHBrexecyti3aiQKtDUk4RFtzZo0bXJyK61jBzQ42Oi1qFfq39oTSsmTChlwhcuod7w8dNjcHtAuVjI/zccOI/o/CfWzpXeD1fN2cAlgXFqw5dQUJmEbadSbN5ztUcEch4uTjJ8+xkFGjlSQSNJElCpqG7SqvT12qiv41xVwEAb605VeExBYZAy3zWZSIiso2ZG2oyfpwWi1KdBA+N5T9bZRULR/m4icUuzTM3V7KLAACX0gttnmPskgrxdoGfuzOUCkAvAZkFWgR5ueC9dadwPrUAb03oilKdqTspLb8Y3m7VX1wzo9ykg+dS89E2yKPC4wua0MrkWQVa+Lo7O7oZRHQNYuaGmgyNk8oqsKkOY+YmOacIkiShSKuTsy2XM2zPp5NiyNwEe7lApVTA3zDiKjWvBPsuZeKTLeex9kQKtpfL/BiLkI0upRdgxd546PS262lOJudavF53IqXSezF2lTV2S7aeR8/XN+CXA5XPPUREVB+YuaFmzVmlRLdwbzg7KXEpoxDHruTAzdn0z/5ypu3MzVVj5sZLzJkT6KFBWl4J0vJLsHjLOfm4M2aLeQJAWrlMzO2f/ov0/BIUanV40FAobe5EkmVwY3zfpu6dtaKL7dmfj+C2ckPziYjqGzM31KxpnJTwcXPGmC4hAIDPd1y0WF08PqPQZlbFvFsKAAI9RebmZFIu9l3Kko87czXf4rzymRvjMPE/jlqvdG68HgD4G7pvajqLcl3p9BKmfLUXj3y7v9LRWrWlqqLLkIioPjC4oWZNoxb/xO++Tqw4/seRJDy2/KC8X6vTy4GMuauGICXYyzK4+fd8hsVxlWVuzIOFrAItSsqsu5SMC4DGtvEHYFn0vGjzWXyy5ZxV0FGotV/dzYmkHGw9nYb1J69WOANzXdR05BgRkT0wuKFmydNFdD0Zh4f3a+2HF8d2sHns5fQCHL+Sg7uX/ouer63HjOUH5ZqbkHLBza7zlqOV4st1a6WaBUrmwcKljEJ0fGUtvtp5Efcv24uFG88AAK4ahpcbh5tnGYKbpOwivL/+DN5bd9pq8sDy2aG6OJqYIz+v6fIU1aFicENEDuDw4Gbx4sWIioqCi4sLYmJisGPHjgqP/fXXXzFixAgEBgbCy8sLsbGxWLduXQO2lpqK32YMwBPD2uL1W7sAEBmERwa3wcODrOtefjqQiHGf7MTuC5nIKizFX8eScSpFdBdF+oth4IGGgmJjD1b57pYIPzGXjnkWqHz9jF4C/vPHSWw/k4aFG8/i72PJ8giu9sGeAICsAvHamNEBxAgqc/asyzkYb+piq48uMfZKEZEjODS4WblyJWbNmoWXXnoJhw4dwqBBgzBmzBjEx8fbPH779u0YMWIE1qxZgwMHDuD666/HzTffjEOHDjVwy6mxaxPogadHtrcalj2qc4j83Dh/zapDV1CmlzDEbP6cUp0EJ6UCrQxLNxgzN0b9WvtZvO7WwgcAkJxjCjxScioPQozdY2qVQg6ijNkT81FUVsFNNWdC1uklPPDVXjz385EKjzl42Sy4qWB5ibqoapg+EVF9cGhws2DBAjz00EOYOnUqOnbsiIULFyIiIgJLliyxefzChQvx3HPP4brrrkN0dDTeeustREdH448//mjgllNT1ctsOYe3xndFkCFo8dQ4YcGd3TGyU7C8PyrAHWrDWlfXtfKDWmX6oo5t7W9x3QFtRfdXck6xXCNT3QxLoIcGfoaC4gKtDtoyPU6ZZW7Op5UvWq7edS9nFGDL6TT8uD8Rvx++gnmrT1hMQpiRX4JLGaZutSw7dUuZ1wgp2S1FRA7gsOBGq9XiwIEDGDlypMX2kSNHYteuXdW6hl6vR15eHvz8/Ko+mAgik7Bm5iB8MrEXBkYH4O3busLLxQkv3tgR/h4atA/xlI9tF2x6HuLtgqHtg+TXfaIsg5tB0QFQKABtmV6ukTEGN90jfHB9+0BUJNBTAy8XtdyFk12oRZxZ5uZ8muVcPFVlhGwd9+SKw/h61yWLUVvmgQ0Aee6fuiouNQVQHC1FRI7gsHlu0tPTodPpEBwcbLE9ODgYKSmVT2Rm9MEHH6CgoAB33nlnhceUlJSgpMSUxs/Nza3wWLo2dArzklf6HtYhGEfnjZL3mQc05WcKfvnGjth9IQMdQjwR7mtajdxT44RwX1cEemiQmleC5OxiZBeWyks7DI4OwJPDo9Hu5b9hay6/QE8NlEoFvF3VyCosRZ+3NlnsP1+uW6p8EXNFkm0EQeaLe5YvIM62U81NntninvUxvJyIqCoOLyguP1S0ugsP/vDDD5g3bx5WrlyJoKCgCo+bP38+vL295Z+IiIg6t5mar4oyNwAQ6e+O7c9ej/891FfuRgKAIC8NFAqFvEDn+bR83LZklzwfTrCXC5xUSgR5usjnBHiYzg80bDfOpFyecakIo/jMQpSU6XD30n/x7E8V19PYGuJurnwwk2mnbinzUWIFTWRGZSJqXhwW3AQEBEClUlllaVJTU62yOeWtXLkSDz30EH788UfccMMNlR47Z84c5OTkyD8JCQl1bjs1X1EB7nB2Ev9ZtA+xXuPJ190ZLmoVXNQqeZu/u6jbCfMRQcr3e+KRU2QKHIxz5QR7m4KbPlGmrlRjsbKPWfGzn7szZg5ra/HebQJFcfPljEIcuJyF3Rcy8dOBRFxMt72ERHJOkdW29HxTAFO+xsZeQ8HNMzfaMj1Kyy02SkRU3xwW3Dg7OyMmJgYbNmyw2L5hwwb079+/wvN++OEHTJkyBd9//z1uvPHGKt9Ho9HAy8vL4oeoImqVEm+N74qnR7RDm8CKF7A05+sugpJQb5G52XspU96nVJiGeYd6mYKb61pZBzfmmZu3xnfFqC6mkV0A0CHEC0oFUFSqs5hMcM2xZJvtslWbY94tZQzAjEGVcRh6XeWVmwywsITZGyJqWA5dW2r27Nm477770Lt3b8TGxmLp0qWIj4/HtGnTAIisy5UrV/Dtt98CEIHN/fffj48++gj9+vWTsz6urq7w9vZ22H1Q83J7NddC6hPlh70XMzG5fysAQKhZZgYAlk3pjSBPF7Q0DPM2LuXg5qxC5zDTv1fjHDruZouCdmnhBVez7BAAeLup0cLXFQmZRVh73JTx/PNoMmZcb5nlAaquuTFmbqIC3HEoPrvOo6UyDbMwlw9uCrRlNVopnYiorhwa3Nx1113IyMjAa6+9huTkZHTp0gVr1qxBZGQkACA5OdlizpvPPvsMZWVlmDFjBmbMmCFvnzx5Mr7++uuGbj5d45ZNuQ7xGYVycbKx5gYQ2Zrr2wdZ1I8Zg5sQbxd5jh0A8DfU36TmmYKRFoZrOauU0Bq6dTw0Toj0c0dCZhHOmhUZxyXnIjWv2KKmB6g6c2OctK+6wY1OL0GSJDipLBO+59Py4evmjNj5myABeH605UzQBSX2X9ahIUiSBL3EEV9ETZHDVwWfPn06pk+fbnNf+YBl69at9d8gomry0DjJgQ0A9G7li0BPDTqFeuH9O7pbFca3MmRwovzd5fl1AMjP1WZBg/Hc6GAPeeVwD40TIv3d8I9pUXLZudR8ObjR6SWsOnTFatkGwHLtqxxDcNPaMFFhVkFppQX9j/7vAA7FZ2HdU4MRYMg2Xc4owPAPtlkcd+JKjsXrplhULEkS7lq6G1kFWqx5cpDFZ0NEjR//iyWykyBPF+x9cTi+ebCP1YzGADC8YzDeGt8Vr97cCUqlAt891Bcf3d0Dkf4iuHjpxo5oF+yBTyb2ks/p38Y0n467xgmtDMcaDTbMqmw+XPzPo0l4poJRVJkFWrnA19QtJWqLtDo9fjt8xeZ52YVabIy7iowCLXacTZO377mQaXVs+aHq9szc7LuUKS+NUZHMAq1FQXdt5JeUYe/FTJxNza+wYJuIGi8GN0R2VNk0BmqVEhP7tpSDmYHRAbi1Rwt5f4cQL6x/aghu7BYqbzMu/AkAHhoVRncJQbCXCJx6tvRBB8PQdfMlGjafSjW7picGtwtE60BTUJRhGDFlHAoe6uMCZ0Nm4qmVR+T5ecwZh7UDwN6LpudaGyOhLmVYBgPVCW5yCkuhtzUJkJmk7CLc8em/GL1wR4Xz5xRqyzD43S0Ys3B7lderTKpZ911JKUd7ETU1Du+WIqKKmQ8ZzywoRYSfG3Y8NwwbTl5F1xbe2H1BjJr65t/LOJyQjQ/u7IGd58TK5ff0icCU/lFoF+wBSQL6v70ZKbnFSMsrQYi3izz029fNGXdeF47vdov6tg0nUxDspcHslUfw6JDWCPFywf92X5bbsd9sNFh6vvU6V+bDzQFRUFyZc6n5GPHhNozr0QIf3tWjwuNOXzUtSVGg1cFDY/2/rxNJucgvKUN+SRmSc4vl2qWaspjssMj+a24RUf1icEPUiLlrnNA6wB0X0gsw0LB+lbOTUs7umNfQHEnMwQ0LRP2Lq1qFebd0hsZJjLhSKMSQ85TcYlzNLUb7Mk+5FsbXTY03xnXF0HZBmPrtfmw9nYZfDlxBUakOT644DKUCFjMrn03NR1aBFr7uzjaDm/IKqhgK/sWOC5AksYCpreAmI78Efu7OFgFHel6JzeDGfDX1S+kFtQ5uUm0MmSeipoPdUkSN3KoZA/D3k4PQNdx6uoPyS0QY9W3tJwc2RsYVzt9ffxqXDV1HCgXg6SKGafdv6w9nlRKJWUUoKjUFJOaBjb9hZuble0QmJz1PZDViW/tbZJkAwN1ZvH9V3VLaMlO3j7G76c+jSXjk2/1YsOEMYt7YiP/tvowEs1qeioIq8zW5yneP1YT54qT2WpaCiBoOMzdEjZy3qxrerrbniTHfPqFXCyRlF6FIq8Ojg9tYHfvMyHbYcyEDp1Ly8OrvJ+TzjUOd3Zyd0Le1H3acTbc6181ZhZdv7ASlAnjh12P4YMMZXNfKTw4yJvWLRFahFnsvmrqsOod5Y++lTIvRUtP+dwAX0wvw82Ox+GFvPH7cn4gss1FducVl8HZV45mfjqC4VI/1J68CAJZsPW8RPFUU3Jwql7mpLVuTHdbUwfgsBHpoEGE27J+IGgaDG6ImbuUj/XA2NR/39m1ZaUFzpL87FtzZA5O+3IN/DbU6PuWCpnm3dMavBxPRtYU3vtp5CXsMwcoHd3THmK6iK2zX+QysPpKEP44myUFGgIczyk8H0zHUUwQ3hszN5YwCrD0hJh8cv3iXRRG0UWpuMbxd1RYriwOAi1plMQorLd+6Dkavl3DKInNTvQVGbUmtZIHR6riUXoAJi3eJ529XPZM6EdkXu6WImri+rf0xqV9ktRacHdDWXx5hBYi1ssy1CfTAs6M6YHSXUHQz6wbrFekrPx9rCHL2XsyUi4cDPDXw9zANfw/xcpHnwknLK8HD3+7HTf/9R95vK7ABRFBhvjaVUUJmIS6kmTIx6Xkl+PtYMu5ftheZhsxPQlahRZaoLpkb8wkVa9MtZZybCACKmuA8P0RNHYMbomuIQqHAk8OjAYgA5JFBrSs8tmu4DwAxW3KwxbpYItA5czUf+YasTICHRp5pGQBaBbihtWFtrj+OJmHDyatWyzLYcu8Xe3D9+6Io2sdNjQtvjYWrWoUyvWTRPZSeX4LHlh/E9jNp+HjTWQDAofhsAJBXbL+cWWgxHFySJCzfcxlnzEZdGWUWaPH59gtyd1Rqbt26pUrKTAFNYpbIIO06n44Hvtprtcq70dHEbLy/7jS0ZXoUl+rw0cazOJGUY/NYIqocu6WIrjFjuobi4CsjLOptbBnVORiT+rXEkHZBFtv9PTSIDvKQl4BwVinh5eIEmBUeh3m7Ymj7QGiclCgpsz1PzOB2gdhuY04dY1dXuK8rlEoFogLccTLZcuI+88yPsdton2GI+i3dw7B8z2Voy/S4kl0k17xsPpWKl1YdBwCcen20vLJ7cakO4z7ZifjMQlxIL8D8CV0tu6VqEdyYn385oxDRwZ6Y+PkeAMBTKw/jx0djrc65ZdFOACKo0+r0+HDjGXy48Uyl3VoJmYUI8tJYFY83BR9uOIONcVfxwyP94OXCtcfIvpi5IboG+bk7V7lmksZJhTfGdcWITsFW+8yLewM8nKFQKODlavpbydPFCe4aJ1zf3hQY9Y3ywwMDWsmv+7fxt1oc1JxxGHcbGyPC9pgVLmcXlSIuOVee86dfa395RXfzoeHmq7Wv2Gtas+6/m8/K9Ty/HExEcanOIluTU4tuKfPMT3xmIXRmGaS9FzOtJiE074qLzyzEYUMWqjLHEnMw6N0tmP2j7dmoG7uPNp3FiaRcLN8dX/XBRDXE4IaIauyu6yLk50mGBTrNa3783EW9zfheYgbmvlF+WPloLObe3BnLpvTGgwOi8MCAVugeYT283aiFj8i4+JvVBc0Z08HquK2n0zDmox04b6jJ6d3KV17zyzzjY75ExVtrTuGplYfx7b+XLEaHacv0VqPFqjOJ3y8HEjF64XZcSBPvcdWsZic+sxBnUy27wi6Uqwc6cNk067MkARqzoC+/gqH0xkyV+aSKjmDeBVcbmQVVz5VEVFMMboioxrqF+2D6UDHc/PaYcHn75NhIhHq74L7YSADAqM4h+N9DfbDIbL2sYR2C8erNnaBxUuHd27qjTys/TOjVAuW18HWVrwEAY7uGYKTheUVa+LgiwEMsXgrAombFWOTra+j2WXXoCl79/QSOJopjIg0Lmy7dfh6AaZh9dWpunv7pCE6l5OGFX44BANLKZW4OlcvEbI5LtXi9zyxASc4pQpHZrM4VFUZfSDcEUrklyDVkfo5fycGP+xLqtPREdWXkl+CJHw6h4ytr8dsh22uSVcS8fYUsuKZ6wJobIqqVZ0e1xw2dghFt1m30n1u7YN4tnS2yOIOiAyu8Rkt/N/w4LRbbz6Th14OWX5DGxUdj2/hjyzNDEe7rimKzyQVVSoVFd0+AhzOeG90eAKwyN5kFWiQbMkw7nh+G/ZcyMeWrffK5nhon3NcvEm/8FSevoxXb2h9rT6SguFQU+LqYZVNW7otH2yBPxET6WmRWDiVkQZIkq8zNoXhxTYVCZGbiknNRptPjzTVxaOHjajE/UHJOMcx7rS6mF6BLC+sM1/lUU9BzIa0AxxKz8Yph/qJgbxcMaVfx772uCrVluO/LvfLvd9uZNIzraR2gViTP7HfWFEeTacvEIrMD2gbUehZsql/M3BBRrSgUCvRq6SvPcGy+vaa6tvCGs0qJcF/TF0WU2QroUQHuUKuU8NA4IdLfDc5OSnz3UF+La+x98QZ5IVJj5iYhswirjyThky3n5Ot4aJwwtH0Q5k/oKp/bKczLKhjoE+Un1yXlFJWiUFuGZf9cxI/7EvD8L8dw2xIxj82RhGz5nFKdhKg5a3DZbI6dyxkF2HJaFE6PN7QvIasQvx66gq92XsIbf8XhoFlmJzmnGClmMyRfTC/AFzsu4IP1py1qdYyZGwA4czUP8/8+Jb8+fkVko7IKtBYBoLnjV3Iw8sNtNhdKrcobf8VZdPnVdDboXLNsWGY15hGqa9eXvX218yKe+/kobjab3oAaF2ZuiMjhfN2dseP56+HmrMLZ1HxcSi+wudyEQqHA6hkDUaLTIcjTNDxdpVRAaVYg7ePmjBY+rriSXYSZPxySt/eI8JGfG9fqAoBW/u5oG+SBMG8XuYaoc5gXvF3VyCzQIqtQizf+isMfR5Is2pOWV2JRL1Oem7MKhVod0vJK4Oumxt19WuLXQ1dwMb1ADrgAQKeXEOHnioTMInneHqMFG87Iz0d1DkGXFt7IKy7FVbOur22n0yy6d06l5GH3hQzc8/luzBjaFs+Mam/Vtilf7UN6fgkmL9tb44kG/zHUJT11QzsxoquGcwrlmhVQp+QUV3Ik8MmWc/ho41l8/3Bf9G7lV+mxDWWrIVjNLNDi3/MZ8HJ1QuewiuvHqOExc0NEjUKwlws8XdTo1dIXE3qFV3ict5taDmw+nRSDYC8NVj7Sz+q4Z0e1R/dwb7QP9sSYLiGYPaId5ow1FSSbL4sQHewBhUKBwWbZm45hXogwZJL+9+9lq8AGAGb/eFgOPib2bWmV/ZlpmFMIAG7t0UJeCyw9X2uR3QGAe/tGVjp6DIA8IuxiuWDi7+PJFq9Pp+Ri7fEUSBLkWaHLq86ip5Ik4cDlTLnr6FB8Fg4nZCM5R8zVM7arqIHKKiyt0aiy3CJTt9TVXOvgxjxD9d6609Dq9BaZqcok5xRh4Dub8Z8/TlS7PTWlUZu+Oid9uQeTl+21GgFHjsXghoiarNFdQrDnxRts/kU/rmcL/P74QKx7ajCWTIrBzOHRFtkeAFjxSD88Org17o9tBQBycBLp7wYvFzUeHSKKppfvsT1c2TiyylmlxCODWuObB/ugo6FLDADuj41EkKcGKqUC9/RpCV83tbygKAAM6xAEF7USKqUCN3YNRai3qX2uahWcnZQI9NSgq6Hm5o2/4jDuk53y/EDOTuJ/4caep76GIfoX0grkOp7zaflWi5eaz8AMiBoaW77ZdQm3LfkX76w9hfT8EoxfvAvjPtmJUp0ElWEOImNtVE26pswzN1mFpRa1VMWlOoxeuAPjPtlpUXisVpkyc7vOpVsM8zf3n9UnkZhVhK92Xqp2e2rKxWxeIZ1eQnq+1iJgq8iX/1zEnZ/+a3MW7uZApxcTZVY0A3lDYnBDRNesfq39MWdsRzlIGNk5BE+PaIf540U9zpguIejZ0gcA4KGx3Ysf7KXBv3OGyauu//eenvB1U+PRwa3h5uyEX6f3x2/TB6B9iCcUCoVFxuj69oFYPrUf/vdQH0T4uSHUxxTc9Gzpgz1zhuOf56/Hf27tLG8/nJCN99eLbNHYLiEWX/pD2gfC08UJZXpJromRJMsh8d/+ewl93txkcQ9nrlp+GRWX6nDmah7m/XESAPD1rkty1sgoxMsFTiqlXBtlDG6+2XUJ/910FmU6MXnjwfgsDH53C+789F9sPS1GieWWG4FmPi/QtjNpOH01D4cTsnHcbLSbu7P4/SdmFWLiF3swauF2q3W/9HoJm0+ZRqKVrzc6GJ+FB77aiz3l7gUQRcLnUm0HTOXZmiMqOdf2zNPmvv33EvZeyrQoIG9Ovt8bj5dWHccNC7Y5uikMboiIjFRKBZ4YHo3+hnochUKBZZOvw7IpvbHnxeH4+J6eVuc8PizaYl2ttkEeOPDyCMwZ2xEAEO7rZlE/ZF403bOlL2IifdG/jXi/CF9T4BPm4wpfd2donFRy5qa8UZ1DcENH0ySLHUI8LdYOMzpmGO6eU1SKN/6Ms9p/OsVyBujnfj6KkR9ut9j2T7n5f8IMgVirANHmc6n5OHM1D3NXn8AHG87g4W/3IyO/BD/uS0B8ZiH2XsrE1G/2Y1PcVeSWW4rDvIB63XFTN9r6E1fl58aZok8lmwKQz3dcsLjOgfgsaHWmGbHN65f2X8rEhMW7sOV0GhZvPW/1O5j3xwncsGB7pQXWkiSWAcm1kXlJrqJ2CBBrogFARkHNF2OtC71ewqpDiXVab606tp1OrfqgBsKCYiKiSvi6O2NYBxFA3NI9DAHuzgjw1Mhf/jcaFhI1p6xk9ucys2xC+3KByPShbaFUKpBfXIaHzdb9UquUmDakDb7eddFixfR+rf3h7KTE34aAIDrIE6O7hMrD2Y1e+/MkyvR6KBUK+cu/fbAnvN3U2HsxE6fMungSswqx2kZ90Yp9CRavjUOgIw2Zm/9uPof/bjYVSW85nYYRH25HqWH5DWcnJbRlesz84RBuKDfr9T9n09Anyg8lZTpsOGkKaNafNAU6xq408+6vr3dewvShbeFuyKrtOmeZkbmaWyx3m5kHNHsuZkCSJHlkX3GpTp6rZ/+lzAqH0S/afA4fmBV4m6uqMLpQWyYv7Fq+aLy+rT2RgqdWHoFKqcD5t8bW2/sUlDSeUW3M3BAR1UD/tgFoF+yJ7x7qi5+nxcoLdVZXS7NuKbXK8n/BLf3d8Nb4rvj4np5Wgc8LYzrg1Otj5MxPp1Av+Lo7Y0i7QAxpF4gRnYIR7uuKBwe0wqwbouGqVuHO3qbC7Pl/n5IzHW+N74p1Tw3Gnb3FTNMbTl5FkVaHd9aewsB3tlTrPsIMwc3ITsEI9tJY7HtxbAe0DnRHZoFWntNmyzND0TfKDwVaHX4/LIInY+Dx8eZz2HE2DWuPp1jMgWPeXZaaWwJJkiyKqQu0Ouw8Z8ooHU3MtmhHal6xvBDprvOm44pL9bhkVtC99XSqPNrMuBRHeWU6fYWBDVB15iY9zxTQNHRwYxzRp9NLFvVN9lZgVrvl6AJrBjdERLUwMDqgVkOTnxnVHnfEhOOXx6wXz6wO46SIA6NFV5aTSolvHuyDz+/vDYVCAYVCgVk3tMPx/4zCO7d1w2uGeh1JErMZe7uqMa5nGABRkB3q7YLErCL0eWsjlphlN54d1R539g7HoGjTkHnzhJQxuIkO9sS/LwzHp5N6IchTg8HtAvHwoNZ46oZ28rEhXi5o4eOKhXf3gPk0SA8NjMLdhqU8vt55CcsMRcDmAaBRSZkeuUVlcubG05Ct2WSY7VmSJBwxdL8ZZ5d+8Ov9GPbBVvx9PBnFpXqEervINVTGiRUB4K9jpgyRMbgpLtVZfEHvOm9dp2MuJafymps0s9FpGfkNG9yYf251XWlep5dw35d7MHvlYat95hNaFtVjEFUdDG6IiBqQl4sa793RHTGRtZuz5dlR7fH86A54fFjbSo9TKUWgc39sK4sJC+++LgJuhuJcD40T3hzfBQCQZ6iDaRPojg/v6o7pQ9vg3du7y/VAAPDZfb3l50GepmyNUqnA6C6h2PPicHw95TooFAoM72haNNVYgBvq7YpIs8DFy0WNqYOiAACbTqXiSEI2nJ2UeOWmTjbvKTWvGJfSRfDx4EBx3sr9CTiVkovknGKk55fASanA0PambqXErCI8tVIsLnp9hyD0aukLAPKSGMWlOmyKM3WFJRhmlI55fQNeXHVM3m7MNlXEVubm7NU8eaSa+dD7hlpP60JaPp5aeRj7zeZiOhSfjT+OJOHDDWfkou+auJxRgB1n0/HroStWo+yyzaYDyK7FgrP2xOCGiKgJ8XN3xmND28Cr3MzQlRnbNRQeGic4q5SY1C/SYt+wDsFYcq/IunQP98ZfMwdhfM9wuR7lpm6h8HN3xsODonCDWcASZmPZAYXCNJmim7MT+rUWAZz5exqXxgAAL1cntA3ylOf/AYCJfVqij1lGLLa1v7zER0JWIZIMGZK7+0TII9hGL9yBx747AABoF+xpUZht7pbuYegdKYKb7WfTUKrTY8sp0SUVYCgKT8/XYvziXSjQ6vDD3gTo9BL0eglbqiiWLV9zs+VUKkZ8uB0vGQKktDzz4KbizM07a0+h69x1mLf6BLRlFQcfO8+l4+6l/+LsVVO9VJFWZ7GcxZKt57Hq0BWLtc32XszEEz8cwkebzuLrXZcqvSdbzO8zKduUrSrS6izuqzprstUnBjdERM2ct6savzzWH79O728xFN1oTFeRdfl1+gCLNbQAMdnhgZdvwEs3doJCocBXD1yH127tbHO9q/KW3t8bC+/qIWdnANPSGADkAO0+Q/DTp5Uf5oztAG83NdoHe0KtUmDuLZ0QZKjp2XMxE5IkuqRCvFzw3u3d5Ll9jF1S17XylY83v///3tMT/Vr7Y3C7QLiqVbicUYjol/7GY8sPAgDG9wyDj5t1wLj5VCrWnkipsk4mJacYRxKysWD9aeSXlOH5X44CAH47nARJkiwyN+n5WuQUleKdtaeshp+v2BuPvJIyfL3rEpbtvGjzvQq1Zbj3iz3YfSETb/wlRr8VaXUY/sFWjPtkpzwE/vRV66Ht680Ktj/Zcs7myK9K79NsZNuVbLNAp1y3nKMzNxwtRUR0DShfoFyeQqGAqoJBXubrhV3fPsj2QTZ4uaitFtQ0n+TQy9UU3EQHeaBXpC80hgnyfnikHwpKyhDh5yZPvvjZNlEQ3SFUzBk0pmsoRncJwVtr4rBs5yWM7hKC2SPa41+zeWzeHN8F91zXUs4ouWucMKJTsNWIsBu7hWHX+QyrL+WHv90vPx8UHSBP3KhWKXBH7wj0aumLZ346grySMoxbvBOSBPx5LBmpZpmaxKyict1SWry06hj+PJqM9SdSsOnpoQBEdifL7P3/OpqMaYaJJAGRDdHrJYuMy7/nM1BcqsOJpByxdEhOMQ7FZ6FnS1+csRHcmMsqLMWGE1dxW4z1jOBHE7NxMb1AXq/NyLz77UpWkc3noq0NW1dUHoMbIiJqMObdUm6G2ZqVSoU8t5CRn7uzPBKtTaBpEVVfNzXm3WKa1FChUOClGzvh+dEd4GQYfRZoVg/UPdzHamj+uJ5hcnAzdWAUOoV5oUeEj0Wh76ODW+Oz7Zbz6IzsFCwHN6U6CW8ZJntcvucyDsVny6u5X0iznE/maGKORbdUUakOfx4VS2acTyvAhbR8/H08RZ7k0NtVjbziUhy7koOEzEJE+LmhuFSHGz/egYIS05ByANDq9Ji3+gSiAky/o18PXUF2YanFtAEAMLxDEDadsuxe23cp0yq4KSgpwy2LdgIAWgd4WMzTVFG31JXs8sENu6WIiOgaEeLlgrZBHgjw0NgcFWXL1EGtseDO7pgzpgNWPz7Q5iKVTmbD6s2XsbCVsbq+fRDmjOmALyf3xss3dZLXMpt+vciSPDuqvTwazUjjpMT1HWxnrd65rZvp/YI9oXFSok2gO1r5i/s7mpiN9EpGSP3nj5N4b91peX6iQdEBuM5Qd/TxprPQlumx+nASErOKkFVYCm2ZHp4uTpjSvxUAMQeR+dpb3++Jx1SzjJPRMLOaKWNXoK3Zkn/ab5rT6GSy5eiq5AqCm5NJlhNBsluKiIiuGQqFAmtmDkKpTm9V31MRF7Wq0sVUywvzccUnE3vB101tNZeQsQ2PmnX3GN3bNxKDogPRyt8NpToJozoHo5W/O6Zf3xZ5xaUIr6BQuV2wJ5ZN6Y2TSbmYNqSNHGj9uC8Bz/1y1CoDZHRrjzD8fjjJalbkTmFeiPB1w56LmfjpQCLOXM2zyoSM6BSMmcOjkV2oxW9VjOQyMu9SnDooCt/tuYwL6QVIzStGoIcGfxxNRvtgT3xpVutjzEIlZhVizq/H5MwVACRmFyGvuBSnU/LkEVkhXi5IyS2WZ5R2FAY3RETUoJydlPJ6XvXlxm7WM0dXxbgYKAA4Oykshr4b586pyLAOwfJM1kZ9W/tBqTAtbFreO7d1w+mUPIsZogGgY4gXru8QBLVKidk/HpaLpV3VKnn+mDFdxCi2hXf3xNnUfJwwZE4eHhSFnKJS/Lg/EYCYSfpKdhGmDoxCmI8rltzbCwqFApH+7ugY4oWTybnYcyETKqUCM384ZNXG82liIsUZyw/K7TBKyhbD7DeaDaUf3jEIy/fEO7xbisENERFRNc0Z0wHz/z6FmVXMMwSIpSnWzhqMjXFXkZRdhPbBnnjl9xMAxESFLmoV5t7cGfd8vhvBXhrEtvbHubR89DGMABvdJQQ+btfh0f8dQKi3C+be3BkSJJxOybMYlj8oOlAObl4Y0xEqpQKD2wXi212X8dE9PVBQopO7yMaYLRcyoK0/TibnYsPJq0jIspyZeXC7QGw/k4ZzqfnYfibNKrABRKF0olkhcYiXizxsP8fB3VIKydFzJDew3NxceHt7IycnB15eXlWfQEREZCBJEi6kFyDK373SNcQqsuZYMj7fcQEL7+ohr8u1/1Im/D00FkXB5nR6yeZK5EbZhVpM/mofYlv744UxHardloPxWZiweJfV9kBPDX58NBbXv78VAODv7my12KeLWmlVsNyzpQ8mx7bCrJWHMaCtP5ZP7VfttlRHTb6/mbkhIiKqJoVCgTaBHlUfWIGxXUMxttxiq1Ut41FZYAMAPm7O+H3GgBq3pWeED3zc1HLx7529w/HI4NZwN8wjZJRRoEUrfzc8NrQNnv/lGJydlPjPLZ3xyu8nUKrTY0r/Vvj7WAqeH91B7jZjtxQRERE1OIVCgbt6R+Cz7RfQMdQLc2/uLK+wDgAxkb7yopuLJvZClxbecHN2QutAd3QO88bA6ECk55Wge4QP5t4shucfNKzZxdFSRERE5BAzh0cjOtgTIzoFWwQ2gFjd/e9jKXh4cGsEGzI5N3cPk/e38HFFi3LLcPi4qqFUwGKBVEdgzQ0RERHZhSRJkCTUqh6pKqy5ISIioganUCgcnrUBOEMxERERNTMMboiIiKhZYXBDREREzQqDGyIiImpWGNwQERFRs8LghoiIiJoVBjdERETUrDC4ISIiomaFwQ0RERE1KwxuiIiIqFlhcENERETNCoMbIiIialYY3BAREVGzcs2tCi5JEgCxdDoRERE1DcbvbeP3eGWuueAmLy8PABAREeHglhAREVFN5eXlwdvbu9JjFFJ1QqBmRK/XIykpCZ6enlAoFHa7bm5uLiIiIpCQkAAvLy+7XbexaO73BzT/e2zu9wc0/3ts7vcHNP97bO73B9TfPUqShLy8PISFhUGprLyq5prL3CiVSoSHh9fb9b28vJrtP1ig+d8f0PzvsbnfH9D877G53x/Q/O+xud8fUD/3WFXGxogFxURERNSsMLghIiKiZoXBjZ1oNBrMnTsXGo3G0U2pF839/oDmf4/N/f6A5n+Pzf3+gOZ/j839/oDGcY/XXEExERERNW/M3BAREVGzwuCGiIiImhUGN0RERNSsMLghIiKiZoXBjR0sXrwYUVFRcHFxQUxMDHbs2OHoJtXavHnzoFAoLH5CQkLk/ZIkYd68eQgLC4OrqyuGDh2KEydOOLDFldu+fTtuvvlmhIWFQaFQ4LfffrPYX537KSkpwRNPPIGAgAC4u7vjlltuQWJiYgPeReWquscpU6ZYfab9+vWzOKYx3+P8+fNx3XXXwdPTE0FBQRg3bhxOnz5tcUxT/hyrc39N/TNcsmQJunXrJk/qFhsbi7///lve35Q/P6Dq+2vqn1958+fPh0KhwKxZs+Rtje0zZHBTRytXrsSsWbPw0ksv4dChQxg0aBDGjBmD+Ph4Rzet1jp37ozk5GT559ixY/K+d999FwsWLMCiRYuwb98+hISEYMSIEfKaXY1NQUEBunfvjkWLFtncX537mTVrFlatWoUVK1bgn3/+QX5+Pm666SbodLqGuo1KVXWPADB69GiLz3TNmjUW+xvzPW7btg0zZszA7t27sWHDBpSVlWHkyJEoKCiQj2nKn2N17g9o2p9heHg43n77bezfvx/79+/HsGHDcOutt8pffk358wOqvj+gaX9+5vbt24elS5eiW7duFtsb3WcoUZ306dNHmjZtmsW2Dh06SC+88IKDWlQ3c+fOlbp3725zn16vl0JCQqS3335b3lZcXCx5e3tLn376aQO1sPYASKtWrZJfV+d+srOzJbVaLa1YsUI+5sqVK5JSqZTWrl3bYG2vrvL3KEmSNHnyZOnWW2+t8Jymdo+pqakSAGnbtm2SJDW/z7H8/UlS8/sMJUmSfH19pS+++KLZfX5GxvuTpObz+eXl5UnR0dHShg0bpCFDhkhPPvmkJEmN879BZm7qQKvV4sCBAxg5cqTF9pEjR2LXrl0OalXdnT17FmFhYYiKisLdd9+NCxcuAAAuXryIlJQUi/vVaDQYMmRIk7zf6tzPgQMHUFpaanFMWFgYunTp0qTueevWrQgKCkK7du3w8MMPIzU1Vd7X1O4xJycHAODn5weg+X2O5e/PqLl8hjqdDitWrEBBQQFiY2Ob3edX/v6MmsPnN2PGDNx444244YYbLLY3xs/wmls4057S09Oh0+kQHBxssT04OBgpKSkOalXd9O3bF99++y3atWuHq1ev4o033kD//v1x4sQJ+Z5s3e/ly5cd0dw6qc79pKSkwNnZGb6+vlbHNJXPeMyYMbjjjjsQGRmJixcv4pVXXsGwYcNw4MABaDSaJnWPkiRh9uzZGDhwILp06QKgeX2Otu4PaB6f4bFjxxAbG4vi4mJ4eHhg1apV6NSpk/zF1tQ/v4ruD2gen9+KFStw4MAB7N+/32pfY/xvkMGNHSgUCovXkiRZbWsqxowZIz/v2rUrYmNj0aZNG3zzzTdyAVxzul+gdvfTlO75rrvukp936dIFvXv3RmRkJP766y9MmDChwvMa4z0+/vjjOHr0KP755x+rfc3hc6zo/prDZ9i+fXscPnwY2dnZ+OWXXzB58mRs27ZN3t/UP7+K7q9Tp05N/vNLSEjAk08+ifXr18PFxaXC4xrTZ8huqToICAiASqWyijpTU1OtItimyt3dHV27dsXZs2flUVPN5X6rcz8hISHQarXIysqq8JimJjQ0FJGRkTh79iyApnOPTzzxBFavXo0tW7YgPDxc3t5cPseK7s+WpvgZOjs7o23btujduzfmz5+P7t2746OPPmo2n19F92dLU/v8Dhw4gNTUVMTExMDJyQlOTk7Ytm0bPv74Yzg5OcltbEyfIYObOnB2dkZMTAw2bNhgsX3Dhg3o37+/g1plXyUlJYiLi0NoaCiioqIQEhJicb9arRbbtm1rkvdbnfuJiYmBWq22OCY5ORnHjx9vkvcMABkZGUhISEBoaCiAxn+PkiTh8ccfx6+//orNmzcjKirKYn9T/xyruj9bmtpnaIskSSgpKWnyn19FjPdnS1P7/IYPH45jx47h8OHD8k/v3r1x77334vDhw2jdunXj+wztXqJ8jVmxYoWkVqulL7/8Ujp58qQ0a9Ysyd3dXbp06ZKjm1YrTz/9tLR161bpwoUL0u7du6WbbrpJ8vT0lO/n7bfflry9vaVff/1VOnbsmHTPPfdIoaGhUm5uroNbblteXp506NAh6dChQxIAacGCBdKhQ4eky5cvS5JUvfuZNm2aFB4eLm3cuFE6ePCgNGzYMKl79+5SWVmZo27LQmX3mJeXJz399NPSrl27pIsXL0pbtmyRYmNjpRYtWjSZe3zsscckb29vaevWrVJycrL8U1hYKB/TlD/Hqu6vOXyGc+bMkbZv3y5dvHhROnr0qPTiiy9KSqVSWr9+vSRJTfvzk6TK7685fH62mI+WkqTG9xkyuLGDTz75RIqMjJScnZ2lXr16WQzhbGruuusuKTQ0VFKr1VJYWJg0YcIE6cSJE/J+vV4vzZ07VwoJCZE0Go00ePBg6dixYw5sceW2bNkiAbD6mTx5siRJ1bufoqIi6fHHH5f8/PwkV1dX6aabbpLi4+MdcDe2VXaPhYWF0siRI6XAwEBJrVZLLVu2lCZPnmzV/sZ8j7buDYD01Vdfycc05c+xqvtrDp/hgw8+KP8/MjAwUBo+fLgc2EhS0/78JKny+2sOn58t5YObxvYZKiRJkuyfDyIiIiJyDNbcEBERUbPC4IaIiIiaFQY3RERE1KwwuCEiIqJmhcENERERNSsMboiIiKhZYXBDREREzQqDGyIiiEX/fvvtN0c3g4jsgMENETnclClToFAorH5Gjx7t6KYRURPk5OgGEBEBwOjRo/HVV19ZbNNoNA5qDRE1ZczcEFGjoNFoEBISYvHj6+sLQHQZLVmyBGPGjIGrqyuioqLw008/WZx/7NgxDBs2DK6urvD398cjjzyC/Px8i2OWLVuGzp07Q6PRIDQ0FI8//rjF/vT0dIwfPx5ubm6Ijo7G6tWr6/emiaheMLghoibhlVdewW233YYjR45g0qRJuOeeexAXFwcAKCwsxOjRo+Hr64t9+/bhp59+wsaNGy2ClyVLlmDGjBl45JFHcOzYMaxevRpt27a1eI///Oc/uPPOO3H06FGMHTsW9957LzIzMxv0PonIDuplOU4iohqYPHmypFKpJHd3d4uf1157TZIksXL2tGnTLM7p27ev9Nhjj0mSJElLly6VfH19pfz8fHn/X3/9JSmVSiklJUWSJEkKCwuTXnrppQrbAEB6+eWX5df5+fmSQqGQ/v77b7vdJxE1DNbcEFGjcP3112PJkiUW2/z8/OTnsbGxFvtiY2Nx+PBhAEBcXBy6d+8Od3d3ef+AAQOg1+tx+vRpKBQKJCUlYfjw4ZW2oVu3bvJzd3d3eHp6IjU1tba3REQOwuCGiBoFd3d3q26iqigUCgCAJEnyc1vHuLq6Vut6arXa6ly9Xl+jNhGR47HmhoiahN27d1u97tChAwCgU6dOOHz4MAoKCuT9O3fuhFKpRLt27eDp6YlWrVph06ZNDdpmInIMZm6IqFEoKSlBSkqKxTYnJycEBAQAAH766Sf07t0bAwcOxPLly7F37158+eWXAIB7770Xc+fOxeTJkzFv3jykpaXhiSeewH333Yfg4GAAwLx58zBt2jQEBQVhzJgxyMvLw86dO/HEE0807I0SUb1jcENEjcLatWsRGhpqsa19+/Y4deoUADGSacWKFZg+fTpCQkKwfPlydOrUCQDg5uaGdevW4cknn8R1110HNzc33HbbbViwYIF8rcmTJ6O4uBgffvghnnnmGQQEBOD2229vuBskogajkCRJcnQjiIgqo1AosGrVKowbN87RTSGiJoA1N0RERNSsMLghIiKiZoU1N0TU6LH3nIhqgpkbIiIialYY3BAREVGzwuCGiIiImhUGN0RERNSsMLghIiKiZoXBDRERETUrDG6IiIioWWFwQ0RERM0KgxsiIiJqVv4PkWDwaH1dyaQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(train_eva_df,'Train','Epoch','Loss',['loss','accuracy'])\n",
    "plot_graph(test_eva_df,'Test','Epoch','Loss',['loss','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e98088e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
